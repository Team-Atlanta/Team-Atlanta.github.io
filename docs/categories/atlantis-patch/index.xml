<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Atlantis-Patch on Team Atlanta</title><link>https://team-atlanta.github.io/categories/atlantis-patch/</link><description>Recent content in Atlantis-Patch on Team Atlanta</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 02 Nov 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://team-atlanta.github.io/categories/atlantis-patch/index.xml" rel="self" type="application/rss+xml"/><item><title>Teaching LLMs to Retrieve: Custom Models for Security Patch Generation</title><link>https://team-atlanta.github.io/blog/post-custom-model/</link><pubDate>Sun, 02 Nov 2025 00:00:00 +0000</pubDate><guid>https://team-atlanta.github.io/blog/post-custom-model/</guid><description>&lt;h2 id="the-typedef-that-changed-everything"&gt;The Typedef That Changed Everything&lt;/h2&gt;
&lt;p&gt;Picture this: you&amp;rsquo;re asking an LLM to patch a security vulnerability in Nginx, a codebase with millions of lines. The bug is clear, the fix location is obvious, but your patch won&amp;rsquo;t compile. Why? Because buried somewhere in the headers are two critical &lt;code&gt;typedef&lt;/code&gt; definitions that the LLM never saw.&lt;/p&gt;
&lt;p&gt;We discovered this the hard way during the AIxCC Semifinals. Challenge &lt;code&gt;challenge-004-nginx-cp/cpv15&lt;/code&gt; became our wake-up call. When we ran our baseline patching agent Aider 20 times without the typedef definitions, only 5 patches compiled successfully. But when we included those typedefs? 18 out of 20 compiled successfully. &lt;strong&gt;That 5/20 → 18/20 leap wasn&amp;rsquo;t about smarter LLMs or better prompts. It was about giving the right context.&lt;/strong&gt;&lt;/p&gt;</description></item><item><title>Every Patch Agent has its Own Story (1) - Martian: Exploring the Unknown with Sophisticated Tools</title><link>https://team-atlanta.github.io/blog/post-crs-patch-agent-martian/</link><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate><guid>https://team-atlanta.github.io/blog/post-crs-patch-agent-martian/</guid><description>&lt;p&gt;As we mentioned in our previous blog post, we enhanced the patching capabilities of Atlantis by ensembling multiple patch agents. In this series of blog posts, we will introduce each of our patch agents in detail and explain the rationale behind their designs.&lt;/p&gt;
&lt;h2 id="diversity-for-good"&gt;Diversity for Good&lt;/h2&gt;
&lt;p&gt;To maximize the effectiveness of ensembling, it is crucial to have diverse agents. If all agents are similar, the ensemble will not perform significantly better than any individual agent. Therefore, we intentionally designed our patch agents to be diverse in their approaches, methodologies, and also models used. We newly developed six patch agents, each with its own unique architecture and motivation, as summarized in the table below.&lt;/p&gt;</description></item><item><title>ClaudeLike: How to Apply a Tool-Based Agent to Patch Generation</title><link>https://team-atlanta.github.io/blog/post-crs-patch-agent-claudelike/</link><pubDate>Fri, 10 Oct 2025 11:00:00 +0000</pubDate><guid>https://team-atlanta.github.io/blog/post-crs-patch-agent-claudelike/</guid><description>&lt;p&gt;Claude Code is an LLM agent specialized in general-purpose programming tasks and remains one of the most powerful LLM agents to date. Inspired by Claude Code&amp;rsquo;s strategy, we developed ClaudeLike, an agent dedicated to patch generation. In this post, we introduce the motivation and key features behind the development of ClaudeLike.&lt;/p&gt;
&lt;h2 id="motivation-applying-a-sota-llm-agent-to-patch-generation"&gt;Motivation: Applying a SOTA LLM Agent to Patch Generation&lt;/h2&gt;
&lt;h3 id="why-did-we-need-to-develop-a-new-agent"&gt;Why Did We Need to Develop a New Agent?&lt;/h3&gt;
&lt;p&gt;When Claude Code was released, we experimented to see whether it could generate patches effectively, as we had previously done with tools like Aider and SWE-Agent. We found that Claude Code performed reasonably well when provided with contextual information such as crash logs. However, we determined that directly integrating Claude Code into Crete would be difficult.&lt;/p&gt;</description></item><item><title>Vincent, One Puzzle for Our Ensemble Toward High-quality Patches</title><link>https://team-atlanta.github.io/blog/post-crs-patch-agent-vincent/</link><pubDate>Fri, 10 Oct 2025 11:00:00 +0000</pubDate><guid>https://team-atlanta.github.io/blog/post-crs-patch-agent-vincent/</guid><description>&lt;p&gt;As mentioned in the previous post, our strategy for patching is to prepare multiple agents to ensure both the robustness and correctness of the system.
To this end, we developed various patch agents, each specialized for different LLM models and tools.&lt;/p&gt;
&lt;p&gt;In this post, we would like to introduce Vincent agent, one of the patch agents running under our ensemble-based patching system.&lt;/p&gt;
&lt;h2 id="right-root-cause-wrong-patches"&gt;Right Root cause, Wrong Patches&lt;/h2&gt;
&lt;p&gt;What surprised us during the competition was that LLMs alone are already quite doing well at generating proper patches.
Given a sanitizer report, LLMs could freely explore the codebase by itself and reason correctly about the given bug—especially when the problematic code appeared near the call stacks in the report.&lt;/p&gt;</description></item><item><title>Ensembles of Agents for Robust and Effective Automated Patching</title><link>https://team-atlanta.github.io/blog/post-crs-patch-integration/</link><pubDate>Sun, 05 Oct 2025 11:00:00 +0000</pubDate><guid>https://team-atlanta.github.io/blog/post-crs-patch-integration/</guid><description>&lt;h2 id="why-ensemble-for-patching"&gt;Why Ensemble for Patching?&lt;/h2&gt;
&lt;p&gt;In the AIxCC competition, finding vulnerabilities is only half the battle. Once a vulnerability is discovered, it must be patched to prevent exploitation. This is where the Atlantis-Patching system comes into play. As the AIxCC&amp;rsquo;s ultimate mission is to make software secure, it awards more points for patching vulnerabilities than for finding them. In particular, the competition rewards &lt;strong&gt;6 points&lt;/strong&gt; for patching a vulnerability, compared to just 2 points for discovering it. As a result, to win the competition, it is crucial to have a robust and efficient patching system that can quickly generate effective patches for discovered vulnerabilities.&lt;/p&gt;</description></item></channel></rss>