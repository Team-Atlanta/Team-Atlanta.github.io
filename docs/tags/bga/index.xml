<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bga on Team Atlanta</title>
    <link>http://localhost:1313/tags/bga/</link>
    <description>Recent content in Bga on Team Atlanta</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 27 Aug 2025 21:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/bga/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>BGA: Self-Evolving Exploits Through Multi-Agent AI</title>
      <link>http://localhost:1313/blog/post-bga/</link>
      <pubDate>Wed, 27 Aug 2025 21:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/post-bga/</guid>
      <description>&lt;h2 id=&#34;why-programs-beat-payloads&#34;&gt;Why Programs Beat Payloads&lt;/h2&gt;&#xA;&lt;p&gt;Here&amp;rsquo;s the problem that changed everything: you need an exploit with exactly 1000 &amp;lsquo;A&amp;rsquo; characters followed by shellcode. Ask an LLM to generate it directly, and you might get 847 A&amp;rsquo;s, maybe 1203 A&amp;rsquo;s â€“ never quite right. But ask it to write &lt;code&gt;payload = &amp;quot;A&amp;quot; * 1000 + shellcode&lt;/code&gt;, and you get perfection every time.&lt;/p&gt;&#xA;&lt;p&gt;This insight sparked our breakthrough during the AIxCC competition. Instead of hoping AI could guess the right attack data, we taught it to write programs that create that data. The result? &lt;strong&gt;Seven unique vulnerabilities discovered&lt;/strong&gt; - exploits that evolved and adapted until they found their targets.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
