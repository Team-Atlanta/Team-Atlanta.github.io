[{
  "section": "Blog",
  "slug": "/blog/post-crs-java-libafl-jazzer/",
  "title": "Jazzer+LibAFL: Insights into Java Fuzzing",
  "description": "How we incorporated LibAFL as a new fuzzing backend for Jazzer",
  "date": "August 14, 2025",
  "image": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/crs-java/libafl-jazzer/jazzer_plus_libafl_hu_23bb1651f2271206.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"267\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/crs-java\\/libafl-jazzer\\/jazzer_plus_libafl_hu_f8cf71fdf029df74.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n",
  "imageSM": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/crs-java/libafl-jazzer/jazzer_plus_libafl_hu_a074aa50a3d82dc4.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/crs-java\\/libafl-jazzer\\/jazzer_plus_libafl_hu_d1bcde7796ef686e.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n",
  "searchKeyword": "",
  "categories": "Atlantis",
  "tags": "retrospective, java",
  "content":"AIxCC involved finding bugs in software written in two languages: C++ and Java. The focus of the competition was on the use of LLMs and AI, however, our teams approach was to balance ambitious strategies alongside proven traditional bug-finding techniques like fuzzing. While our team was deeply familiar with fuzzing C++ from decades of academic research and industry work, Java was uncharted territory for us. In part of our Java fuzzing development we created a fork of Jazzer that uses LibAFL as the fuzzing backend and it is available as part of our open source release. This post details some of the lessons we learned about Java fuzzing and the creation of this fork.\nDARPA chose Jazzer as their baseline fuzzer and sanitizer framework for Java challenges.\nJazzer Jazzer is an open-source Java Fuzzer developed by the Code Intelligence company. It makes use of LibFuzzer (written in C++) using the Java Native Interface (JNI). The architecture of Jazzer is roughly:\nr J J u T P a a n a r v z O r o a z n g g e e e r r ( t a ) m J J N N S I I a n C o v 1 2 f . . u U z p L z d L * _ a V f t t t M u a e e L F z r s i u z g t L b z i e O i C F z n t n b + u e g _ e F + z r r I u z R l u n z e u o n p z r n o n u e D p e t r r * r i . f v c e e p e r p d b a c k Jazzer begins by using the JNI to make a call to LLVMFuzzerRunDriver which is LibFuzzer\u0026rsquo;s recommended way of using it as a library. This starts the C++ fuzzing loop inside libFuzzer where Jazzer\u0026rsquo;s stub fuzz driver fuzz_target_runner.cpp implements a testOneInput method. This method is very simple and uses the JNI to call to a private static int runOne(long dataPtr, int dataLength) in Java.\nFrom here, Jazzer takes the void* input from LibFuzzer and converts it into the appropriate type before handing it off to the Java fuzzing entrypoint such as fuzzerTestOneInput(byte[] input).\nOn the Java side of things, Jazzer makes use of the JaCoCo code coverage library and ASM to inject instrumentation hooks into the program\u0026rsquo;s edges. These coverage tracking hooks insert a call to the recordCoverage(int id) method in CoverageMap.java. Jazzer here uses the UNSAFE.putByte function from sun.misc.Unsafe to directly write the edge into the coverage map memory location. LibFuzzer makes use of the LLVM Sanitizer Coverage (SanCov) API to receive coverage feedback. Jazzer hooks into this system by using the __sanitizer_cov_pcs_init method to set where in memory the coverage map is being stored.\nWhen control flow returns from the Java fuzzerTestOneInput program and flows back to the fuzzing loop inside LibFuzzer, it can now mutate the input and we can successfully fuzz a Java program.\nNote: This explanation glosses over details such as how Jazzer also instruments comparison functions and provides them to LibFuzzer for value-feedback based mutation.\nThe State of Jazzer and LibFuzzer Unfortunately, right as the AIxCC competition started, Code Intelligence announced that they had stopped maintaining Jazzer as an open-source project in favor of their commercial offerings. That change has since been reverted, however, Jazzer has not had any substantial new features or optimizations made to it since then.\nAdditionally, LibFuzzer, while it is a very mature and well-built fuzzer is also on maintenance mode. LibFuzzer was created by Kostya Serebryany under the LLVM umbrella when he was employed at Google but since then Google\u0026rsquo;s priorities have shifted. The LibFuzzer documentation notes:\nThe original authors of libFuzzer have stopped active work on it and switched to working on another fuzzing engine, Centipede. LibFuzzer is still fully supported in that important bugs will get fixed. However, please do not expect major new features or code reviews, other than for bug fixes.\nJust because Jazzer and LibFuzzer are in maintenance mode doesn\u0026rsquo;t mean the rest of the fuzzing community is. Projects like AFL++ have continued to incorporate ideas from research work and industry creating far more capable fuzzers.\nJazzer+LibAFL This brings us to one area we worked on: using LibAFL as the fuzzing engine for Jazzer instead of LibFuzzer. LibAFL is an awesome project that can be summarized as a fuzzer-library. Instead of an end-to-end fuzzer, you code the bits of glue that deliver your fuzzing payload and provide feedback and in return you get a fast performant fuzzer.\nImportantly for us, LibAFL contains a sub-project called libafl_libfuzzer. This is meant to be a drop-in replacement for LibFuzzer that can use harnesses and binaries built for LibFuzzer but fuzz them using LibAFL. This seemed like a great thing to try out for us to get the advanced features in LibAFL for free. As some of our past work like autofz has demonstrated, ensembling a bunch of different fuzzers with varying characteristics tends to yield great results when fuzzing.\nImplementation It wasn\u0026rsquo;t quite a drop-in replacement experience for us: it turned out that Jazzer actually used a fork for LibFuzzer with some changes made and libafl_libfuzzer wasn\u0026rsquo;t entirely feature-complete. However, a few days of integration left us with a Jazzer derivative that seemed to be able to explore code paths complimentary to the base fuzzer. Some of the notable changes we had to make are below:\nJazzer added a feature to LibFuzzer to allow the fuzzing loop to stop and return control to the caller of LLVMFuzzerRunDriver instead of killing the entire program.\nWe added the same feature in libafl_libfuzzer:\nlet result = unsafe { crate::libafl_libfuzzer_test_one_input(Some(*$harness), buf.as_ptr(), buf.len()) }; match result { -2 =\u0026gt; { // A special value from Jazzer indicating we should stop // the fuzzer but not kill the whole program. *stop_fuzzer.borrow_mut() = true; eprintln!(\u0026#34;[libafl] Received -3 from harness, setting stop.\u0026#34;); ExitKind::Crash } Sanitizers in C/C++ programs usually trigger signals to indicate an issue, such as AddressSanitizer (ASan) raising a SIGSEGV when it detects an error. Jazzer instead uses a method called __jazzer_set_death_callback to indicate a corpus triggered an issue in a sanitizer. We added this same function to our libafl_libfuzzer.\nAs mentioned previously, LibFuzzer uses SanCov to gather coverage information. This isn\u0026rsquo;t the only thing that SanCov provides though: in an effort to quickly find magic numbers like 0xdeadbeef when fuzzing, SanCov also hooks onto comparisons and calls methods like __sanitizer_cov_trace_cmp8 to indicate a comparison between two 8-byte numbers. This method is implemented like so in LibFuzzer:\nvoid __sanitizer_cov_trace_cmp8(uint64_t Arg1, uint64_t Arg2) { uintptr_t PC = reinterpret_cast\u0026lt;uintptr_t\u0026gt;(GET_CALLER_PC()); fuzzer::TPC.HandleCmp(PC, Arg1, Arg2); } Notice that it uses a macro to retrieve the calling program counter. If Jazzer were to use these methods from the JNI directly, they would all register with the same program counter. Hence Jazzer adds variants of these methods such as __sanitizer_cov_trace_cmp8_with_pc that pass the program counter.\nWe implemented these same _with_pc SanCov functions.\nLibFuzzer also gathers data on comparisons performed in strcmp, memcmp and other common libc functions to find magic strings. This is done by intercepting calls to these methods in FuzzerInterceptors.cpp:\nstatic void fuzzerInit() { ... REAL(memcmp) = reinterpret_cast\u0026lt;memcmp_type\u0026gt;( getFuncAddr(\u0026#34;memcmp\u0026#34;, reinterpret_cast\u0026lt;uintptr_t\u0026gt;(\u0026amp;memcmp))); ... } ATTRIBUTE_INTERFACE int memcmp(const void *s1, const void *s2, size_t n) { int result = REAL(memcmp)(s1, s2, n); void *caller_pc = GET_CALLER_PC(); __sanitizer_weak_hook_memcmp(caller_pc, s1, s2, n, result); return result; } and then sending the arguments and result to functions like __sanitizer_weak_hook_memcmp. Here we encountered two issues, libafl_libfuzzer lacked implementations for __sanitizer_weak_hook_memmem and __sanitizer_weak_hook_strstr. We added those two methods.\nAdditionally, Jazzer had implemented a custom hook function called __sanitizer_weak_hook_compare_bytes which we also had to implement.\nThere were also many other smaller changes such as making the libafl_libfuzzer crash filenames match the filename that LibFuzzer uses. We are thankful to the Jazzer team for having such a thorough set of unit tests and integration tests that allowed us to be confident our fork of Jazzer would work.\nThe Bugs! During this process we found a few bugs in the libafl_libfuzzer drop-in replacement. We fixed some of these locally and reported them upstream wherever we could.\nA build issue had caused the function interceptor hooks like __sanitizer_weak_hook_memcmp to become dead. This meant that these hooked functions were just silently never getting called reducing the feedback the fuzzer had to work with.\nhttps://github.com/AFLplusplus/LibAFL/issues/3043\nThe calls for constant comparisons such as __sanitizer_cov_trace_cmp8 to represent 8-byte integer comparison had an incorrect macro implementation causing all comparisons to be considered as 1-byte.\nhttps://github.com/AFLplusplus/LibAFL/issues/3094\nlibafl_libfuzzer is sometimes unable to solve some simple harnesses because its memory-comparison hooks do not provide feedback on how close the values being compared are.\nhttps://github.com/AFLplusplus/LibAFL/issues/3042\nWe reported this bug upstream but did not contribute our fix because it was a little hacky.\n"},{
  "section": "Blog",
  "slug": "/blog/post-atl-infra/",
  "title": "Atlantis Infrastructure",
  "description": "Beginning",
  "date": "August 13, 2025",
  "image": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/atl-infra/infra_hu_66fae50b5b3588d3.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"420\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/atl-infra\\/infra_hu_c3d73c1b5704794c.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n",
  "imageSM": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/atl-infra/infra_hu_861da9d828610e3f.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/atl-infra\\/infra_hu_2920da624ffa59bd.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n",
  "searchKeyword": "",
  "categories": "Atlantis",
  "tags": "infrastructure",
  "content":"The AIxCC competition is not just about creating automated bug-finding and patching techniques \u0026ndash; it is about building a cyber reasoning system (CRS) that can do both without any human assistance. To succeed, a CRS must excel in three critical infrastructure areas:\nReliability: Run continuously for weeks without intervention. Scalability: Handle many challenge projects concurrently. Budget Utilization: Maximize Azure cloud and LLM credit usage. Submission Management: Consistently deliver valid POVs, Patches, SARIF assessments, and Bundles. In this post, we will share how we designed the infrastructure of our CRS, Atlantis, to meet these keys and make it as robust as possible. We could not have won AIxCC without the exceptional work of our infrastructure team.\n1. Bootstrapping Atlantis starts by launching CRS-level k8s nodes for four essential services:\nCRS Webserver: Listens for incoming challenge projects (CPs) and SARIF reports. Log Collector: Aggregates and forwards logs to the AIxCC organizers. LiteLLM: Manages LLM usage within budget and logs all requests/responses. Custom LLM: Helps patch generation based on fine-tuned LLM. 2. Per-Challenge Scaling When a new CP arrives, the CRS webserver spins up a dedicated CP manager on its own k8s node. The CP manager then:\nBuilds the CP and allocates both Azure Cloud and LLM budgets proportionally for this CP.\nLaunches bug-finding modules depending on the language:\nC-based CPs =\u0026gt; Atlantis-Multilang + Atlantis-C Java-based CPs =\u0026gt; Atlantis-Multilang + Atlantis-Java Launches Atlantis-Patch and Atlantis-SARIF for patch generation and SARIF assessment.\nIssues LiteLLM API keys to modules to enforce per-CP and per-module LLM budgets.\nThis per-challenge scaling significantly boosts reliability \u0026ndash; if crash while handling one CP, it does not affect others. Redundant bug-finding modules further improve stability and coverage; for example, if Atlantis-C fails on certain CPs, Atlantis-Multilang can still find vulnerabilities in them.\nAs a result, during the final competition,\n73% of submitted POVs come from Atlantis-Multilang 15% from Atlantis-C 12% from Atlantis-Java Notably, 81% of all POVs were found in C-based CPs and 19% in Java-based CPs.\nIn addition, for each harness in a CP, Atlantis-Multilang and Atlantis-Java run on nodes sized according to the allocated Azure budget, while Atlantis-C operates on a fixed pool of up to 15 nodes. This isolation ensures that even if a module fails on one harness, it does not impact the others.\nThis per-challenge scaling also enabled us to make full use of the allocated Azure Cloud and LLM credit budgets. In the final round, as shown below, we recorded the highest usage among all seven teams \u0026ndash; spending $73.9K of the $85K Azure Cloud budget and $29.4K of the $50K LLM credit budget.\nNotably, the above image and the AIxCC organizers show that we primarily used o4-mini, GPT-4o, and o3. However, these are based on the number of requests rather than the actual LLM credits spent in dollars. Based on our experience from the practice and internal rounds, we allocated LLM budgets in the final round as shown below. While our plan was to rely mostly on Anthropic and OpenAI models in the final round, we do not yet have precise data on how much we spent for each provider.\n+--------------------+-----------+------------+----------+--------+-----------+ | CRS | OpenAI | Anthropic | Gemini | Grok | Total | +--------------------+-----------+------------+----------+--------+-----------+ | Atlantis-multilang | 0.48 | 0.48 | 0.05 | 0.00 | 1.00 | | Atlantis-C/java | 0.40 | 0.30 | 0.30 | 0.00 | 1.00 | | Atlantis-patch | 0.30 | 0.60 | 0.10 | 0.00 | 1.00 | +--------------------+-----------+------------+----------+--------+-----------+ | Atlantis-multilang | $8839.29 | $8839.29 | $883.93 | $0.00 | $18562.50 | | Atlantis-C/java | $7425.00 | $5568.76 | $5568.76 | $0.00 | $18562.50 | | Atlantis-patch | $3712.50 | $7425.00 | $1237.50 | $0.00 | $12375.00 | | Atlantis-sarif | $75.00 | $100.00 | $250.00 | $75.00 | $500.00 | +--------------------+-----------+------------+----------+--------+-----------+ | Total | $20051.79 | $21933.04 | $7940.18 | $75.00 | $50000.00 | +--------------------+-----------+------------+----------+--------+-----------+ 3. Submission and Task Management When a bug-finding module discovers a POV, it sends the result to the CP Manager. The CP Manager then:\nVerifies that the POV indeed triggers a crash. Deduplicates POVs based on their stack traces and heuristics. Submit unique POVs to the AIxCC organiziers. Forwards unique POVs to Atlantis-Patch and Atlantis-SARIF for patch generation and SARIF assessment. Once patches and SARIF reports are produced, they are returned to the CP Manager and CP Manager submits them to the AIxCC organizers. At the end, the CP Manager groups each POV with its corresponding patch and assessment (based on the POV hash) into a bundle and submits it to the AIxCC organizers.\nAs a result, we were able to successfully submit numerous POVs, patches, SARIF assessments, and bundles with high accuracy in the final round and ultimately won the competition, as shown below. Notably, our bundle score was significantly higher than other teams, even when accounting for the large number of POVs we found and patches we generated. This demonstrates that Atlantis was able to effectively map the relationships between discovered POVs, generated patches, and SARIF assessments \u0026ndash; a capability that can be incredibly valuable for real-world developers.\nTesting, Testing, and Testing! While developing Atlantis, we conducted extensive testing to fix bugs and evaluate the effectiveness of each module. Under the leadership of Jiho Kim, we prepared over 50 CP benchmarks and tested Atlantis against them using a test version of AIxCC competition server. This allowed us to perform end-to-end testing not only in the three practice rounds provided by the AIxCC organizers, but also in four additional internal rounds. Across these seven rounds, we identified and fixed numerous bugs, ultimately making Atlantis far more robust. Notably, some modules like Atlantis-Multilang has their own CIs to test and evaluate themselves based on our benchmarks. We plan to release our benchmarks once we determine a way to prevent GenAI models from training on them.\nFixed a fatal bug right before the deadline However, even with extensive testing, we failed to catch a fatal bug in Atlantis-Patch, described in our previous post, until the final moments before submission. The bug was related to enforcing a rule that Atlantis-Patch must not modify the given fuzzing harnesses. Our implementation treated any file whose path contained fuzz as a fuzzing harness and blocked modifications accordingly. Everything worked fine in our internal tests. But, during the final testing with the AIxCC organizers’ API right before submission, we discovered that all CPs had their directories prefixed with ossfuzz. As a result, Atlantis-Patch refused to generate any patches. Initially, I suspected the issue was due to the non-deterministic nature of LLMs. However, Soyeon spotted unusual log entries with Permission denied, revealing that the patch process was blocked because it attempted to modify a fuzzing harness. This was discovered just a few hours before the final submission. I urgently called our Atlantis-Patch teammates in South Korea at 3 AM their time. Fortunately, we fixed the issue within an hour and managed to submit Atlantis before the deadline.\nFrom the Author After the Last Commit Since no human intervention was allowed during the competition, we spent a significant amount of time on infrastructure development and testing. All of infrastructure team members not only worked on their assigned roles (e.g., bug finding, SARIF assessment) but also contributed to infra development and testing on the side. I am deeply grateful to them, and this experience really helped me understand why companies have dedicated DevOps teams. In particular, I have a feeling that as LLMs and GenAI become more widely adopted, specialized LLM DevOps teams will also emerge. I will never forget the times when our LiteLLM had to be rebooted frequently because it could not keep up with requests from our modules. Overall, it was an incredible experience to go beyond a research prototype and operate such a large-scale system in a real competition while collaborating and communicating closely with all the sub-teams (Atlantis-C/Java/Multilang/SARIF/Patch) throughout the journey.\n"},{
  "section": "Blog",
  "slug": "/blog/post-afc/",
  "title": "AIxCC Final and Team Atlanta",
  "description": "Atlantis in CTF competitions",
  "date": "August 12, 2025",
  "image": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/afc/afc-team_hu_1e8c562fabab95ab.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"194\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/afc\\/afc-team_hu_2f9a4e9f5ef2a770.jpeg';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n",
  "imageSM": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/afc/afc-team_hu_6fe684af5f1e3bb0.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/afc\\/afc-team_hu_6bcf2ba292725a9e.jpeg';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n",
  "searchKeyword": "",
  "categories": "Milestone",
  "tags": "AFC",
  "content":"Two years after its first announcement at DEF CON 31, our team stood on stage as the winners of the AIxCC Final—a moment we had been working toward since the competition began.\nYet when we heard we placed 1st, relief overshadowed excitement. Why? While competing head-to-head with world-class teams like Theori was a privilege, the real-time, long-running nature of this competition demanded extreme engineering reliability alongside novel approaches to succeed.\nBalancing innovation with stability under time pressure proved our greatest challenge. We simply hoped our Cyber Reasoning System (CRS) would run as intended— but it exceeded expectations, outperforming other teams in most categories by significant margins.\nIn this post, I\u0026rsquo;ll answer the most common questions we received from the DEF CON audience and share the story behind our victory.\nWhy were we so anxious? In this competition, a single bug can be fatal. One line of code nearly destroyed our chances.\nWe discovered this critical bug, which almost ended everything, during integration testing— just hours after submitting our final version, and mere hours before the deadline. The problematic code was designed to skip patch generation for fuzzing harnesses. In all previous challenge projects and our benchmarks, fuzzing harness source files contained \u0026ldquo;fuzz\u0026rdquo; in their paths (e.g., \u0026ldquo;fuzz/\u0026rdquo; or \u0026ldquo;http_request_fuzzer.cc\u0026rdquo; in nginx)— a simple but effective heuristic to avoid false positives.\nThe problem? During our final integration test, we discovered the organizers had prefixed all OSS-Fuzz projects with \u0026ldquo;ossfuzz\u0026rdquo; (e.g., \u0026ldquo;r3-ossfuzz-sqlite3\u0026rdquo;). The irony wasn\u0026rsquo;t lost on us—here we were, building an autonomous CRS powered by state-of-the-art AI, nearly defeated by a string matching bug. In postmortem, we figured none of the CP is named with the \u0026ldquo;fuzz\u0026rdquo; prefix though!\nL0. System Robustness is Priority #1 As our near-miss demonstrated, a single bug can kill a CRS entirely. The autonomous system is that brittle. So how did we balance engineering for robustness with novel research needed to win?\nOur answer: N-version programming with orthogonal approaches.\nAtlantis isn\u0026rsquo;t a single CRS—it\u0026rsquo;s a group of multiple independent CRSs, each designed by specialized teams (C, Java, Multilang, Patch, and SARIF). These teams deliberately pursued orthogonal strategies to maximize both coverage and fault tolerance.\nFor bug finding alone, we deployed three distinct CRSs:\nAtlantis-Multilang: Built for robustness and language-agnostic bug finding Atlantis-C: Optimized specifically for C/C++ vulnerabilities Atlantis-Java: Tailored for Java-specific bug patterns Design Overview of Atlantis (stay tuned for our Technical Report).\rThese CRSs deliberately made orthogonal approaches; Atlantis-Multilang took conservative paths (no instrumentation at build time) while Atlantis-C took risky approaches requiring heavy compilation-time instrumentation:\nAtlantis-C CRS: ↑ instrumentation: libafl-based, instrument-based directed fuzzer ↓ LLM usage: seed generation, input mutation Time-based resource allocation Atlantis-Multilang CRS: ↓ instrumentation: libfuzzer-based, simple seed-based directed fuzzer ↑ LLM: seed/blob generation, input format reverser, callgraph, dictgen, etc Space-based resource allocation By maintaining minimal sharing between CRSs and intentionally making orthogonal design decisions, we ensured that a failure in one component wouldn\u0026rsquo;t cascade through the entire system. When one approach failed, others continued operating— true fault tolerance through diversity.\nL1. Don\u0026rsquo;t Give Up on Traditional Program Analysis Unlike DARPA\u0026rsquo;s Cyber Grand Challenge, where CRSs dealt with an artificial architecture limited to 7 system calls, AIxCC evaluates CRSs against real-world, complex open source software— the foundation of today\u0026rsquo;s cyber infrastructure.\nThis shift changes everything. Traditional program analysis tools that can\u0026rsquo;t scale to handle real-world complexity would doom any CRS.\nWe initially hoped to stand on the shoulders of giants, evaluating most commodity solutions to save development time. Unfortunately, even state-of-the-art tools like SWAT and SymCC weren\u0026rsquo;t ready for large-scale software analysis. Each required substantial engineering to become competition-ready.\nUltimately, we invested heavily in extending traditional tools. For both C and Java, we developed three categories:\nEnsemble fuzzers: LibAFL for Java/C, libFuzzer, AFL++, custom Jazzer, custom format fuzzers Concolic executors: Extended SymCC for C, custom implementation for Java Directed fuzzers: Custom implementations for C and Java Each tool required non-trivial engineering efforts to be effective. The lesson: AI alone isn\u0026rsquo;t enough—traditional program analysis remains essential, but it must be extensively adapted for real-world scale.\nL2. Ensembling to Promote Diversity Research shows that ensemble fuzzing outperforms single campaigns with equivalent computing resources, as demonstrated by autofz. Atlantis embraces this principle everywhere: coverage-guided fuzzers, directed fuzzers, concolic executors, and patching agents.\nPatching particularly benefits from LLM diversity— what the ML community calls \u0026ldquo;hallucination,\u0026rdquo; systems engineers call \u0026ldquo;non-determinism,\u0026rdquo; and we call \u0026ldquo;creativity.\u0026rdquo; By ensembling multiple agents with orthogonal approaches, Atlantis harnesses this non-deterministic nature of LLMs.\nThe Critical Role of Oracles. Ensembling only works when oracles exist to judge correctness.\nIn fuzzing, hardware provides our first oracle: segmentation faults from invalid memory access are caught efficiently through page table violations. Software sanitizers extend this scope— ASAN for memory unsafety bugs, UBSAN for undefined behavior, MSAN for memory leaks— detecting bugs long before crashes occur.\nFor patching, the Proof-of-Vulnerability (PoV) serves as our oracle. We validate patches by re-running the PoV against patched programs. We say \u0026ldquo;likely correct\u0026rdquo; because patches might work through unintended mitigation rather than true fixes.\nConsider these problematic \u0026ldquo;patches\u0026rdquo;:\nRecompiling C code with MTE or PAC on ARM to suppress PoVs Wrapping Java entry points in broad catch(Exception) blocks Our agents carefully avoid such mitigations. Yet semantic correctness remains subjective— which is why AIxCC provides optional test.sh scripts as additional oracles for our patching agents.\nDesign of Patching Agents.\rBuilding Specialized Agents. During preparation, we recognized a key insight: building one universally powerful agent is harder than building multiple specialized agents for specific tasks. This echoes the philosophy behind AlphaEvolve and AlphaCode.\nSurprisingly, smaller models like GPT-4o-mini often outperformed larger foundation models and even reasoning models for our tasks. We speculate that its 8 billion parameters hit a sweet spot— large enough to understand code patterns, small enough to avoid overthinking simple fixes.\nPractical Constraints on Scaling. Unlike AlphaCode\u0026rsquo;s massive agent scaling, we faced a practical bottleneck: validating patches in large codebases takes minutes if not hours (e.g., 10+ minutes for nginx). This forced Atlantis-Patching to limit itself to six agents, focusing on quality over quantity.\nTheori took a radically different approach: purely static analysis, producing three correct patches without PoVs. This demonstrates LLMs\u0026rsquo; remarkable ability to understand code semantics without runtime validation, which we\u0026rsquo;d like to explore further.\nThe scoreboard reveals the trade-off: Theori\u0026rsquo;s 44.4% accuracy yielded an Accuracy Modifier of 0.9044 ($1 - (1 - 0.4444)^4$), while our PoV-validated approach achieved 0.9999 ($1 - (1 - 0.9127)^4$).\nOur CRS can generate patches without PoVs, but we deliberately chose not to—a strategic decision we debated extensively and validated through our internal benchmark.\nPost-competition, we\u0026rsquo;re excited to explore PoV-free patching\u0026rsquo;s full potential.\nL3. LLM 101: How to Babysit Jack-Jack? During our CTFRadio interview, Yan mentioned that Shellfish had to babysit LLMs for their agents. The analogy resonates: LLMs are like Jack-Jack Parr from Incredibles— a superpowered baby with multiple, unpredictable abilities that even his superhero parents don\u0026rsquo;t fully understand.\nLike Jack-Jack, LLMs have not one superpower but many, and we\u0026rsquo;re still discovering how to harness them effectively. We \u0026ldquo;gaslight\u0026rdquo; our LLMs into specific roles, telling them they\u0026rsquo;re \u0026ldquo;security researchers\u0026rdquo; or even researchers from Google DeepMind.\nThe Evolution of Prompting Techniques. Throughout the competition, we witnessed firsthand the rapid evolution of foundation models and prompting strategies. Early tricks like \u0026ldquo;I\u0026rsquo;ll give you a $200 tip\u0026rdquo; surprisingly generated longer, more detailed responses. Techniques multiplied: Chain-of-Thought (CoT), Tree-of-Thoughts (ToT), Self-Consistency (SC).\nWe tested everything and integrated what worked, like \u0026ldquo;Think step by step\u0026rdquo; prompts. Agentic architectures evolved in parallel: ReAct, Reflection, tool use, multi-agent systems, sub-agents— we adopted many (as shown above).\nManaging Rapid Change. The pace of change in the LLM space is unprecedented. Every vendor claims benchmark supremacy, making it impossible to evaluate every new claim or technique.\nOur solution: continuous empirical testing. We evaluate performance daily through CI using our internal benchmark, monitoring for sudden drops or improvements. (Shellfish even built an LLM agent specifically for this task!)\nTo avoid vendor lock-in, we built abstraction layers. LiteLLM serves as our proxy, multiplexing requests and responses across different LLM providers for each agent.\nHandling External Dependencies. Since LLMs are externally managed services, Atlantis must handle various failure modes:\nToken limits exceeded Daily/subscription quotas hit Unexplained downtime or delays We experienced all of these during the exhibition rounds and built resilience mechanisms accordingly.\nL4. LLM-Augmented, LLM-Opinionated, and LLM-Driven Atlantis employs LLMs through three distinct integration strategies, each with different levels of trust and autonomy.\nLLM-Augmented: Extending Traditional Tool. In this approach, LLMs enhance traditional analysis techniques where conventional methods struggle with scale. Fuzzing tools integrate LLMs for:\nInput generation Dictionary generation Seed generation Here, LLMs fill gaps where traditional techniques fail to scale to real-world software complexity.\nLLM-Opinionated: Optimistic Suggestions. Tools like Testlang and Harness Reverser operate with calculated risk. LLMs provide likelihood-based suggestions that workflows treat as hints—similar to optimistic concurrency control.\nWhen predictions are correct, the system benefits significantly. When wrong, we pay a performance penalty but maintain correctness.\nLLM-Driven: Autonomous Navigation. Our most ambitious approach gives LLMs full autonomy. The MLLA agent and POC Gen in Java CRS autonomously navigate code repositories, generating \u0026ldquo;blobs\u0026rdquo;—inputs designed to trigger identified bugs from diffs or SARIF reports.\nThis strategy bets on LLMs having latent security intuition buried in their weights, allowing them to reason about entire codebases independently.\nSo How Well Did Atlantis Perform? Atlantis dominated the scoreboard, earning top scores in nearly every category. Remarkably, we accumulated roughly the same total points as the second and third place teams combined.\nWhile we\u0026rsquo;re still analyzing the complete dataset, early observations suggest our CRS excelled on certain challenge projects (like Wireshark) where other teams struggled. Our conservative strategy proved decisive: high accuracy in crash reports and patches yielded a near-perfect accuracy multiplier, while our strong bundle scores validated our careful approach to matching PoVs with patches and SARIF reports.\nReal-World Bugs. Does this approach work in the wild? During the final, all competing CRSs collectively discovered 6 C/C++ bugs and 12 Java bugs in real-world software. Atlantis contributed 3 of each category, including a 0-day vulnerability in SQLite discovered during the semi-final.\nWhat\u0026rsquo;s Next? For a casual discussion of our journey and lessons learned, check out our CTFRadio interview:\nOpen Source Release. Our competition CRS code is publicly available, but the current system requires substantial infrastructure: Microsoft Azure deployment, Terraform, Kubernetes, Tailscale, and external LLM service dependencies.\nTo make Atlantis accessible to the broader community, we\u0026rsquo;re creating a streamlined fork that:\nRemoves competition-specific APIs Runs on a single workstation via Docker Compose Includes a revised benchmark suite for standardized evaluation Call for Collaboration. We\u0026rsquo;re launching continuous bug hunting on OSS-Fuzz projects. To sustain this effort, Team Atlanta is donating $2.0M (50% of our prize) to SSLab at Georgia Tech for:\nOngoing research in autonomous security systems with LLM Expenses to continuously run Atlantis to open source projects Scholarship to PhD students and postdocs Join us in advancing autonomous security research! And we are seeking funding for public research \u0026ndash; OpenAI joined this effort to make donation to us along with the API credits.\nComing Soon.\nTechnical Report: Detailed system architecture and findings (releasing in two weeks) Blog Series: Deep dives into specific CRS components and strategies Postmortem: Analysis of the final competition data and effectiveness of each techniques/CRSs "},{
  "section": "Blog",
  "slug": "/blog/post-tkctf-2024/",
  "title": "Hacking Redefined: How LLM Agents Took on University Hacking Competition",
  "description": "Atlantis in CTF competitions",
  "date": "December 3, 2024",
  "image": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/tkctf2024/ai-vs-human_hu_1cca12a5e59caa7e.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"315\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/tkctf2024\\/ai-vs-human_hu_b2d4288e091735d0.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n",
  "imageSM": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/tkctf2024/ai-vs-human_hu_e893e5f2759e3f4b.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/tkctf2024\\/ai-vs-human_hu_4deee4d9fabe84eb.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n",
  "searchKeyword": "",
  "categories": "Milestone",
  "tags": "Atlantis CRS",
  "content":"For the first time, we deployed our hybrid system, powered by LLM agents—Atlantis—to compete in Georgia Tech’s flagship CTF event, TKCTF 2024. During the competition, Atlantis concentrated on two pivotal areas: vulnerability analysis and automatic vulnerability remediation. Remarkably, the system uncovered 10 vulnerabilities and produced 7 robust patches1, showcasing the practicality and promise of our approach in a real-world hacking competition.\nIn this blog, I’ll delve into some fascinating insights and essential lessons from the CTF experience. As we prepare to open-source the full details of our system following AIxCC competition rules, this milestone reflects more than just a technical achievement—it embodies our commitment to advancing LLM-driven security research.\nWhat might appear as another incremental step in CTF competitions actually marks a significant leap in our journey. The success of Atlantis at TKCTF was a testament to the dedication and expertise of our exceptional team: Andrew Chin, Jiho Kim, Gyejin Lee, Seunggi Min, Kevin Stevens, Woosun Song, and Hanqing Zhao,\nWe invite you to follow us on Twitter/X (@TeamAtlanta24) to stay updated on our work at the intersection of AI and security.\nAIxCC, Atlantis CRS, and CTF AIxCC Finalists\rAI Cyber Challenge (AIxCC) is a two-year competition designed to advance the state of the art in AI-based security research. We, Team Atlanta, are proud to be one of the 7 finalist teams, presenting a novel cyber reasoning system, Atlantis CRS.\nThe concept behind our system is straightforward: emulate the mindset of skilled security researchers and hackers using LLM agents, augmented by advanced program analysis techniques.\nAs seasoned veterans of CTF competitions, we’ve always aspired to bring AI and LLMs into the CTF arena. This time, we deployed our system alongside human players, concentrating on analyzing source code repositories and patching identified vulnerabilities. Georgia Tech’s CTF competition, TKCTF, offered the perfect setting to put our system to the test.\nThe challenges were designed by students and staff from Georgia Tech’s CS6265 Fall 2024 class. In addition to Georgia Tech participants, teams from across the University System of Georgia (USG) joined the competition. The winning team received a $1,000 prize.\nRecap of Atlantis\u0026rsquo;s Performance Atlantis Dashboard\rOver the course of the competition, we ran Atlantis CRS on 12 different CTF challenges, each with a time limit of approximately 4 hours. By the end, our system successfully identified 10 vulnerabilities and generated 7 robust patches.\nWhile the overall difficulty of the challenges was moderate, Atlantis successfully identified most of the intended memory safety vulnerabilities and even uncovered two unintended vulnerabilities. The competition also provided several interesting cases and valuable lessons, which I’ll share below.\nMaze Puzzles One of the challenges was a maze puzzle. It generated a random maze where the intended solution required players to write an algorithm to parse the map and find the path through the maze.\nClick to view an example of the maze puzzle ./target ######################################### #P* * * * * * # # *** ***** * *** * *** *** * ******* * # # * * * * * * * * * # #** ***** ***** * * * * *********** *** # # * * * * * * * * * * # # *** * *** ***** *** *** * * * ******* # # * * * * * * * * * # # *** *** * * * *** *** * * ************# # * * * * * * * * * * # #****** * ******* *** *** ********* * * # # * * * * * * * * * # # * ******* * ***** *** * * ******* *** # # * * * * G# ######################################### Steps: 0 ========= # another run ./target ######################################### #P * * * * * * * # #** * * * *** * *** ***** * * ***** * * # # * * * * * * * * * * * * * # # *** * * * *** * *** * * * * * * ***** # # * * * * * * * * * * * * # #****** * ******* * * ******* * *** * **# # * * * * * * * * * # # * *** * * ********* * *** *** * * *** # # * * * * * * * * * # # ******* ***** * * * * * ******* * *** # # * * * * * * * * * * * # # * *** *** * *** * ******* *** ***** * # # * * * * * *G# ######################################### Steps: 0 ========= Interestingly, Atlantis correctly navigated the maze through several runs of trial and error. This demonstrated the ability of LLM agents to enhance program analysis by “understanding” the high-level intent of a program.\nDiscovering Unintended Vulnerabilities Beyond the intended vulnerabilities, Atlantis also discovered two unintended ones:\nFloating Point Exception (FPE): A vulnerability causing stack underflow. Insufficient Input Validation: An unchecked empty input led to a stack buffer overflow. Although the challenges were not overly complex, this highlights Atlantis’s potential in automated testing and vulnerability discovery.\nUndiscovered Vulnerabilities Due to Logic Flaws Atlantis struggled with challenges involving logical flaws. For example, in the two-sum challenge:\nChallenge Details: The task didn’t involve memory safety issues but required exploiting unordered_map hash collisions to manipulate the control flow in the twoSum function. Missed Opportunity: Since Atlantis was tuned to prioritize memory safety issues, it overlooked this logic-based vulnerability. This underscored the need to enhance Atlantis’s capabilities in analyzing logical vulnerabilities, an area we plan to focus on in future improvements.\nCase Studies: How Atlantis Patches Vulnerabilities A standout feature of Atlantis is its capability to generate patches for discovered vulnerabilities. In this CTF, Atlantis produced 7 fully sound patches, showcasing both the system’s confidence and its ease in handling the challenges, which were relatively straightforward.\nIn this section, I’ll provide a summary of key cases and the patching decisions made by Atlantis’s agent system.\nUnsafe functions replaced by safer ones diff --git a/double_trouble.c b/double_trouble.c index 83a693b..e8f0737 100644 --- a/double_trouble.c +++ b/double_trouble.c @@ -5,8 +5,9 @@ void foo(char *msg) { char buf[80]; - strcpy(buf, msg); // Vulnerable: No bounds checking - strcpy(msg, buf); // Vulnerable: No bounds checking + strncpy(buf, msg, sizeof(buf) - 1); // Use strncpy for bounds checking + buf[sizeof(buf) - 1] = \u0026#39;\\0\u0026#39;; // Ensure null termination + strncpy(msg, buf, strlen(msg)); // Use strncpy for bounds checking } One of the principles guiding Atlantis is avoiding the use of unsafe functions. For instance, in one case, Atlantis replaced strcpy with strncpy and added a null termination check to ensure the safety of string operations. Given that the C language inherently includes many unsafe functions, Atlantis takes a conservative approach by systematically replacing these functions with their safer counterparts wherever possible.\nBounds checking and termination handling diff --git a/target.c b/target.c index f512754..7f7bfef 100644 --- a/target.c +++ b/target.c @@ -23,9 +23,16 @@ func run_at_end = dont_print_key; uint32_t read_int(void) { char buf[10]; ssize_t size = read(0, buf, sizeof(buf) - 1); + if (size \u0026lt;= 0) { + if (size == 0) { + printf(\u0026#34;Error: End of input stream\\n\u0026#34;); + } else { + perror(\u0026#34;Error: Read failed\u0026#34;); + } + return 0; // or handle error as needed + } buf[size] = \u0026#39;\\0\u0026#39;; uint32_t res = atoi(buf); - return res; } diff --git a/ymca.c b/ymca.c index 5bf7de7..ca10ea1 100644 --- a/ymca.c +++ b/ymca.c @@ -87,7 +87,10 @@ bool is_in_array(int step, int arr[], int size) { void vulnerable() { char buffer[64]; printf(\u0026#34;Enter your Steps of the YMCA Song:\\n\u0026#34;); - read(STDIN_FILENO, buffer, 1000); + ssize_t bytes_read = read(STDIN_FILENO, buffer, sizeof(buffer) - 1); + if (bytes_read \u0026gt; 0) { + buffer[bytes_read] = \u0026#39;\\0\u0026#39;; // Null-terminate the buffer + } } diff --git a/rop.c b/rop.c index 5df7c9c..c858436 100644 --- a/rop.c +++ b/rop.c @@ -83,7 +83,7 @@ int main(int argc, char** argv) { fgets(buf, sizeof(buf), stdin); // make newline null terminator int input_len = strlen(buf); - if (buf[input_len - 1] == \u0026#39;\\n\u0026#39;) { + if (input_len \u0026gt; 0 \u0026amp;\u0026amp; buf[input_len - 1] == \u0026#39;\\n\u0026#39;) { buf[input_len - 1] = \u0026#39;\\0\u0026#39;; } Out-of-bounds vulnerabilities are among the most common issues in software. Atlantis addresses this by employing dedicated optimizations to thoroughly check array and memory bounds, ensuring that user inputs are constrained within a safe range. This proactive approach minimizes the risk of unintended memory access and enhances overall software reliability.\nSafe rewrite of the entire function diff --git a/target.c b/target.c index c364280..712ac08 100755 --- a/target.c +++ b/target.c @@ -14,25 +14,27 @@ int null_pos = -1; int newline_pos = -1; void copy_buffer(unsigned char* buf) { -\tnull_pos = -1; -\tnewline_pos = -1; -\tint i = 0; -\twhile (i \u0026lt; BUF_SIZE) { -\tif (newline_pos \u0026lt; 0 \u0026amp;\u0026amp; gbuf[i] == 0xa) { -\tnewline_pos = i; -\t} -\tif (null_pos \u0026lt; 0 \u0026amp;\u0026amp; gbuf[i] == 0x0) { -\tnull_pos = i; -\t} -\tgbuf[i] = (gbuf[i] ^ key[i]) \u0026amp; 0xff; -\ti++; -\t} -\tint j = 0; -\twhile (j \u0026lt; min(null_pos, newline_pos)) { -\tbuf[j] = gbuf[j]; -\tj++; -\t} -\tprintf(\u0026#34;Copied %d of %d bytes!\\n\u0026#34;, min(null_pos, newline_pos), strlen(gbuf)); + null_pos = BUF_SIZE; + newline_pos = BUF_SIZE; + int i = 0; + while (i \u0026lt; BUF_SIZE) { + if (newline_pos == BUF_SIZE \u0026amp;\u0026amp; gbuf[i] == 0xa) { + newline_pos = i; + } + if (null_pos == BUF_SIZE \u0026amp;\u0026amp; gbuf[i] == 0x0) { + null_pos = i; + } + gbuf[i] = (gbuf[i] ^ key[i]) \u0026amp; 0xff; + i++; + } + int j = 0; + int copy_len = min(null_pos, newline_pos); + copy_len = min(copy_len, 20); // Ensure we don\u0026#39;t exceed buf size + while (j \u0026lt; copy_len) { + buf[j] = gbuf[j]; + j++; + } + printf(\u0026#34;Copied %d of %d bytes!\\n\u0026#34;, copy_len, BUF_SIZE); } Atlantis typically aims to keep patches minimal, modifying the fewest possible lines to reduce the risk of altering intended code behavior or introducing unsoundness. However, after several iterations, if Atlantis cannot produce an optimal minimal patch, it will optionally rewrite the entire function to comprehensively address the vulnerability. This approach balances precision with robustness, ensuring both safety and functionality.\nError handling correction diff --git a/stackmachine.c b/stackmachine.c index 7b95b62..d00d75f 100644 --- a/stackmachine.c +++ b/stackmachine.c @@ -88,11 +88,11 @@ void push(int *values, int *idx, int elt) { int pop(const int *values, int *idx) { if (*idx \u0026lt;= START) { myerrno = -1; - return 0; + return INT_MIN; // Return a special error code } (*idx)--; - int ret = values[*idx]; - return ret; + return values[*idx]; +} } void main_loop(); @@ -144,9 +144,13 @@ void print_utf8(wchar_t *wcs) { */ void ret(int *values, int *idx) { r1 = pop(values, idx); - if (myerrno != 0) { return; } + if (myerrno != 0 || r1 == INT_MIN) { + puts(\u0026#34;Error: Attempted to pop from an empty stack.\u0026#34;); + return; + } printf(\u0026#34;%d\\n\u0026#34;, r1); } +} For bugs involving inter-procedural logic, Atlantis analyzes cross-procedural data flow and incorporates error handling into its patches. For instance, when addressing a vulnerability in stackmachine.c, Atlantis correctly handled the error case by introducing logic to return a special error code, ensuring robust and predictable behavior.\nThat said, the patches produced during this competition don’t fully showcase Atlantis’s potential, as the challenges lacked sophistication. For a more advanced example, check out our work on the SQLite3 zero-day vulnerability. Since then, Atlantis has undergone multiple rounds of improvement, and we remain committed to open-sourcing the full details of our system after the AIxCC competition.\nUnsound Patches Atlantis was unable to patch three of the challenges due to unsound patches. Two of these failures stemmed from the puzzle-like nature of the challenges:\nMaze Challenge: This challenge required finding the path through a maze. A patch that fixes the vulnerability inadvertently altered the intended behavior, conflicting with the challenge\u0026rsquo;s goals. Shellcode-Golf Challenge: This challenge demanded crafting a specific shellcode to pass a verification check. A patch that addressed the vulnerability ended up modifying the check logic, effectively changing the intended behavior. The third unsound patch occurred due to the complexity of inter-procedural logic, which presented challenges for Atlantis in accurately resolving the issue without disrupting the intended program flow.\nClick me to show the pseudo code uint32_t foo(void) { char buf[10]; ssize_t size = read(0, buf, sizeof(buf) - 1); buf[size] = \u0026#39;\\0\u0026#39;; uint32_t res = atoi(buf); return res; } void main_loop() { while (1) foo(); } When a read operation fails due to empty input, the size variable becomes -1, leading to a stack underflow and subsequent crash. While Atlantis’s patch system successfully addresses the root cause and prevents an ASAN crash, it inadvertently introduces an infinite loop due to the lack of handling for empty input within the main loop. As a result, this patch is deemed unsound because it fails to fully resolve the issue in a functional and robust manner.\nTeam Atlanta\u0026rsquo;s Next Steps Frankly, Atlantis still has a long way to go before becoming a seamless autonomous CTF competitor for pwnable challenges, which remains one of our team’s long-term goals.\nTo move closer to this vision, we are focusing on the following improvements to create a fully autonomous CTF pwner agent:\nChallenge Understanding:\nWhile we concentrate on pwnable challenges, modern pwnables often come in diverse formats, such as kernel drivers and patched browsers. Atlantis needs to identify the challenge format and generate appropriate analysis code to handle these variations, aiming for more general-purpose functionality.\nBinary Analysis Support:\nCurrently, Atlantis supports only source code repositories. Since many CTF challenges are distributed as binaries, we plan to integrate our own decompilation framework to enable binary analysis. This approach will offer a tailored experience, moving beyond existing tools like IDA Pro to provide more comprehensive support.\nAutomatic Exploit Generation:\nAt present, Atlantis can only generate Proof-of-Concept (PoC) code to trigger vulnerabilities. Our goal is to enable the generation of more powerful exploits, such as arbitrary read/write primitives, expanding its utility and effectiveness in real-world scenarios.\nCustomized LLM Models:\nWe are working on customizing LLM models specifically for security analysis. Tailored models could reduce the need for sophisticated prompts, simplify our system architecture, and improve both speed and accessibility, making Atlantis more efficient and user-friendly.\nBy addressing these challenges, we aim to push the boundaries of AI in cybersecurity and bring Atlantis closer to becoming a fully autonomous and versatile competitor.\nThe Ending Note As someone from a generation that grew up learning computer science through CTF competitions, I can confidently say that CTFs have been instrumental in teaching us about operating systems and security. Looking ahead, I sincerely hope that CTFs will once again serve as a platform to teach us how to develop new language models and agents for security research.\nI hope our first step in TKCTF 2024 inspires more researchers to join this vibrant and innovative community. If you’re passionate about AI or security, I invite you to follow us on Twitter/X (@TeamAtlanta24) and join us on this exciting journey.\nFully sound patches address root causes while preserving correct system behavior.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{
  "section": "Blog",
  "slug": "/blog/post-asc-sqlite/",
  "title": "Autonomously Uncovering and Fixing a Hidden Vulnerability in SQLite3 with an LLM-Based System",
  "description": "SQLite3 in ASC",
  "date": "August 28, 2024",
  "image": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/asc-sqlite/sqlite-bug-finding_hu_a9e04e86be1a5c41.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"315\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/asc-sqlite\\/sqlite-bug-finding_hu_2fc6cc4e4258aef8.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n",
  "imageSM": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/asc-sqlite/sqlite-bug-finding_hu_a8b2201771155688.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/asc-sqlite\\/sqlite-bug-finding_hu_e756cbb8915fac76.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n",
  "searchKeyword": "",
  "categories": "Vulnerability Analysis",
  "tags": "Atlantis CRS",
  "content":"Without knowing beforehand that the challenge project involved SQLite3, our team, Team Atlanta, entered our Cyber Reasoning System (CRS), named Atlantis, into the AI Cyber Challenge organized by ARPA-H, DARPA, and the White House.\nRemarkably, Atlantis secured six first-bloods and autonomously identified and patched a real bug in SQLite31, earning us a $2 million prize and a place in the grand finals of AIxCC. For more details, check out our team\u0026rsquo;s announcement blog.\nIn this blog, we will outline our very high-level approach to using LLMs for bug detection and vulnerability remediation, provide an analysis of the fixed SQLite3 vulnerability, and discuss the challenges of using our LLM agents for such fixes.\nFollow us on Twitter/X (@TeamAtlanta24) if you\u0026rsquo;re interested in AI or security.\nThe Atlantis Cyber Reasoning System Atlantis is a end-to-end, large language model (LLM)-based bug-finding and fixing system designed to function entirely without human intervention. It is capable of handling complex systems like the Linux kernel and supports a range of modern programming languages, including C/C++, Java, and others.\nOur design philosophy is simple: to emulate the mindset of experienced security researchers and hackers through LLM agents, enhanced with advanced program analysis techniques.\nAtlantis is specifically designed to replicate the behavior of human researchers, particularly in auditing the Git repositories of open-source software (OSS). To harness the full potential of LLMs and address their limitations in tackling complex problems, we incorporate traditional program analysis techniques (both dynamic and static) to assist LLMs in decision-making.\nOne of the interesting features of Atlantis is our \u0026ldquo;baby-security-AGI\u0026rdquo; system, which can emulate the code auditing process based on the habits of the security experts on our team. It\u0026rsquo;s not magic; we\u0026rsquo;ve distilled our collective experience and common practices in manual auditing and reverse engineering into structured prompts, significantly enhancing the system\u0026rsquo;s capabilities.\nAll source code will be open-sourced in accordance with the AIxCC competition rules.\nThe Off-by-One Access in SQLite3 The hidden vulnerability was discovered in the FTS5 module of SQLite3 (link). The bug is located in the trigram tokenizer, which processes each contiguous sequence of three characters as a token, enabling FTS5 to support more general substring matching.\nWhen creating a virtual table, users can specify options in the trigram field (e.g., case_sensitive 1) as outlined in the documentation.\nHowever, if users fail to provide key-value pairs, SQLite3 does not adequately check for this and assumes that the value is present. This assumption can lead to an off-by-one access error. Because SQLite3 allocates a sufficient heap buffer in such cases, the off-by-one access is confined to the space within an allocated heap chunk. Additionally, due to SQLite\u0026rsquo;s good coding practices, it uses MallocZero to ensure no uninitialized variables exist, which ultimately results in a zero pointer dereference.\nClick me to show the vulnerable code in SQLite3 static int fts5TriCreate( void *pUnused, const char **azArg, int nArg, Fts5Tokenizer **ppOut ){ int rc = SQLITE_OK; TrigramTokenizer *pNew = (TrigramTokenizer*)sqlite3_malloc(sizeof(*pNew)); UNUSED_PARAM(pUnused); if( pNew==0 ){ rc = SQLITE_NOMEM; }else{ int i; pNew-\u0026gt;bFold = 1; pNew-\u0026gt;iFoldParam = 0; for(i=0; rc==SQLITE_OK \u0026amp;\u0026amp; i\u0026lt;nArg; i+=2){ const char *zArg = azArg[i+1]; \u0026lt;---- off-by-one if( 0==sqlite3_stricmp(azArg[i], \u0026#34;case_sensitive\u0026#34;) ){ if( (zArg[0]!=\u0026#39;0\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;1\u0026#39;) || zArg[1] ){ \u0026lt;----null dereference rc = SQLITE_ERROR; }else{ pNew-\u0026gt;bFold = (zArg[0]==\u0026#39;0\u0026#39;); } }else if( 0==sqlite3_stricmp(azArg[i], \u0026#34;remove_diacritics\u0026#34;) ){ if( (zArg[0]!=\u0026#39;0\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;1\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;2\u0026#39;) || zArg[1] ){ \u0026lt;-- rc = SQLITE_ERROR; }else{ pNew-\u0026gt;iFoldParam = (zArg[0]!=\u0026#39;0\u0026#39;) ? 2 : 0; } }else{ rc = SQLITE_ERROR; } } if( pNew-\u0026gt;iFoldParam!=0 \u0026amp;\u0026amp; pNew-\u0026gt;bFold==0 ){ rc = SQLITE_ERROR; } if( rc!=SQLITE_OK ){ fts5TriDelete((Fts5Tokenizer*)pNew); pNew = 0; } } *ppOut = (Fts5Tokenizer*)pNew; return rc; } Working proof-of-concepts (PoCs) to trigger the bug are as follows:\nPoC 1: CREATE VIRTUAL TABLE t USING fts5(s, tokenize=\u0026#39;trigram case_sensitive\u0026#39;); PoC 2: CREATE VIRTUAL TABLE t USING fts5(s, tokenize=\u0026#39;trigram remove_diacritics\u0026#39;); While human researchers might consider this bug trivial, I am still pleased that our system enabled us to be the only team to discover a real bug—even without knowing that the challenge project was SQLite3.\nAuto-Generated Patch vs Human-Write Patch Automatically patching the aforementioned bug is challenging because a perfect patch2 requires the human or AI patcher to understand that the arguments are paired and that the null pointer dereference results from an off-by-one heap access within a glibc-allocated heap chunk.\nIn other words, the actual root cause is a semantic inconsistency concerning the paired arguments. Therefore, the patcher needs to recognize that the code must verify if the number of arguments is even.\nGenerating Patches Using a Naive Zero-Shot Prompt To get a basic sense of how LLMs reason about patch generation, I asked the models to analyze the root cause and generate a patch for the given bug using the crash logs, PoCs, and the code of the fts5TriCreate() function.\nA patch generated by Claude 3.5 sonnet --- a/sqlite3.c +++ b/sqlite3.c @@ -255841,7 +255841,7 @@ static int fts5TriCreate( for(i=0; rc==SQLITE_OK \u0026amp;\u0026amp; i\u0026lt;nArg; i+=2){ const char *zArg = azArg[i+1]; if( 0==sqlite3_stricmp(azArg[i], \u0026#34;case_sensitive\u0026#34;) ){ - if( (zArg[0]!=\u0026#39;0\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;1\u0026#39;) || zArg[1] ){ + if( !zArg || (zArg[0]!=\u0026#39;0\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;1\u0026#39;) || zArg[1] ){ rc = SQLITE_ERROR; }else{ pNew-\u0026gt;bFold = (zArg[0]==\u0026#39;0\u0026#39;); @@ -255849,7 +255849,7 @@ static int fts5TriCreate( }else if( 0==sqlite3_stricmp(azArg[i], \u0026#34;remove_diacritics\u0026#34;) ){ - if( (zArg[0]!=\u0026#39;0\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;1\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;2\u0026#39;) || zArg[1] ){ + if( !zArg || (zArg[0]!=\u0026#39;0\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;1\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;2\u0026#39;) || zArg[1] ){ rc = SQLITE_ERROR; }else{ pNew-\u0026gt;iFoldParam = (zArg[0]!=\u0026#39;0\u0026#39;) ? 2 : 0; A patch generated by ChatGPT 4o --- a/sqlite3.c +++ b/sqlite3.c @@ -255835,11 +255835,15 @@ pNew-\u0026gt;iFoldParam = 0; for(i=0; rc==SQLITE_OK \u0026amp;\u0026amp; i\u0026lt;nArg; i+=2){ const char *zArg = azArg[i+1]; - if( 0==sqlite3_stricmp(azArg[i], \u0026#34;case_sensitive\u0026#34;) ){ + if( i+1 \u0026gt;= nArg || azArg[i+1] == NULL ){ + rc = SQLITE_ERROR; + }else if( 0==sqlite3_stricmp(azArg[i], \u0026#34;case_sensitive\u0026#34;) ){ if( (zArg[0]!=\u0026#39;0\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;1\u0026#39;) || zArg[1] ){ rc = SQLITE_ERROR; }else{ pNew-\u0026gt;bFold = (zArg[0]==\u0026#39;0\u0026#39;); } }else if( 0==sqlite3_stricmp(azArg[i], \u0026#34;remove_diacritics\u0026#34;) ){ if( (zArg[0]!=\u0026#39;0\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;1\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;2\u0026#39;) || zArg[1] ){ rc = SQLITE_ERROR; The patches are fairly close to \u0026ldquo;good patches,\u0026rdquo; and the models almost identify the root causes. However, due to my simple zero-shot prompt, there are still a few minor issues. For example, the patch generated by Claude effectively mitigates the bug by ensuring that a null pointer is always handled when the bug is triggered, but it doesn\u0026rsquo;t fully address the off-by-one root cause. The patch generated by GPT-4o addresses the bounds check but only covers the case_sensitive branch, missing the remove_diacritics branch.\nNonetheless, the results are promising given that a simple prompt was used, and the LLMs almost arrived at the correct solution. However, in real-world scenarios, patch generation can be much more challenging, as it requires locating the source code of the vulnerable parts in a large codebase, dealing with indeterminate results, handling hallucinations, and validating correctness, among other issues.\nPatch Generated by Atlantis CRS Atlantis CRS generates patches by incrementally applying domain-specific knowledge with multiple LLM agents and iteratively refining them using a custom validation oracle. The patch generated by Atlantis is as follows:\ndiff --git a/ext/fts5/fts5_tokenize.c b/ext/fts5/fts5_tokenize.c index f12056170..552f14be9 100644 --- a/ext/fts5/fts5_tokenize.c +++ b/ext/fts5/fts5_tokenize.c @@ -1299,8 +1299,10 @@ static int fts5TriCreate( pNew-\u0026gt;bFold = 1; pNew-\u0026gt;iFoldParam = 0; for(i=0; rc==SQLITE_OK \u0026amp;\u0026amp; i\u0026lt;nArg; i+=2){ - const char *zArg = azArg[i+1]; - if( 0==sqlite3_stricmp(azArg[i], \u0026#34;case_sensitive\u0026#34;) ){ + const char *zArg = (i+1 \u0026lt; nArg) ? azArg[i+1] : NULL; + if (zArg == NULL) { + rc = SQLITE_ERROR; + } else if( 0==sqlite3_stricmp(azArg[i], \u0026#34;case_sensitive\u0026#34;) ){ if( (zArg[0]!=\u0026#39;0\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;1\u0026#39;) || zArg[1] ){ rc = SQLITE_ERROR; }else{ The auto-generated patch successfully checks the bounds and provides additional protection against null pointer dereference. In this case, Atlantis spent ~15 minutes for the entire building, patch generation, iteration, and correctness-validation process, demonstrating its promising potential for application to real-world software.\nAdmittedly, our patching techniques are still in their prototype stages, as we require more time to apply and evaluate the many new ideas we have. However, the patch above illustrates that LLM-based automated vulnerability remediation is a very feasible direction.\nThanks Seunggi for collecting the statistics when patching the bug.\nOfficial SQLite3 patch Click me to show SQlite3's official patch commit e9b919d550262076d1b8453c3d6852b88822b922 Author: drh \u0026lt;\u0026gt; Date: Tue Aug 6 22:49:01 2024 +0000 Improved robustness of parsing of tokenize= arguments in FTS5. [forum:/forumpost/171bcc2bcd|Forum post 171bcc2bcd]. FossilOrigin-Name: d9f726ade6b258f8723f90d0b04a4682e885e30939eb29773913e4dfc8e85503 diff --git a/ext/fts5/fts5_tokenize.c b/ext/fts5/fts5_tokenize.c index 3e9fdff3e..08de0d60d 100644 --- a/ext/fts5/fts5_tokenize.c +++ b/ext/fts5/fts5_tokenize.c @@ -79,7 +79,7 @@ static int fts5AsciiCreate( int i; memset(p, 0, sizeof(AsciiTokenizer)); memcpy(p-\u0026gt;aTokenChar, aAsciiTokenChar, sizeof(aAsciiTokenChar)); - for(i=0; rc==SQLITE_OK \u0026amp;\u0026amp; i\u0026lt;nArg; i+=2){ + for(i=0; rc==SQLITE_OK \u0026amp;\u0026amp; i\u0026lt;nArg-1; i+=2){ const char *zArg = azArg[i+1]; if( 0==sqlite3_stricmp(azArg[i], \u0026#34;tokenchars\u0026#34;) ){ fts5AsciiAddExceptions(p, zArg, 1); @@ -90,6 +90,7 @@ static int fts5AsciiCreate( rc = SQLITE_ERROR; } } + if( i\u0026lt;nArg ) rc = SQLITE_ERROR; if( rc!=SQLITE_OK ){ fts5AsciiDelete((Fts5Tokenizer*)p); p = 0; @@ -381,17 +382,16 @@ static int fts5UnicodeCreate( } /* Search for a \u0026#34;categories\u0026#34; argument */ - for(i=0; rc==SQLITE_OK \u0026amp;\u0026amp; i\u0026lt;nArg; i+=2){ + for(i=0; rc==SQLITE_OK \u0026amp;\u0026amp; i\u0026lt;nArg-1; i+=2){ if( 0==sqlite3_stricmp(azArg[i], \u0026#34;categories\u0026#34;) ){ zCat = azArg[i+1]; } } - if( rc==SQLITE_OK ){ rc = unicodeSetCategories(p, zCat); } - for(i=0; rc==SQLITE_OK \u0026amp;\u0026amp; i\u0026lt;nArg; i+=2){ + for(i=0; rc==SQLITE_OK \u0026amp;\u0026amp; i\u0026lt;nArg-1; i+=2){ const char *zArg = azArg[i+1]; if( 0==sqlite3_stricmp(azArg[i], \u0026#34;remove_diacritics\u0026#34;) ){ if( (zArg[0]!=\u0026#39;0\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;1\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;2\u0026#39;) || zArg[1] ){ @@ -416,6 +416,7 @@ static int fts5UnicodeCreate( rc = SQLITE_ERROR; } } + if( i\u0026lt;nArg ) rc = SQLITE_ERROR; }else{ rc = SQLITE_NOMEM; @@ -1298,7 +1299,7 @@ static int fts5TriCreate( int i; pNew-\u0026gt;bFold = 1; pNew-\u0026gt;iFoldParam = 0; - for(i=0; rc==SQLITE_OK \u0026amp;\u0026amp; i\u0026lt;nArg; i+=2){ + for(i=0; rc==SQLITE_OK \u0026amp;\u0026amp; i\u0026lt;nArg-1; i+=2){ const char *zArg = azArg[i+1]; if( 0==sqlite3_stricmp(azArg[i], \u0026#34;case_sensitive\u0026#34;) ){ if( (zArg[0]!=\u0026#39;0\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;1\u0026#39;) || zArg[1] ){ @@ -1316,6 +1317,7 @@ static int fts5TriCreate( rc = SQLITE_ERROR; } } + if( i\u0026lt;nArg ) rc = SQLITE_ERROR; if( pNew-\u0026gt;iFoldParam!=0 \u0026amp;\u0026amp; pNew-\u0026gt;bFold==0 ){ rc = SQLITE_ERROR; The patch changes the loop boundary and checks early exit to prevent out-of-bounds access. After the loop, there\u0026rsquo;s an additional check:\nif( i\u0026lt;nArg ) rc = SQLITE_ERROR; This check ensures that all arguments were processed. If i is less than nArg after the loop, it means there was an odd numbers of arguments, which is considered an error because the arguments should always come in pairs.\nInterestingly, the maintainer patched fts5UnicodeCreate() and fts5AsciiCreate() as well because similar code patterns existing there. It actually shows the strength of human-write patches because developers remember potential buggy paths in their code base. However, the additional checks are actually unnecessary because the checks are already at the beginning of the functions. It demonstrates human-write patches are not perfect as well.\nstatic int fts5AsciiCreate( void *pUnused, const char **azArg, int nArg, Fts5Tokenizer **ppOut ){ int rc = SQLITE_OK; AsciiTokenizer *p = 0; UNUSED_PARAM(pUnused); if( nArg%2 ){ \u0026lt;---- already checks rc = SQLITE_ERROR; }else{ --\u0026gt; unnecessary checks \u0026lt;--- I believe the maintainers notice the issues so that they changed their patch in b651084 by checking if (nArgs % 2) == 0 at the beginning of fts5TriCreate(). At the same time, they removed the unnecessary patches in fts5AsciiCreate() and fts5UnicodeCreate().\nThe Author\u0026rsquo;s Random Thoughts By leveraging generative AI models (GenAI) as \u0026ldquo;high-level\u0026rdquo; static analysis tools, we can significantly enhance automated bug finding, thanks to their proficiency in code explanation. For example, complex program analysis tasks such as points-to analysis and inter-procedural analysis, which are challenging for traditional compilers, can be approached differently using GenAI through retrieve-augmented generation (RAG). Additionally, GenAI opens new possibilities for automatic exploit generation and vulnerability remediation due to its strong capabilities in code writing.\nHowever, GenAI is not a cure-all and is far from perfect. That\u0026rsquo;s why our hybrid system is designed to improve GenAI\u0026rsquo;s performance in security research by addressing common issues such as LLM hallucinations, scalability, and domain-specific challenges for particular software.\nAIxCC has provided our team with a fantastic opportunity to put into practice the insights gained from decades of security research in both academia and industry. If you\u0026rsquo;re interested in learning more about our team and the work done by our team members, please feel free to contact us!\nFollow us on Twitter/X (@TeamAtlanta24) if you\u0026rsquo;re interested in AI or security.\nDiscovering previously unknown bugs does not count as a valid score in the competition. Team Atlanta secured a finalist spot by submitting the intended bugs and patches for AIxCC.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe word \u0026ldquo;perfect patch\u0026rdquo; is a vague concept in my mind. To clarify, a patch should comprehend the context and semantics of the program and address the actual root causes rather than merely adding superficial checks to bypass address sanitizers (ASAN).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{
  "section": "Blog",
  "slug": "/blog/post-atl/",
  "title": "Announcing Team Atlanta!",
  "description": "Beginning",
  "date": "August 13, 2024",
  "image": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/atl/team_hu_10735b3afab863cf.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"315\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/atl\\/team_hu_d7c7a8cfea6b78e0.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n",
  "imageSM": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/atl/team_hu_de4aeb404ef1d2c1.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/atl\\/team_hu_7581c14e352d94f1.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n",
  "searchKeyword": "",
  "categories": "Announcement",
  "tags": "team",
  "content":"Hello, world! We are Team Atlanta, the minds behind Atlantis, our innovative AI-driven cybersecurity solution competing in the prestigious DARPA AIxCC.\nOur team is a collaborative powerhouse made up of six leading institutions: Georgia Tech, GTRI, Samsung Research, Samsung Research America, KAIST, and POSTECH. Each of these organizations is led by Georgia Tech alumni, and includes past winners of prestigious hacking competitions such as DEF CON CTF, Pwn2Own and kernelCTF.\nFor the past several months, we have been diligently preparing for this competition, combining our expertise in AI, cybersecurity, and software engineering. Last week, we proudly competed in the AIxCC Semifinals, showcasing our hard work and dedication to advancing cybersecurity through artificial intelligence.\nThe Journey Begins When AIxCC was announced last year, we quickly assembled a team of friends, including Zellic and SSLab. At that time, much was uncertain; details about the game format, scoring rubric, proof-of-vulnerability (PoV), sanitizers, harnesses, supported programming languages, and proof-of-understanding (PoU) were all unclear. Our team, however, started preparing for the competition from last October.\nMany of our team members previously participated in the DARPA Cyber Grand Challenge (CGC) as part of Crspy, where we were responsible for bug finding and exploitation generation. DARPA CGC was an ambitious endeavor that sparked numerous innovative research directions afterward. However, the competition was not without its challenges, particularly due to the gamification of the event; the scoring metrics and rules significantly influenced the outcomes. In the end, the competing Cyber Reasoning Systems (CRS) that focused on operating reactively\u0026ndash;prioritizing the availability score over fixing bugs\u0026ndash; tended to score higher, as exploitation proved to be far more difficult than patching.\nAware of the gamification issues from CGC, we anticipated that to excel in AIxCC our CRS should leverage AI, particularly LLMs, aggressively in various depths and levels of the CRS pipelines. With this in mind, we strategically chose to focus our efforts on two key directions:\nStatic Analysis. To encourage the use of LLMs and set AIxCC apart from CGC, we anticipated that AIxCC would strongly advocate for the adoption of static analysis while steering away from the dominant use of fuzzing1. It\u0026rsquo;s important to note that finding bugs is quite different from finding crash- or bug-triggering inputs. The latter offers a clear advantage in objectively and autonomously verifying the discovered bug, but it has a much narrower scope compared to the former. In practice, the triggering aspect, also known as the reachability problem, is a significantly more challenging and crucial issue to address, where dynamic tools like fuzzing have a clear edge.\nFine-tuning LLMs for Source Code. Specialization is always an advantage when possible. Given that each CRS will likely need to support more than 10 programming languages during the competition, we decided to fine-tune both in-house and open-source models for analyzing code. This approach is conceptually similar to commitPack, but focuses on commits related to bugs like their fixes, bug-introducing commits, descriptions, and public exploits, if available. Our expectation was that training with this data would enable the fine-tuned LLM to reason about security bugs, their fixes, and likely input corpus, more effectively than the foundational model.\nWe quickly realized that to pursue these directions effectively, we first needed a dataset: a benchmark. Our team divided tasks into three areas: 1) static analysis using LLM prompts/agents, 2) developing a C benchmark from sources like CGC and OSS-Fuzz, and 3) collecting a training dataset pairing CVEs with patches and PoCs for open-source projects to fine-tune our in-house code model at Samsung or to leverage open-source LLMs.\nRemarkably, within 4-5 months, we accomplished all three goals, and our LLM-based Cyber Reasoning System (CRS), dubbed Skynet, performed surprisingly well on our benchmark, and fine-tuning on a smaller dataset shows some promises like in python.\nTime flew by. The cold winter of 2023 ended, and we found ourselves in the new year of 2024. I vividly remember that around this time, our dear friends from Zellic left our team to pursue the Small Business Innovation Research (SBIR) track, which DARPA supports with $1 million for the competition. Unfortunately, Georgia Tech and Samsung were not eligible for this award.\nKick-off with Surprises! At the kick-off event on March 29th, AIxCC unveiled the first challenge project: the Linux kernel, along with an example vulnerability, CVE-2021-43267. This bug is well documented, and its PoC exploit is publicly available, making it an excellent example to work on.\nWhat makes this bug even more intriguing is the story behind it. A security researcher audited the Linux kernel source code using CodeQL. Specifically, the researcher was searching for instances where 16-bit size parameters are passed to the kmalloc() function for memory allocation, using a dataflow-based CodeQL query. The intuition was that a 16-bit size parameter could easily lead to an integer overflow when accessing the allocated object. However, the discovered bug was not caused by an integer overflow, but an out-of-bound heap overflow due to a missing sanity check on the size and related inputs.\nstatic bool tipc_crypto_key_rcv(struct tipc_crypto *rx, struct tipc_msg *hdr) { struct tipc_crypto *tx = tipc_net(rx-\u0026gt;net)-\u0026gt;crypto_tx; struct tipc_aead_key *skey = NULL; u16 key_gen = msg_key_gen(hdr); u16 size = msg_data_sz(hdr); u8 *data = msg_data(hdr); ... /* Allocate memory for the key */ skey = kmalloc(size, GFP_ATOMIC); if (unlikely(!skey)) { pr_err(\u0026#34;%s: unable to allocate memory for skey\\n\u0026#34;, rx-\u0026gt;name); goto exit; } /* Copy key from msg data */ skey-\u0026gt;keylen = ntohl(*((__be32 *)(data + TIPC_AEAD_ALG_NAME))); memcpy(skey-\u0026gt;alg_name, data, TIPC_AEAD_ALG_NAME); memcpy(skey-\u0026gt;key, data + TIPC_AEAD_ALG_NAME + sizeof(__be32), skey-\u0026gt;keylen); The skey was allocated with a size based on the user-provided hdr, but skey-\u0026gt;key was copied up to skey-\u0026gt;keylen, which was also user-controlled and could therefore be inconsistent with size. Unfortunately, the kernel did not perform a sanity check on these two parameters, causing an out-of-boundary access.\ncommit fa40d9734a57bcbfa79a280189799f76c88f7bb0 Author: Max VA \u0026lt;maxv@sentinelone.com\u0026gt; Date: Mon Oct 25 17:31:53 2021 +0200 tipc: fix size validations for the MSG_CRYPTO type The function tipc_crypto_key_rcv is used to parse MSG_CRYPTO messages to receive keys from other nodes in the cluster in order to decrypt any further messages from them. This patch verifies that any supplied sizes in the message body are valid for the received message. diff --git a/net/tipc/crypto.c b/net/tipc/crypto.c index c9391d38de85..dc60c32bb70d 100644 --- a/net/tipc/crypto.c +++ b/net/tipc/crypto.c @@ -2285,43 +2285,53 @@ static bool tipc_crypto_key_rcv(struct tipc_crypto *rx, struct tipc_msg *hdr) u16 key_gen = msg_key_gen(hdr); u16 size = msg_data_sz(hdr); u8 *data = msg_data(hdr); +\tunsigned int keylen; + +\t/* Verify whether the size can exist in the packet */ +\tif (unlikely(size \u0026lt; sizeof(struct tipc_aead_key) + TIPC_AEAD_KEYLEN_MIN)) { +\tpr_debug(\u0026#34;%s: message data size is too small\\n\u0026#34;, rx-\u0026gt;name); +\tgoto exit; +\t} + +\tkeylen = ntohl(*((__be32 *)(data + TIPC_AEAD_ALG_NAME))); + +\t/* Verify the supplied size values */ +\tif (unlikely(size != keylen + sizeof(struct tipc_aead_key) || +\tkeylen \u0026gt; TIPC_AEAD_KEY_SIZE_MAX)) { +\tpr_debug(\u0026#34;%s: invalid MSG_CRYPTO key size\\n\u0026#34;, rx-\u0026gt;name); +\tgoto exit; +\t} Two checks were added to fix this bug: verifying that size is greater than the minimum key size, and ensuring that keylen is consistent with size, thereby preventing access beyond the allocated object.\nMisunderstanding 1: PoV Given a massive Linux repository (yes, 20 million lines of code), where should we start? The LLM approach is all about asking the right questions, also known as prompt engineering. We utilized various techniques like Chain-of-Thought (CoT) and Tree-of-Thoughts (ToT), and were exploring Retrieval Augmented Generation (RAG) to quickly identify known 1-day bugs.\nAt that time, context size was limited; the most advanced model, gpt-3.5 turbo (yes, pre-gpt-4 era) from OpenAI, supported 16k tokens, making it crucial to ask the right question! We initially tried identifying potentially vulnerable code snippets using a range of static analysis tools, including CodeQL, Semgrep and various tools from academic publications, and then filtered the results with LLMs. We even considered diffing the upstream Linux kernel against the provided repository, so that our CRS can look at the modified part of the code first.\nWe were confident our decision; to promote the use of AI tools, the AIxCC organizers would design the competition in a way that allows a single CRS codebase to explore any code repository using 10+ programming languages and their combinations.\nAh, around that time, Google had just announced gemini-pro with an impressive 128k context and the potential to support 1 million tokens! Meanwhile, gpt-4 introduced a game-changing feature called function calling, which allows the LLM to select which callback to use and integrate the results back into the prompt at runtime. We felt that everything was evolving favorably for our CRS to adopt these cutting-edge techniques.\nHowever, PoV turned out to mean bug-triggering input or a crashing input. To demonstrate the existence of a bug, each CRS needed to formulate an input that the referee could quickly verify. While this approach is straightforward and objective for the competition, it significantly discourages the adoption of LLMs in finding bugs. Our team quickly realized that we needed to pivot to the dynamic approaches like fuzzing for the competition.\nvoid tipc_trigger(uint8_t *smashbuf, uint32_t smashlen, int seqno) { uint8_t pkt[0x1000]; uint32_t w0, w1, w2, w3, w4, w5; w0 = hdr_version(TIPC_VERSION); w0 |= hdr_size(6); w0 |= hdr_user(MSG_CRYPTO); w0 |= hdr_msg_size(24 + 36 + KEY_SIZE); w1 = 0; w2 = seqno; w3 = NODE_ID; w4 = 0; w5 = 0; memset(pkt, 0, sizeof(pkt)); gen_tipc_hdr(pkt, w0, w1, w2, w3, w4, w5); memcpy(pkt+24, \u0026#34;HAXX\u0026#34;, 4); *(uint32_t*)(pkt+24+32) = be32(KEY_SIZE + SMASH_SIZE + smashlen); // \u0026lt;- (1) memset(pkt+24+36, \u0026#39;C\u0026#39;, KEY_SIZE); memset(pkt+24+36+KEY_SIZE, \u0026#39;D\u0026#39;, SMASH_SIZE); memcpy(pkt+24+36+KEY_SIZE + SMASH_SIZE, smashbuf, smashlen); tipc_send(pkt, sizeof(pkt)); } Formulating a bug-triggering input, including ensuring its reachability, is a far more challenging task than simply spotting buggy code in the repository. The strength of fuzzing, perhaps the opposite of a sophisticated LLM, is that once a bug is found, you almost always have a bug-triggering input.\nIn CVE-2021-43267, using CodeQL and auditing, one could identify this bug, but triggering it is an entirely different challenge, not to mention exploiting it. For example, TIPC must be properly set up first, and the keylen needs to be precisely crafted in (1) to trigger the bug.\nMisunderstanding 2. Harnesses Sorry, what\u0026rsquo;s the input needed to trigger CVE-2021-43267? even with a fuzzer?\nTo fuzz the Linux kernel, we needed a user program that calls a sequence of system calls with various arguments. Considering the Linux kernel has over 400 system calls to explore, this was far from ideal for a competition setting.\nWe initially assumed that harnesses and test cases would be provided to indicate which parts of the Linux kernel should be checked for bugs. To tackle this, we implemented and adopted various versions of Linux kernel fuzzers, including a custom kernel syscall fuzzer with kcov and kcmp, and also utilized the most popular Linux fuzzer, Syzkaller. However, our focus remained on determining which sequences of system calls to test, using syscall traces and static analysis of the provided program, and then correctly formulating an end-to-end userspace program to trigger the bug.\n/*** * Blob begins with a 4 byte command count * [4-bytes command count] * Currently there are two commands: * 0 - send a packet blob * [4-bytes size][4-bytes send flags][size-bytes packet data] * 1 - send a netlink packet * [4-bytes Message Type][4-bytes Message Flags][4-bytes Netlink Protocol][4-bytes size][size bytes data] * blob_size MUST be a trusted value */ int harness( uint8_t *blob, uint32_t blob_size) { ... } The Linux Kernel CP was announced in April and came with a harness, linux_test_harness.c. This announcement was full of surprises; the program\u0026rsquo;s structure was provided by the harness, which is alas what we primarily focused on, and the blob needed to be fed to the harness in a way that triggers the bug. The types of system calls we could interact with were limited by the harness, and our task was to find the right data input that would lead the harness to invoke the necessary sequence of system calls with the correct parameters. In other words, we needed to understand the harness first before dealing with the Linux kernel bugs.\nLater, the Jenkins harness was announced, and more surprisingly, it was a fuzz driver (often called a fuzzing harness), a standalone program designed to invoke APIs for fuzz testing. In May, a new CP, called mock-cp (a userspace program), was introduced along with a new harness format, which was simply a shell script executing a CP binary with the provided input. Such diverse formats got us thinking that our CRS should adopt LLM to figure out the structure of the programs and CPs first; like how to compile, how to correctly run, etc.\nBy June, the harness format was officially established - surprisingly, yet not entirely unexpected: libfuzzer for userspace programs (mock-cp and Nginx), jazzer for Java programs (Jenkins), while retaining the blob-based harness for the Linux kernel. We continually updated our CRS to adapt to these changes, but many of these decisions rendered our LLM-based components unnecessary. This decision, however, greatly helped all the participating teams by reducing the engineering time needed for game operation. Unfortunately, we were too proactive in reacting to these changes and ended up wasting some engineering time as a result 😊.\nA harness\u0026rsquo;s role is crucial in the AIxCC competition; it sets the context for the CRS to trigger the bug and serves as a key factor in adjusting the difficulty of bug discovery. Therefore, it\u0026rsquo;s important to strike a balance: it should provide enough detail to relieve the CRS from unnecessary burdens, allowing it to focus on bug finding, but without revealing too much information about the bugs.\nMisunderstanding 3. Proof-of-understanding Unlike CGC, which treated the PoV (a proof-of-concept exploit) as sufficient proof of bug discovery, AIxCC required additional information—specifically, the bug type as classified by CWE, to be provided along with the PoV. This was an interesting decision, as AIxCC required CRS to find bugs in the source code, whereas CGC focused on discovering bugs in binaries.\nOur team spent a lot of time brainstorming how to accurately identify CWE categories, primarily by using LLM prompts that leverage crashing inputs, sanitizer reports, related code snippets, outputs from static analyzers, and more. However, the notion of CWEs can be ambiguous when used as a scoring mechanism for the competition. For instance, should CVE-2021-43267 be classified as (1) CWE-122 (Heap-based Buffer Overflow), (2) CWE-787 (Out-of-bounds Write), or (3) CWE-20 (Improper Input Validation)? The first two describe the symptoms caused by the bug, while the third identifies the root cause, as the patch for this bug involved adding input validations.\nIn the end, AIxCC shifted the focus from PoV to identifying the bug-introducing commit (BIC) - the specific hash or commit ID in the git repository. Combined with the fuzzing harness and PoV, the CRS\u0026rsquo;s task was to run the fuzzing harness and perform a git-bisect to pinpoint the BIC in the repository. We did a simple bisecting in the semifinal but lots of improvement required to be functional for the final event.\nMisunderstanding 4. Semantic patching Patching is one of the most intriguing aspects of AIxCC. In CGC, the PoV was typically a simple exploit (like arbitrary read/write/execute), so mitigation strategies (e.g., adding a stack canary) could effectively thwart the PoV. In fact, patches could be applied without even knowing the specific bug; for example, adding a stack canary to all functions in a binary can prevent buffer overflow exploits that might exist in some places.\nThe challenge in CGC was that the focus was on the binary, and the organizers introduced rules such as a minimum number of bytes changed and performance overheads added to the scoring rubric (e.g., instrumenting all memory accesses to prevent out-of-bound errors). These rules were designed to encourage competitors to generate correct patches. Ultimately, this forced CRS to weigh the pros and cons of universal patching, as both exploiting and patching were extremely difficult during the CGC era, resulting in a trade-off between losing points from exploitation versus losing points from patching and availability.\nIn AIxCC, the CRS must generate a semantically correct patch that not only fixes the identified PoV but also maintains the functional correctness of the CP. This is a tricky task, as correctness cannot be formally defined for CRS - some functional changes may be acceptable, while others may not, depending on the code owner\u0026rsquo;s criteria. One approach to addressing this ambiguity is to provide test code to see if the patch passes the provided, so-called public tests. However, CRS must still account for private tests set by the organizers.\nIn the semifinals, our CRS submitted a patch that successfully prevented the crash and passed the public tests given to us during the competition, but was ultimately rejected in the private functionality tests. We\u0026rsquo;re eager to learn more about the bug and the patch!\nMisunderstanding 5: Sanitizers The concept of sanitizers was unclear to our team until we encountered their concrete implementation for memory-safe languages like Java, and more specifically, for Jenkins, a web application written in Java! The role of a sanitizer, essentially a bug oracle, is to determine whether a bug has been correctly triggered.\nIn memory-unsafe languages like C, standard tools like ASAN and UBSAN can serve as sanitizers to catch memory-safety issues with low or no false positives (e.g., out-of-bound accesses should never occur). However, in memory-safe languages, things get trickier. For example, is executing a command a legitimate feature in CI tools like Jenkins, or should it be treated as a command injection (CWE-78)?\nIn other words, sanitizers are more CP-specific rather than programming language-specific; each CP needs to provide custom sanitizers (e.g., path traversal sanitizers).\nOur team initially spent time working on finding web-related bugs like XSS or CSRF in Jenkins - areas where we believed LLMs could excel in seed generation. However, once AIxCC announced that the sanitizers for Java would be jazzer sanitizers, we decided to shift our focus more towards standard jazzer-based fuzzing.\nSemifinal Our team dedicated most of our engineering effort to building a CRS for the Linux Kernel, and we\u0026rsquo;re proud that our CRS was able to find and correctly generate a patch for CVE-2021-43267 in the end. However, during the semifinal, it appeared that only one harness was provided, similar to the exemplar, and none of the CRSes functioned properly for the Linux Kernel. We loved to know more about how our Linux CRS functioned during the competition.\nIn summary, our CRS earned a total of six achievement badges: five for discovering bugs (i.e., first bloods) and one for a patch.\nOur CRS found several unique bugs, which we will describe in a later blog post!\nAside from the known CPs—Linux (C), Jenkins (Java), and Nginx (C) - there were new CPs introduced, namely Tika (Java) and sqlite3 (C). Our CRS performed relatively well on sqlite3, but unfortunately, our Java CRS struggled with Tika. We would love to learn more about what happened during the competition. Tika, a popular file format parser, has many unique features, such as recursively parsing embedded objects, which may have contributed to the challenges we faced.\nLooking Ahead to the AIxCC Final 🎉 AIxCC Finalists\rWe are thrilled that our team has advanced to the AIxCC finals! We have several ideas that could make the competition even more exciting:\nDifferent execution times based on code complexity.\nThe Linux kernel, with its 6,000 files and 20 million lines of code, requires substantial time for bookkeeping like building, bootstrapping, and bisecting. Compared to smaller programs (e.g., 200k in Tika), it would be beneficial to allocate more time for CRSes to navigate such complex codebases.\nMore programming languages and their combinations.\nTop candidates include Python, Rust, and JavaScript/HTML, along with combinations like JNI (C) in Java or Rust device drivers in the Linux kernel. These would offer a more comprehensive evaluation of CRS capabilities in diverse and challenging settings where CRS is most needed.\nStandardized execution environments.\nStandardizing the compiler (e.g., clang-18), runtime (e.g., JVM version), and base Docker image ahead of time would help teams explore more advanced techniques, such as LLM-based instrumentation, in a controlled environment.\nImproved visualization during the competition.\nWhile the AIxCC village was impressively set up, competing teams and participants had limited visibility into the competition\u0026rsquo;s progress and how each CRS was functioning. To capture more attention from the DEF CON audience, it would be beneficial to expose more technical information during the competition - such as showing current prompts of each CRS in turn, their CPU usage, or even stdout from CRSes (for fun), along with explanations of the progress.\nWith our baseline system up and running, it’s time for our team to explore the possibility of incorporating LLMs or ML techniques into our CRS workflow. If you’re passionate about AIxCC and as committed to the competition as we are, feel free to contact us!\nWe are fortunate to have support from generous sponsors like GT/GTRI, Samsung, and KAIST/NYU. If your company is interested in sponsoring our team, we would be happy to discuss further!\nLast but not least, we want to extend our heartfelt thanks to the AIxCC organizers for launching the competition we\u0026rsquo;ve been craving. Hackers thrive on competition-driven innovation, and this has been an exciting opportunity for all of us.\nI think it’s one of the worst names ever chosen by a security researcher; non-security folks often think it\u0026rsquo;s a really dumb technique. But if you dig into the details, it\u0026rsquo;s actually an impressive AI tool. It operates entirely autonomously, adapting to unknown code, self-learning from past executions, using feedback loops similar to backpropagation, and employing cost functions like coverage maps, and more! Most importantly, like deep learning, it works incredibly well in practice!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"}]
